# Dimensionality Reduction {#sec-dimensionality-reduction}



Dimensionality reduction tends to trade lower performance to get higher training speed and easier visualization. However, it makes the pipeline more complex and may not effective. Therefore, we first try to train your system with the original data before considering using dimensionality reduction.

There are 2 main approaches: projection (PCA, random projection) and manifold learning (locally linear embedding).


## The Curse of Dimensionality

Because of the huge number of spaces in high dimensions, high-dimensional datasets are at risk of being very sparse: most training instances are likely to be far away from each other. Therefore, a new instance will likely be far away from any training instance, making predictions much less reliable than in lower dimensions, since they will be based on much larger extrapolations. In short, the more dimensions the training set has, the greater the risk of overfitting it.


## Projection

Suppose that we want to turn a 3D dataset into 2D (called *subspace*), we will project all instances perpendicularly onto this subspace. 

![Turn a 3D dataset into 2D](images/lower-dimension.png){#fig-lower-dimension}

### PCA

PCA first identifies the righ hyperplane that lies closest to the data, and then it projects the data onto it. This hyperplane preserves maximum variance and minimizes the mean squared distance between the original dataset and its projection.

![Select the right hyperplane (c1)](images/select-hyperplane.png){#fig-select-hyperplane}

How does PCA do that? 

1. Center the data (minus mean).
2. Find an axis (principle component) accounts for the largest amount of variance. 
3. Each next priciple components orthogonal to the previous one accounting for the largest amount of the remaining variance.

To find the principle components, PCA use SVD technique (*singular value decomposition*) that decompose data X into UΣV⊺ where V⊺ contains the unit vectors that define all the principal components.

$$X = UΣV^T$$

Then, we multiply matrix X with V⊺ to get the lower-dimension data.

$$X_d = XV^T$$

::: {.callout-tip}
Hyperparameter: n_components, svd_solver

Attribute of PCA: components_, explained_variance_ratio_, n_components_
:::

Choose the right number of dimensions by setting the n_components hyperparameter.

```{python}

```

Tuning the n_components hyperparameter to compress the data.

```{python}

```

Decompress the transformed data.

$$X_{recovered} = X_dV^T$$

```{python}

```

**Randomized PCA**: svd_solver='random'; quickly find an approximation of d principle components, auto if max(m,n)>500 and n_components < 80% of min(m,n).

```{python}

```

**Incremental PCA**: use np.array_split, partial_fit; fit mini-batch of data

```{python}

```



### Random Projection

For very high-dimension dataset, PCA can be too slow. Therefore, random projection is solution. It works by implementing random linear projection.

1. Choose the optimal number of dimensions by sklearn.random_projection.johnson_lindenstrauss_min_dim: compute minimum number of dimensions to ensure the distances won’t change by more than a given tolerance.

$$d ≥ 4log(m)\frac{1}{(\frac{1}{2}ε2 - \frac{1}{3}ε3)}$$

d: target dimension
m: number of instances
ε: tolerance
2. Generate matrix P of shape [d,n] (n: number of features), with mean 0 and variance 1/d.
3. Reduced matrix = X @ P.T

```{python}

```


### Sparse Random Projection

Work as same as random projection, except for random matrix is sparse, so that it use much less memory and train much faster and is preferred.



## Manifold Learning

In many cases the subspace may twist and turn, projection is not useful. Simply projecting onto a plane (e.g., by dropping x3) would squash different layers together (left). Instead, we will unroll to obtain the 2D dataset (right).

![Projecting and unrolling the roll](images/project-unroll.png){#fig-project-unroll}

The roll is an example of 2D manifold. Generally, a d-dimensional manifold is a part of an n-dimensional space (where d < n) that locally resembles a d-dimensional hyperplane. 

However, manifold learning may not always lead to a better or simpler solution, it all depends on dataset.

![Good use case and not good use case of manifold learning](images/manifold-downside.png){#fig-manifold-downside}


### LLE (Locally Linear Embedding)

LLE is used for nonlinear task, do not scale well.

1. Measuring how each training instance linearly relates to its nearest neighbors (k-nearest neighbors).
2. Find low-dimensional data where these local relationships are best preserved.

```{python}

```

