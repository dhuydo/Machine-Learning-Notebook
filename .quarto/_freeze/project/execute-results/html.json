{
  "hash": "269c597560329d79aa78ef105c085751",
  "result": {
    "markdown": "# A Complete Machine Learning Project {#sec-project}\n\n::: {.callout-tip collapse=\"true\"}\n## Main Steps\n1. Look at the big picture.\n2. Get the data.\n3. Explore and visualize the data to gain insights.\n4. Prepare the data for machine learning algorithms. \n5. Select a model and train it.\n6. Fine-tune model.\n7. Present solution.\n8. Launch, monitor, and maintain system.\n:::\n\n\n\n## Datasets\n\n**Popular open data repositories**\n\n- https://openml.org/  \n- https://Kaggle.com\n- https://PapersWithCode.com  \n- [UC Irvine Machine Learning Repository — Amazon’s AWS datasets](https://archive.ics.uci.edu/)\n- [TensorFlow datasets](https://www.tensorflow.org/datasets?hl=vi)\n\n**Meta portals (they list open data repositories)**\n\n- https://DataPortals.org  \n- https://OpenDataMonitor.eu\n\n**Other pages listing many popular open data repositories**\n\n- [Wikipedia’s list of machine learning datasets](https://en.wikipedia.org/wiki/- List_of_datasets_for_machine-learning_research)\n- https://Quora.com\n- [The datasets subreddit](https://www.reddit.com/r/datasets/)\n\nIn this chapter we’ll use the California Housing Prices dataset from the StatLib repository. This dataset is based on data from the 1990 California census. It is not exactly recent but it has many qualities for learning.\n\n\n## Look at the big picture\n\n**Questions**\n\n1. Business objective? Current solution (if any)?\n2. How to use and benefit from the model? \n3. Data Pipelines?\n4. Determine kind of model\n5. Select preformance measures\n6. Check the Assumptions\n\n\n**Answers**\n\n- Predict median housing price in any district. The results are used for another ML system for investment analysis. The current solution is to gather up-to-date information about a district, or to estimate manually using complex rules if no information.\n- The current solution is costly and time-consuming, and it was often off by more than 30%. Therefore, a ML model could be more useful\n- Data pipelines: A sequence of data processing components. Pipelines are very common in machine learning systems, since there is a lot of data to manipulate and many data transformations to apply.\n- This is a regression task (labeled data), batch learning (data is quite small and not change too much) and model-based learning\n- Metrics for regression:\n\n::: {.callout-note collapse=\"true\"}\n## Expand to learn more about metrics\nBoth the RMSE and the MAE are ways to measure the distance between two vectors: the vector of predictions and the vector of target values. Various distance measures, or norms, are possible:\\\n• Computing the root of a sum of squares (RMSE) corresponds to the Euclidean norm: this is the notion of distance we are all familiar with. It is also called the l2 norm, noted |·|₂ (or just |·|).\\\n• Computing the sum of absolutes (MAE) corresponds to the l1 norm, noted |·|₁. This is sometimes called the Manhattan norm because it measures the distance between two points in a city if you can only travel along orthogonal city blocks.\\\n• More generally, the lk norm of a vector v containing n elements is defined as ∥v∥k = (|v₁|ᵏ + |v₂|ᵏ + ... + |vₙ|ᵏ)¹/ᵏ. l0 gives the number of nonzero elements in the vector, and l∞ gives the maximum absolute value in the vector.\\\nThe higher the norm index, the more it focuses on large values and neglects small ones. This is why the RMSE is more sensitive to outliers than the MAE. But when outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs very well and is generally preferred.\n![L1 norm and L2 norm](images/l1_l2.png)\n:::\n\nRMSE (root mean squared error): l2 norm\\\n$$\nRMSE(X, y)  = \\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}(y_{hat}^{(i)} - y^{(i)})^2}\n$$\n\nMAE (mean squared error): l1 norm\\\n$$\nMAE(X, y)  = \\frac{1}{m}\\sum_{i=1}^{m}|y_{hat}^{(i)} - y^{(i)}|\n$$\n\n- Check with the team in charge of the downstream system that use out output whether it is suitable or not (e.g. it is terrible if after several months building model you realize that they need ordinal output not numerical one)\n\n\n## Get the Data\n\n### Download Data\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n## Load data\n\nfrom pathlib import Path\nimport pandas as pd\nimport tarfile\nimport urllib.request\n\ndef load_data(url):\n\tpath = Path(\"datasets/housing.tgz\")\n\tif not path.is_file():\n\t\tPath(\"datasets\").mkdir(parents=True, exist_ok=True)\n\t\turllib.request.urlretrieve(url, path)\n\t\ttarfile.open(path).extractall(path='datasets')\n\treturn pd.read_csv('datasets/housing/housing.csv')\n\nurl = 'https://github.com/ageron/data/raw/main/housing.tgz'\ndata = load_data(url)\n```\n:::\n\n\n### Quick Look to Data Structure: head(), info(), describe(), value_counts(), histplot()\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n## Quick look\n\npd.set_option('display.max_columns', None)\t\t# display all columns\ndata.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>housing_median_age</th>\n      <th>total_rooms</th>\n      <th>total_bedrooms</th>\n      <th>population</th>\n      <th>households</th>\n      <th>median_income</th>\n      <th>median_house_value</th>\n      <th>ocean_proximity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-122.23</td>\n      <td>37.88</td>\n      <td>41.0</td>\n      <td>880.0</td>\n      <td>129.0</td>\n      <td>322.0</td>\n      <td>126.0</td>\n      <td>8.3252</td>\n      <td>452600.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-122.22</td>\n      <td>37.86</td>\n      <td>21.0</td>\n      <td>7099.0</td>\n      <td>1106.0</td>\n      <td>2401.0</td>\n      <td>1138.0</td>\n      <td>8.3014</td>\n      <td>358500.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-122.24</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1467.0</td>\n      <td>190.0</td>\n      <td>496.0</td>\n      <td>177.0</td>\n      <td>7.2574</td>\n      <td>352100.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-122.25</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1274.0</td>\n      <td>235.0</td>\n      <td>558.0</td>\n      <td>219.0</td>\n      <td>5.6431</td>\n      <td>341300.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-122.25</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1627.0</td>\n      <td>280.0</td>\n      <td>565.0</td>\n      <td>259.0</td>\n      <td>3.8462</td>\n      <td>342200.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThere are total 10 features, each row represents a district observation\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndata.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 10 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   longitude           20640 non-null  float64\n 1   latitude            20640 non-null  float64\n 2   housing_median_age  20640 non-null  float64\n 3   total_rooms         20640 non-null  float64\n 4   total_bedrooms      20433 non-null  float64\n 5   population          20640 non-null  float64\n 6   households          20640 non-null  float64\n 7   median_income       20640 non-null  float64\n 8   median_house_value  20640 non-null  float64\n 9   ocean_proximity     20640 non-null  object \ndtypes: float64(9), object(1)\nmemory usage: 1.6+ MB\n```\n:::\n:::\n\n\nData with 10 columns and 20640 rows\n9 numerical features, 1 categorical feature\n'total_bedrooms' has only 20433 non-null values\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndata.describe(include='all')\t\t# describe all type of data\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>housing_median_age</th>\n      <th>total_rooms</th>\n      <th>total_bedrooms</th>\n      <th>population</th>\n      <th>households</th>\n      <th>median_income</th>\n      <th>median_house_value</th>\n      <th>ocean_proximity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20433.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20640</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>&lt;1H OCEAN</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>9136</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>-119.569704</td>\n      <td>35.631861</td>\n      <td>28.639486</td>\n      <td>2635.763081</td>\n      <td>537.870553</td>\n      <td>1425.476744</td>\n      <td>499.539680</td>\n      <td>3.870671</td>\n      <td>206855.816909</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>2.003532</td>\n      <td>2.135952</td>\n      <td>12.585558</td>\n      <td>2181.615252</td>\n      <td>421.385070</td>\n      <td>1132.462122</td>\n      <td>382.329753</td>\n      <td>1.899822</td>\n      <td>115395.615874</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-124.350000</td>\n      <td>32.540000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>1.000000</td>\n      <td>0.499900</td>\n      <td>14999.000000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-121.800000</td>\n      <td>33.930000</td>\n      <td>18.000000</td>\n      <td>1447.750000</td>\n      <td>296.000000</td>\n      <td>787.000000</td>\n      <td>280.000000</td>\n      <td>2.563400</td>\n      <td>119600.000000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>-118.490000</td>\n      <td>34.260000</td>\n      <td>29.000000</td>\n      <td>2127.000000</td>\n      <td>435.000000</td>\n      <td>1166.000000</td>\n      <td>409.000000</td>\n      <td>3.534800</td>\n      <td>179700.000000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>-118.010000</td>\n      <td>37.710000</td>\n      <td>37.000000</td>\n      <td>3148.000000</td>\n      <td>647.000000</td>\n      <td>1725.000000</td>\n      <td>605.000000</td>\n      <td>4.743250</td>\n      <td>264725.000000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>-114.310000</td>\n      <td>41.950000</td>\n      <td>52.000000</td>\n      <td>39320.000000</td>\n      <td>6445.000000</td>\n      <td>35682.000000</td>\n      <td>6082.000000</td>\n      <td>15.000100</td>\n      <td>500001.000000</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfor ft in data.select_dtypes('object'):\t\t# choose 'object' features only\n\tprint(data[ft].value_counts())\n\tprint(f'Number of classes: {data[ft].nunique()}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nocean_proximity\n<1H OCEAN     9136\nINLAND        6551\nNEAR OCEAN    2658\nNEAR BAY      2290\nISLAND           5\nName: count, dtype: int64\nNumber of classes: 5\n```\n:::\n:::\n\n\nQuite imbalanced classes\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n## Plot histogram of numerical features\n\nimport matplotlib.pyplot as plt\n\ndata.hist(bins=50, figsize=(8,8))\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](project_files/figure-html/cell-7-output-1.png){width=660 height=653}\n:::\n:::\n\n\n*housing_median_age*: capped at range (1,52)\\\n*median_income*: scaled and capped at range ~ (0.5, 15)\\\n*median_house_value*: capped at top range of 500,000\\\nThese features are `skewed` and have very `different scales` => Transforming and Feature Scaling\n\n\n### Create a Test Set: train_test_split()\n`random sampling`: data must be large enough, otherwise there is a risk of sampling bias\n`stratified sampling`: based on some very important features, help avoid *bias* and ensure train set and test set are *representative* to full dataset\n\n*Here we assump that median_income is very important feature to predict household value*\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n## Create new feature (income group)\nimport numpy as np\n\ndata['income_grp'] = pd.cut(data['median_income'], bins=[0,1.5,3,4.5,6,np.inf], labels=[1,2,3,4,5])\n\ndata.income_grp.value_counts().sort_index().plot.bar(rot=0, grid=True)\nplt.title('Frequency of income group')\nplt.xlabel('Income group')\nplt.ylabel('Frequency')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](project_files/figure-html/cell-8-output-1.png){width=602 height=449}\n:::\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n## Split data into train, val, test set\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(data.drop('median_house_value', axis=1), data.median_house_value, stratify=data['income_grp'], test_size=0.2, random_state=24)\n```\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n## Drop the 'income_grp' after using\n\nfor df in [x_train, x_test]:\n\tdf.drop('income_grp', axis=1, inplace=True)\n```\n:::\n\n\n## Explore and visualize the data to gain insights\n\nIf the data is very large, we will sample an exploration set to manipulate faster and easier. On the other hand, just work directly on full set if the data is quite small\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\ndf_vis = data.copy()\n```\n:::\n\n\n### Visualize\n\n**Plot**\n\n::: {.cell fig-width='2' execution_count=11}\n\n::: {.cell-output .cell-output-display}\n![The geographical plot of data](project_files/figure-html/fig-geoplot-output-1.png){#fig-geoplot width=544 height=508 fig-align='center'}\n:::\n:::\n\n\nLarge point size: larger population\nBlue -> red: higher house price\n\n**Correlation**\n\nThere are 2 ways to perform: heatmap and pairgrid map\n\n::: {#fig-corr .cell fig-width='2' execution_count=12}\n\n::: {#fig-corr-1 .cell-output .cell-output-display execution_count=12}\n```\n<Axes: >\n```\n\nCorrelation Plot\n:::\n\n::: {.cell-output .cell-output-display}\n![](project_files/figure-html/fig-corr-output-2.png){#fig-corr-2 width=681 height=544 fig-align='center'}\n:::\n:::\n\n\n::: {.cell fig-width='1' execution_count=13}\n\n::: {.cell-output .cell-output-display}\n![PairGrid Plot of numerical features](project_files/figure-html/fig-pairgrid-output-1.png){#fig-pairgrid width=962 height=938 fig-align='center'}\n:::\n:::\n\n\n*Try pd.plotting.scatter_matrix*\n\nLook closely too the relation between 'median_house_value' and 'median_income', we see there is a strong positive correlation, but there are some clearly horizontal line at 500,000; 450,000; 350,000 and roughly 280,000. We should remove these instances to prevent the algorithms learning these patterns.\n\n::: {.cell execution_count=14}\n\n::: {.cell-output .cell-output-display}\n![Median income versus median house value](project_files/figure-html/fig-corr2-output-1.png){#fig-corr2 width=696 height=509 fig-align='center'}\n:::\n:::\n\n\n### Attributes combination\n\nUseful when we want to find better features to predict\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\ndf_vis['room_per_house'] = df_vis['total_rooms']/df_vis['households']\ndf_vis['bedroom_ratio'] = df_vis['total_bedrooms']/df_vis['total_rooms']\ndf_vis['people_per_house'] = df_vis['population']/df_vis['households']\n\ncorr_matrix = df_vis.corr(numeric_only=True)\ncorr_matrix['median_house_value'].sort_values(ascending=False)\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\nmedian_house_value    1.000000\nmedian_income         0.688075\nroom_per_house        0.151948\ntotal_rooms           0.134153\nhousing_median_age    0.105623\nhouseholds            0.065843\ntotal_bedrooms        0.049686\npeople_per_house     -0.023737\npopulation           -0.024650\nlongitude            -0.045967\nlatitude             -0.144160\nbedroom_ratio        -0.255880\nName: median_house_value, dtype: float64\n```\n:::\n:::\n\n\n",
    "supporting": [
      "project_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}