{
  "hash": "646766258e3f136b30c8ce9320537ec2",
  "result": {
    "markdown": "---\ntitle: Hands-on Classification\n---\n\n\n\n\n\n## Describe the used dataset\n\n- Name: MNIST\n- Author: Yann LeCun, Corinna Cortes, Christopher J.C. Burges\n- Content: 70,000 images of digits handwritten\n- Source: [MNIST Website](http://yann.lecun.com/exdb/mnist/)\n\n\n## Get data\n\n### Download data\n\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom sklearn.datasets import fetch_openml\n\nmnist = fetch_openml('mnist_784', as_frame=False)       # as_frame=False: get data as Numpy Array instead of Pandas DataFrame\nmnist.DESCR\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/usr/local/anaconda3/envs/dhuy/lib/python3.11/site-packages/sklearn/datasets/_openml.py:1022: FutureWarning:\n\nThe default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\n\"**Author**: Yann LeCun, Corinna Cortes, Christopher J.C. Burges  \\n**Source**: [MNIST Website](http://yann.lecun.com/exdb/mnist/) - Date unknown  \\n**Please cite**:  \\n\\nThe MNIST database of handwritten digits with 784 features, raw data available at: http://yann.lecun.com/exdb/mnist/. It can be split in a training set of the first 60,000 examples, and a test set of 10,000 examples  \\n\\nIt is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image. It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting. The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.  \\n\\nWith some classification methods (particularly template-based methods, such as SVM and K-nearest neighbors), the error rate improves when the digits are centered by bounding box rather than center of mass. If you do this kind of pre-processing, you should report it in your publications. The MNIST database was constructed from NIST's NIST originally designated SD-3 as their training set and SD-1 as their test set. However, SD-3 is much cleaner and easier to recognize than SD-1. The reason for this can be found on the fact that SD-3 was collected among Census Bureau employees, while SD-1 was collected among high-school students. Drawing sensible conclusions from learning experiments requires that the result be independent of the choice of training set and test among the complete set of samples. Therefore it was necessary to build a new database by mixing NIST's datasets.  \\n\\nThe MNIST training set is composed of 30,000 patterns from SD-3 and 30,000 patterns from SD-1. Our test set was composed of 5,000 patterns from SD-3 and 5,000 patterns from SD-1. The 60,000 pattern training set contained examples from approximately 250 writers. We made sure that the sets of writers of the training set and test set were disjoint. SD-1 contains 58,527 digit images written by 500 different writers. In contrast to SD-3, where blocks of data from each writer appeared in sequence, the data in SD-1 is scrambled. Writer identities for SD-1 is available and we used this information to unscramble the writers. We then split SD-1 in two: characters written by the first 250 writers went into our new training set. The remaining 250 writers were placed in our test set. Thus we had two sets with nearly 30,000 examples each. The new training set was completed with enough examples from SD-3, starting at pattern # 0, to make a full set of 60,000 training patterns. Similarly, the new test set was completed with SD-3 examples starting at pattern # 35,000 to make a full set with 60,000 test patterns. Only a subset of 10,000 test images (5,000 from SD-1 and 5,000 from SD-3) is available on this site. The full 60,000 sample training set is available.\\n\\nDownloaded from openml.org.\"\n```\n:::\n:::\n\n\n### Quick Look\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n## Size of dataset\n\nX,y = mnist.data, mnist.target\nprint(X.shape, y.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(70000, 784) (70000,)\n```\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n## Quick look\n\nimport matplotlib.pyplot as plt\n\ndef plot_digit(data):\n    image = data.reshape(28,28)\n    plt.imshow(image, cmap='binary')   # binary: grayscale color map from 0 (white) to 255 (black)\n    \nsome_digit = X[0]    # Look at first digit\nplot_digit(some_digit)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hands-on-classification_files/figure-html/cell-4-output-1.png){width=415 height=411}\n:::\n:::\n\n\n### Create train, test set\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n## Split dataset into train set and test set as its describe (train: first 60000 images, test: last 10000 images)\n\nX_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\nprint(X_train.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(60000, 784)\n```\n:::\n:::\n\n\n## Create a Binary Classfier(5 or non-5)\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n## Target labels\n\ny_train_5 = (y_train == '5')\ny_test_5 = (y_test == '5')\n```\n:::\n\n\n### Stochastic Gradient Descent\n\n\n#### Train model\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.linear_model import SGDClassifier\n\nsgd_clf = SGDClassifier(random_state=42)\nsgd_clf.fit(X_train, y_train_5)\n\nsgd_clf.predict([some_digit])\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\narray([ True])\n```\n:::\n:::\n\n\n#### Evaluate model\n\n::: {.callout-tip}\nMetrics:\\\n    - Accuracy\\\n    - Confusion matrix: Precision, Recall (TPR), FPR, ROC, ROC AUC\\\n    - Plot: Precision-Recall Curve, ROC Curve\\\nUse case:\\\n    - Precision-Recall Curve: aim to care more about `false positives` than the `false negatives`\\\n    - Otherwise: ROC Curve\n:::\n\n**Accuracy**\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.model_selection import cross_val_score\n\ncross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring='accuracy')\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\narray([0.95035, 0.96035, 0.9604 ])\n```\n:::\n:::\n\n\n::: {.callout-warning}\nThe accuracy scores are pretty good, but it may be due to the class imbalance. Let take a look at a Dummy Model which always classify as the most frequent class\n:::\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n## Dummy classifier\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import cross_val_score\n\ndummy_model = DummyClassifier(random_state=248)\ncross_val_score(dummy_model, X_train, y_train_5, cv=3, scoring='accuracy')\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\narray([0.90965, 0.90965, 0.90965])\n```\n:::\n:::\n\n\n::: {.callout-important}\nThe accuracy scores are over 90% because there's only about 10% of training set are 5 digit\\\n=> With class imbalance, accuracy score is not a useful metric\\\n=> We will use other metrics such as Precision, Recall, ROC Curve, AUC\n:::\n\n**Confusion Matrix**\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\n\ny_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\nconfusion_matrix(y_train_5, y_train_pred)\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\narray([[53892,   687],\n       [ 1891,  3530]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n## Precision and Recall\n\nfrom sklearn.metrics import precision_score, recall_score\n\nprint(f'Precision scores: {precision_score(y_train_5, y_train_pred):.4f}')\nprint(f'Recall scores: {recall_score(y_train_5, y_train_pred):.4f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPrecision scores: 0.8371\nRecall scores: 0.6512\n```\n:::\n:::\n\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n## F1-score\n\nfrom sklearn.metrics import f1_score\n\nprint(f'F1-score: {f1_score(y_train_5, y_train_pred):.4f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nF1-score: 0.7325\n```\n:::\n:::\n\n\n**Precision-Recall Trade-off**\n\n- Compute the scores of all instances in the training using *decision_function*\n- Change the threshold to see the difference\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ny_score = sgd_clf.decision_function([some_digit])\n\nthreshold = [0, 1000, 3000]\nfor thr in threshold:\n    print(f'With threshold of {thr:4d}: predicted value is {y_score>thr}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWith threshold of    0: predicted value is [ True]\nWith threshold of 1000: predicted value is [ True]\nWith threshold of 3000: predicted value is [False]\n```\n:::\n:::\n\n\n::: {.callout-important}\n**How to choose the suitable threshold?**\n\n- Use Precision-Recall Curve\n- precision_recall_curve: require *scores* computed from decision_function or *probabilities* from predict_proba\n:::\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n## Precision-Recall Curve\n\n\n### Compute scores by decision_function\n\ny_scores = cross_val_predict(sgd_clf, X_train, y_train_5, method='decision_function')\n\n### Plot Precision-Recall Curve vs Threshold\n\nfrom sklearn.metrics import precision_recall_curve\n\nprecisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n\nplt.plot(thresholds, precisions[:-1], label='Precision', color='darkslateblue')\nplt.plot(thresholds, recalls[:-1], label='Recall', color='crimson')\nplt.grid()\nplt.legend(loc='center left')\nplt.xlim([-100000,40000])\nplt.title('Precision and Recall versus Threshold')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hands-on-classification_files/figure-html/cell-14-output-1.png){width=592 height=431}\n:::\n:::\n\n\n::: {.callout-note}\nThe higher Precision, the lower Recall and vice versa\n:::\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\n## Plot Precision versus Recall\n\nplt.plot(recalls, precisions)\nplt.title('Precision versus Recall')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.grid()\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hands-on-classification_files/figure-html/cell-15-output-1.png){width=589 height=449}\n:::\n:::\n\n\n::: {.callout-tip}\nDepend on your project, you would trade between precision and recall\n:::\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\n## Find Threshold of over 0.90 Precision\n\nidx_90_precision = (precisions >= 0.90).argmax()\nthreshold_90_precision = thresholds[idx_90_precision]\nthreshold_90_precision\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\n3045.9258227053647\n```\n:::\n:::\n\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\ny_train_90_precision = (y_scores > threshold_90_precision)\n\nfrom sklearn.metrics import accuracy_score\nprint(f'Accuracy score: {accuracy_score(y_train_5, y_train_90_precision):.4f}')\nprint(f'Precision score: {precision_score(y_train_5, y_train_90_precision):.4f}')\nprint(f'Recall score: {recall_score(y_train_5, y_train_90_precision):.4f}')\nprint(f'F1 score: {f1_score(y_train_5, y_train_90_precision):.4f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy score: 0.9626\nPrecision score: 0.9002\nRecall score: 0.6587\nF1 score: 0.7608\n```\n:::\n:::\n\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\n## ROC AUC\n\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\nprint(f'AUC score: {roc_auc_score(y_train_5, y_scores):.4f}')       \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAUC score: 0.9648\n```\n:::\n:::\n\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\n## ROC Curve\n\nfpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\nidx_threshold_90_precision = (thresholds<=threshold_90_precision).argmax()      # thresholds listed decreasing => use (<=)\nfpr_90, tpr_90 = fpr[idx_threshold_90_precision], tpr[idx_threshold_90_precision]\n\nplt.plot(fpr, tpr, label='ROC Curve', color='darkslateblue')\nplt.plot([fpr_90], [tpr_90], 'o', label='Threshold for 90% precision', color='crimson')\nplt.title('ROC Curve')\nplt.xlabel('False Positive Rate (Fall-out)')\nplt.ylabel('True Positive Rate (Recall)')\nplt.legend(loc='center right')\nplt.grid()\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hands-on-classification_files/figure-html/cell-19-output-1.png){width=589 height=449}\n:::\n:::\n\n\n::: {.callout-important}\nAnother trade-off: The higher TPR, the lower FPR and vice versa\n:::\n\n### Logistic Regression\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LogisticRegression\n\nlogistic = LogisticRegression(random_state=29)\n\ny_pred_logis = cross_val_predict(logistic, X_train, y_train_5, cv=3, method='predict_proba')[:,1]\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/usr/local/anaconda3/envs/dhuy/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning:\n\nlbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n\n/usr/local/anaconda3/envs/dhuy/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning:\n\nlbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n\n/usr/local/anaconda3/envs/dhuy/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning:\n\nlbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n\n```\n:::\n:::\n\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\n## Measure performance\n\nthreshold = 0.5\nf1_logis = f1_score(y_train_5, y_pred_logis>=threshold)\nauc_logis = roc_auc_score(y_train_5, y_pred_logis>=threshold)\n\nprint(f'F1 score Random Forest: {f1_logis:.4f}')\nprint(f'AUC Random Forest: {auc_logis:.4f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nF1 score Random Forest: 0.8487\nAUC Random Forest: 0.9004\n```\n:::\n:::\n\n\n### Random Forest\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier(random_state=42)\n\ny_train_pred_rf = cross_val_predict(rf_clf, X_train, y_train_5, cv=3, method='predict_proba')[:,1]\n```\n:::\n\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\n## Measure performance\n\nthreshold = 0.5\nf1_rf = f1_score(y_train_5, y_train_pred_rf>=threshold)\nauc_rf = roc_auc_score(y_train_5, y_train_pred_rf>=threshold)\n\nprint(f'F1 score Random Forest: {f1_rf:.4f}')\nprint(f'AUC Random Forest: {auc_rf:.4f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nF1 score Random Forest: 0.9275\nAUC Random Forest: 0.9358\n```\n:::\n:::\n\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\n## PR Curve\n\nprecisions_rf, recalls_rf, thresholds_rf = precision_recall_curve(y_train_5, y_train_pred_rf)\n\nplt.plot(recalls, precisions, \"-\", label='SGD')\nplt.plot(recalls_rf, precisions_rf, label='Random Forest')\nplt.title('Precision versus Recall')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.legend()\nplt.grid()\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hands-on-classification_files/figure-html/cell-24-output-1.png){width=589 height=449}\n:::\n:::\n\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\n## ROC Curve\n\nfpr_rf, tpr_rf, thresholds = roc_curve(y_train_5, y_train_pred_rf)\n\nplt.plot(fpr, tpr, label='SGD', color='darkslateblue')\nplt.plot(fpr_rf, tpr_rf, label='Random Forest', color='crimson')\nplt.title('ROC Curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.grid()\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hands-on-classification_files/figure-html/cell-25-output-1.png){width=589 height=449}\n:::\n:::\n\n\n## Multiclass Classification\n\n- LogisticRegression, RandomForestClassifier, GaussianNB: *natively* handle Multiclass Classification\n\n- SGDClassifier and SVC: *strictly* binary classifiers\n    - `ovo`: one versus one strategy, preferred with scale poorly algorithms (i.e. SVC)\n    - `ovr`: one versus rest strategy, preferred for almost algorithms\n\n\n### SVC\n\n#### Default: ovo strategy\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\nfrom sklearn.svm import SVC\n\nsvc_clf = SVC(random_state=42)\nsvc_clf.fit(X_train[:1000], y_train[:1000])\nsvc_clf.predict([some_digit])\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\narray(['5'], dtype=object)\n```\n:::\n:::\n\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\n## Scores from decision_function\n\nsome_digit_svc = svc_clf.decision_function([some_digit])\nsome_digit_svc.round(4)\n```\n\n::: {.cell-output .cell-output-display execution_count=26}\n```\narray([[ 1.7583,  2.7496,  6.1381,  8.2854, -0.2873,  9.3012,  0.7423,\n         3.7926,  7.2085,  4.8576]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\n## Class of highest score\n\nidx_svc = some_digit_svc.argmax()\nidx_svc\n```\n\n::: {.cell-output .cell-output-display execution_count=27}\n```\n5\n```\n:::\n:::\n\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\n## Classes of prediction\nsvc_clf.classes_[idx_svc]\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```\n'5'\n```\n:::\n:::\n\n\n#### Force: ovr strategy\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\n## Train model\n\nfrom sklearn.multiclass import OneVsRestClassifier\n\novr_svc_clf = OneVsRestClassifier(SVC(random_state=42))\novr_svc_clf.fit(X[:1000], y_train[:1000])\novr_svc_clf.predict([some_digit])\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```\narray(['5'], dtype='<U1')\n```\n:::\n:::\n\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\n## Compute scores\n\nsome_digit_ovr_svc = ovr_svc_clf.decision_function([some_digit])\nsome_digit_ovr_svc.round(4)\n```\n\n::: {.cell-output .cell-output-display execution_count=30}\n```\narray([[-1.3439, -1.5195, -1.221 , -0.9294, -2.0057,  0.6077, -1.6226,\n        -0.9998, -1.2764, -1.7031]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\n## Class of hishest score\n\nsome_digit_ovr_svc.argmax()\n```\n\n::: {.cell-output .cell-output-display execution_count=31}\n```\n5\n```\n:::\n:::\n\n\n::: {.cell execution_count=32}\n``` {.python .cell-code}\n## Extract classes\n\novr_svc_clf.classes_\n```\n\n::: {.cell-output .cell-output-display execution_count=32}\n```\narray(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype='<U1')\n```\n:::\n:::\n\n\n### SGD\n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\n## Train model\n\nfrom sklearn.linear_model import SGDClassifier\n\nsgd_clf = SGDClassifier(random_state=42)\nsgd_clf.fit(X_train, y_train)\nsgd_clf.predict([some_digit])\n```\n\n::: {.cell-output .cell-output-display execution_count=33}\n```\narray(['3'], dtype='<U1')\n```\n:::\n:::\n\n\nThat's incorrect. As we can see,The Classifier is not very confident about its prediction. \n\n::: {.cell execution_count=34}\n``` {.python .cell-code}\n## Compute scores\n\nsgd_clf.decision_function([some_digit])\n```\n\n::: {.cell-output .cell-output-display execution_count=34}\n```\narray([[-31893.03095419, -34419.69069632,  -9530.63950739,\n          1823.73154031, -22320.14822878,  -1385.80478895,\n        -26188.91070951, -16147.51323997,  -4604.35491274,\n        -12050.767298  ]])\n```\n:::\n:::\n\n\nWe will use cross validation to evaluate our model\n\n::: {.cell execution_count=35}\n``` {.python .cell-code}\ncross_val_score(sgd_clf, X_train, y_train, cv=3, scoring='accuracy')\n```\n\n::: {.cell-output .cell-output-display execution_count=35}\n```\narray([0.87365, 0.85835, 0.8689 ])\n```\n:::\n:::\n\n\nWe can scale the data to get better result\n\n::: {.cell execution_count=36}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train.astype('float64'))\ncross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring='accuracy')\n```\n\n::: {.cell-output .cell-output-display execution_count=36}\n```\narray([0.8983, 0.891 , 0.9018])\n```\n:::\n:::\n\n\nLet's look at the confusion matrix of our prediction\n\n::: {.cell execution_count=37}\n``` {.python .cell-code}\n## Predict using cross_val_predict\n\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\ny_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\n```\n:::\n\n\nConfusion matrix with (right) and without (left) normalization.\n\n::: {.cell execution_count=38}\n``` {.python .cell-code}\nfig,ax = plt.subplots(1,2,figsize=(9, 4))\n\nConfusionMatrixDisplay.from_predictions(y_train, y_train_pred, ax=ax[0])\nax[0].set_title(\"Confusion matrix\")\nConfusionMatrixDisplay.from_predictions(y_train, y_train_pred, ax=ax[1], normalize='true', values_format='.0%')\nax[1].set_title(\"CM normalized by row\")\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hands-on-classification_files/figure-html/cell-39-output-1.png){width=725 height=329}\n:::\n:::\n\n\nIn row #5 and column #8 on the left plot, it's means 10% of true 5s is misclassified as 8s. Kinda hard to see the errors made by model. Therefore, we will put 0 weight on correct prediction (error plot).\n\nConfustion matrix with error normalized by row (left) and by column (right) (normalize=['true','pred'])\n\n::: {.cell execution_count=39}\n``` {.python .cell-code}\nfig,ax = plt.subplots(1,2,figsize=(9, 4))\n\nsample_weight = (y_train != y_train_pred)\n\nConfusionMatrixDisplay.from_predictions(y_train, y_train_pred, ax=ax[0],sample_weight=sample_weight, normalize='true', values_format='.0%')\nax[0].set_title(\"Confusion matrix\")\nConfusionMatrixDisplay.from_predictions(y_train, y_train_pred, ax=ax[1],sample_weight=sample_weight, normalize='pred', values_format='.0%')\nax[1].set_title(\"CM normalized by row\")\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hands-on-classification_files/figure-html/cell-40-output-1.png){width=725 height=329}\n:::\n:::\n\n\nIn row #5 and column #8 on the left plot, it's means 55% of errors made on true 5s is misclassified as 8s.\n\nIn row #5 and column #8 on the right plot, it's means 19% of misclassified 8s are actually 5s.\n\nAnalyzing the made errors can help us gain insights and why the classifier failing\n\n\n\n## Multilabel Classification\n\nOutput is multilabel for each instances. For example, we will classify whether the digit is large (>7) and is odd\n\n### K Nearest Neighbors\n\n::: {.cell execution_count=40}\n``` {.python .cell-code}\n## Train model\n\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsClassifier\n\ny_train_large = (y_train >= '7')\ny_train_odd = (y_train.astype('int8') % 2 == 1)\ny_train_multilabel = np.c_[y_train_large, y_train_odd]\n\nknn = KNeighborsClassifier()\nknn.fit(X_train_scaled, y_train_multilabel)\nknn.predict([some_digit])\n```\n\n::: {.cell-output .cell-output-display execution_count=40}\n```\narray([[False,  True]])\n```\n:::\n:::\n\n\nCompute average F1 score across all labels (equally important)\n\n::: {.cell execution_count=41}\n``` {.python .cell-code}\n## Evaluate model\n\ny_train_pred_knn = cross_val_predict(knn, X_train_scaled, y_train, cv=3)\nf1_score(y_train, y_train_pred_knn, average='macro')\n```\n\n::: {.cell-output .cell-output-display execution_count=41}\n```\n0.9396793112547043\n```\n:::\n:::\n\n\nAnother approach is to give each label a weight equal to its number of instances\n\n::: {.cell execution_count=42}\n``` {.python .cell-code}\nf1_score(y_train, y_train_pred_knn, average='weighted')\n```\n\n::: {.cell-output .cell-output-display execution_count=42}\n```\n0.940171964265114\n```\n:::\n:::\n\n\n### SVC\n\n- SVC does not natively support multilabel classification. Therefore, there are 2 strategies:\n1. Train one model per label. It turns out that it's hard to capture the dependencies between labels\n2. Train models sequentially (ChainClassifier): using input features and all predictions of previous models in the chain\n\n::: {.cell execution_count=43}\n``` {.python .cell-code}\nfrom sklearn.multioutput import ClassifierChain\n\nchain_clf = ClassifierChain(SVC(), cv=3, random_state=42)\nchain_clf.fit(X_train_scaled[:2000], y_train_multilabel[:2000])\nchain_clf.predict([some_digit])\n```\n\n::: {.cell-output .cell-output-display execution_count=43}\n```\narray([[0., 1.]])\n```\n:::\n:::\n\n\n## Multioutput Classification\n\n- Multiclass-multilabel classification\n- For example, we will build a model that removes noise from an digit image\n- Output is a clean image 28x28: multilabel (one label per pixel) and multiclass (pixel intensity range from 0-255 per label)\n\n::: {.cell execution_count=44}\n``` {.python .cell-code}\n## Create a noisy train set\n\nnp.random.seed(42)\n\nnoise = np.random.randint(0,100,(len(X_train), 28*28))\nX_train_noise = X_train + noise\ny_train_noise = X_train\n\nnoise = np.random.randint(0,100,(len(X_test), 28*28))\nX_test_noise = X_test + noise\ny_test_noise = X_test\n```\n:::\n\n\nLet's look at sample images\n\n::: {.cell execution_count=45}\n``` {.python .cell-code}\nplt.subplot(1,2,1)\nplot_digit(X_train_noise[0])\nplt.subplot(1,2,2)\nplot_digit(y_train_noise[0])\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hands-on-classification_files/figure-html/cell-46-output-1.png){width=566 height=279}\n:::\n:::\n\n\n::: {.cell execution_count=46}\n``` {.python .cell-code}\nknn.fit(X_train_noise, y_train_noise)\ny_pred_noise = knn.predict([X_train_noise[0]])\nplot_digit(y_pred_noise)\n```\n\n::: {.cell-output .cell-output-display}\n![](hands-on-classification_files/figure-html/cell-47-output-1.png){width=415 height=411}\n:::\n:::\n\n\n",
    "supporting": [
      "hands-on-classification_files"
    ],
    "filters": [],
    "includes": {}
  }
}