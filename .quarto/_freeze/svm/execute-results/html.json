{
  "hash": "5fd3346d5dd3cfec3a7f91e66383274a",
  "result": {
    "markdown": "# Support Vector Machine {#sec-svm}\n\n\n\nSupport Vector Machine (SVM) is a powerful and versatile machine learning model, capable of performing linear or nonlinear classification, regression, and even novelty detection. It works well with small to medium-sized datasets, but unfortunately do not scale well and sensitive to feature scaling.\n\n::: {.callout-note collapse=\"true\"}\n## Important notes to remember\n- SVMs: fit the largest street while limiting the margin violations, supported by support vector on the street (SVC) or off the street (SVR).\\\n- Hyperparameters: C, gamma ([Really simple explaination](https://www.quora.com/What-are-C-and-gamma-with-regards-to-a-support-vector-machine))\\\n- SVMs: only have `predict` and `decision_function` method.\\\n- Kernel trick: handle non-linear dataset efficiently\\\n- LinearSVC/LinearSVR use *liblinear* library: optimize for linear SVMs, scale well\n- SVC/SVR classes use *libsvm*: support kernel trick, scale badly\n:::\n\n\n\n## Linear SVM Classification (Support Vector Classification - SVC)\n\nSupport Vector Machine (Large Margin Classification): Fitting the widest street between classes, supported by support vector instances *on* the street. \n\n![Large margin classification](images/svm.png){#fig-svm}\n\nSVMs are sensitive to feature scaling. As we can see, SVM seperate the data better with scaled data.\n\n![SVMs are sensitive to feature scaling](images/svm-scaling.png){#fig-feature-scaling}\n\n**Hard margin/Soft margin classification**\\\n- Hard margin: All instances must be off the street, only work with linearly seperable data and sensitive to outliers\\\n- Soft margin: improve weakness of hard margin by allow limiting margin violations\n\n::: {.callout-note}\n**Hyperparameter C**: the penalty on any misclassified data point.\\\n    - High: high penalty, stricter classification, narrower street and tends to overfit\\\n    - Low: low penalty, allow larger number of misclassifications, wider street and tends to underfit\n:::\n\n![Different C parameters](images/parameter-c.png){#fig-parameter-c}\n\n**Implement SVC**\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom sklearn.datasets import load_iris\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\n\ndata = load_iris(as_frame=True)\nX = data.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\ny = (data.target == 2) # Iris virginica\nsome_flower = X[2,:]\nsvc = make_pipeline(StandardScaler(),\nLinearSVC(random_state=29))\nsvc.fit(X, y)\nprint(svc.predict([some_flower]))\nprint(svc.decision_function([some_flower]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[False]\n[-6.34263777]\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/usr/local/anaconda3/envs/dhuy/lib/python3.11/site-packages/sklearn/svm/_classes.py:32: FutureWarning:\n\nThe default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n\n```\n:::\n:::\n\n\n## Non-linear SVM Classification {#sec-nonlinear-svm}\n\nWith non-linearly seperable datasets in low dimensions, we want to transform them to a higher dimension where they will be linearly sepparable. Imagine \"raising\" the green points, then you can sepparate them from the red points with a plane (hyperplane).\n\n![Non-linearly seperable](images/nonlinearly-seperable.png){#fig-nonlinearly-seperable}\n\nTo do that, we can use more complex models (Random forest, etc.) or add more features (Polynomial features, similarity features using Gaussian RBF, etc.), but this will lead to a huge bunch of new features and computationally expensive.\n\nTherefore, SVM supply a powerful technique called `kernel trick`, allow us to get the same result as if add many polynomial/similarity features, without actually having to add them.\n\n**Polynomial kernel**\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.svm import SVC\n\npoly_svc = make_pipeline(StandardScaler(),\nSVC(kernel='poly', degree=3, C=10, coef0=1))\n```\n:::\n\n\n**Gaussian RBF kernel**\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nrbf_svc = make_pipeline(StandardScaler(),\nSVC(kernel='rbf', C=10, gamma=5))\n```\n:::\n\n\n::: {.callout-note}\n- coef0 (poly kernel): controls how much the model is influenced by high-degree terms versus low-degree terms.\\\n- gamma (RBF kernel): high => overfitting, low => underfitting\\\n\n- gamma: controls the shape of the \"peaks\" where you raise the points\n    - High: pointed bump (narrow bell-shaped curve), each instanceâ€™s range of influence is smaller, tend to wiggling around individual instances\n    - Low: softer, broader bump (wide bell-shaped curve), vice versa.\n:::\n\n![Different C and gamma parameters](images/c-gamma.png){#fig-c-gamma}\n\n## SVMs Classes Computational Complexity\n\n![BigO of SVM classification](images/svm-bigO.png){#fig-svm-bigO}\n\n\n## SVM Regression (Support Vector Regression - SVR)\n\nOpposed to SVC, SVR tries to fit as many instances as possible on the street while limiting margin violations (instances *off* the street)\n\nHyperparameter epsilon: control the width of the street\n    - Low: narrow street, more support vector, tend to too complex\n    - High: wide street, less support vector, tend to too simple\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\nfrom sklearn.svm import SVR\n\nnp.random.seed(29)\nm = 100\nX = 6 * np.random.rand(m, 1) - 3\ny = 0.5 * X ** 2 + X + 2 + np.random.randn(m, 1)\n\nsvr = make_pipeline(StandardScaler(),\nSVR(kernel='poly', degree=5, C=0.001, epsilon=0.1))\nsvr.fit(X,y.ravel())\nsvr.predict([[3]])\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\narray([3.31601563])\n```\n:::\n:::\n\n\n![Different epsilons](images/different-epsilon.png){#fig-different-epsilon}\n\n\n## Understand the Fundamentals of SVM\n\n\nTo predict the class of an instance, SVM compute decision function, then compare to the margin of the street to predict.\n\n$$y = Î¸_{0} + Î¸^{Î¤}X$$\n\nSuppose that the margin is (-1,1). With the same margin, to make the wider street, we have to make the Î¸ smaller. \n\n![A smaller weights results in a larger margin](images/small-weights.png){#fig-small-weigths}\n\n\n### Quadratic Programming Problem (QP solver)\n\n**Hard margin classification**\n\nTo avoid the margin violations, we have to minimize the Î¸ while making the decision function â‰¥1 for positive instances and â‰¤-1 for negative instances. This constraint can be written using t = 1 or t = -1 repectively:\n\n$$\n\\begin{gather}\nminimize(Î¸,Î¸_{0})\\;\\;\\frac{1}{2}Î¸^{Î¤}Î¸\\\\\nsubject\\;to\\;\\;t(Î¸_{0} + Î¸^{Î¤}X) â‰¥ 1;\\;\\;t_{i} = [-1;1]\n\\end{gather}\n$$\n\n**Soft margin classification**\n\nTo perform soft margin classification, we add a *slack variable Î¶(i) â‰¥ 0* for each instance: Î¶ measure how much the instance is allowed to violate the margin.\n\nExpectedly, we want to keep Î¶ as small as possible to reduce margin violations, but we also want the margin as wide as possible (too greedy ðŸ˜†). Don't worry, this is where the C parameter comes into play.\n\n$$\n\\begin{gather}\nminimize(Î¸,Î¸_{0})\\;\\;\\frac{1}{2}Î¸^{Î¤}Î¸ + CÎ¶\\\\\nsubject\\;to\\;\\;t(Î¸_{0} + Î¸^{Î¤}X) â‰¥ 1 - Î¶;\\;\\;t_{i} = [-1;1];\\;Î¶_{i}â‰¥0\n\\end{gather}\n$$\n\n\n### Gradient Descent\n\nCost function: `hinge loss` or the `squared hinge loss` (*loss* hyperparameter)\n\nDecision function:\\\n    - â‰¥ 1: true label is positive => loss = 0\\\n    - â‰¤-1: true label is negative => loss = 0\n\nBy default: LinearSVC use squared hinge loss, while SGDClassifier use hinge loss\n\n![The hinge loss and squared hinge loss](images/hinge-loss.png){#fig-hinge-loss}\n\n\n### Kernelized SVMs\n\nAs mentioned in @sec-nonlinear-svm, when we want to perform on more complex model like polynomial or RBF, kernel trick can compute the dot product in the minimization work directly on the original vectors a and b, without even know about the transformation. The @fig-kernel-trick illustrate the kernel trick for a second-degree polynomial \n\n![Kernel trick for a second-degree polynomial](images/kernel-trick.png){#fig-kernel-trick}\n\nThese are the common kernels, in which K is the kernel function:\n\n![Common kernels](images/common-kernels.png){#fig-common-kernels}\n\n- d: degree\n- r: coef0\n- Î³: gamma, â‰¥ 0\n\n\n::: {.callout-important}\nLearn more about Dual problem, equation to make predictions with kernel trick\n:::\n\n",
    "supporting": [
      "svm_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}