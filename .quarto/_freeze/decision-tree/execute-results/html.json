{
  "hash": "67c6a57348252742976b0abcb329ee2f",
  "result": {
    "markdown": "# Decision Tree {#sec-decision-tree}\n\n\n\nDecision trees are versatile machine learning algorithms that can perform both classification and regression tasks, and even multioutput tasks. It works by recursively partitioning the data into subsets based on **statements** of features, making decisions whether or not the statement is **True** or **False**.\n\n\n::: {.callout-note collapse=\"true\"}\n## Important notes to remember\n- Handle both numerical and categorical data.\\\n- Do not require feature scaling.\\\n- The algorithm selects the best feature at each decision point, aiming to maximize information gain (for classification) or variance reduction (for regression). Decision trees are interpretable, easy to visualize, and can handle both numerical and categorical data. However, they may be prone to overfitting, which can be addressed using techniques like pruning.\\\n- Ensemble methods, such as Random Forests and Gradient Boosting, often use multiple decision trees to enhance predictive performance and address the limitations of individual trees.\\\n- Decision Tree *(white box model, interpretable ML)* vs Random Forest, NNs *(black box model)*\n\n- Some DT algorithm:\n\t- ID3, C4.5 (Information Gain/ Information Gain Ratio)\\\n\t- CART (Gini Index)\\\n\t- SLIQ, SPRINT (Gini Index)\\\n:::\n\n\n\n### Training and Visualizing\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n## Train a decision tree\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\n\niris = load_iris(as_frame=True)\nX_train = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\ny_train = iris.target\n\ndt_clf = DecisionTreeClassifier(max_depth=2, random_state=29)\ndt_clf.fit(X_train, y_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=2, random_state=29)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=2, random_state=29)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n## Plot a decision tree\n\nfrom sklearn.tree import export_graphviz        # or export_text\nfrom graphviz import Source\n```\n:::\n\n\n![Decision tree](images/decision-tree.png){#fig-decision-tree}\n\nThe structure of a decision tree:\n\n1. Root node: depth 0, at top\n2. Branch nodes: split data based on statements\n3. Leaf nodes: output, no child nodes \n\n@fig-decision-tree, in a node:\\\n- Samples: count instances in that node\\\n- Value: count instances of each class\\\n- Gini (Gini impurity): a measurement, measure a node is pure (0) or not (-> 1)\n$$G_{i}=1-\\sum_{k=1}^{n}p_{i,k}^{2}$$\n\n\tGi: Gini impurity of node i\\\n\tpi,k: probability of `class k instances` in total instances in node `i`\n\n**Other measurements**:\n\n- Entropy (Entropy impurity) : measure randomness or uncertainty\n$$H_{i} = -∑ p_{i,k}log_2(p_{i,k})$$\n\n- Information Gain = (Entropy before split) - (weighted entropy after split)\\\n- Information Gain Ratio = Information Gain / SplitInfo\\\n- SplitInfo (Split Information): potential worth of splitting a branch from a node\n$$SplitInfo(A) = -∑ p_{k}log_2(p_{k})$$\n\n::: {.callout-note}\nExample:\\\nGi = 1 – (0/54)2 – (49/54)2 – (5/54)2 ≈ 0.168\\\nHi = –(49/54) log2 (49/54) – (5/54) log2 (5/54) ≈ 0.445\n:::\n\n\n- Use cases:\n- `Gini` and `Entropy`: No big difference\n- `Gini` is faster => good default\n- `Gini`: isolate the most frequent class in its own branch\n- `Entropy`: produce slightly more balanced trees\n- `Information Gain` tends to prefer attributes that create many branches (e.g. 12 instances with 12 classes => Entropy = 0)\n- `Information Gain Ratio`: regularize Information Gain\n- Sometimes, attribute A is chosen because its Information Gain Ratio is really low => Set Information Gain threshold\n\n\n### CART algorithm\n\n- sklearn use *CART* algorithm produce *binary tree* (2 children only)\n- Other algorithms such as *ID3* have 2 or more children\n- Algorithm choose *feature k* and *threshold tk* producing purest subsets, weighted by their sizes. CART cost function for `classification`:\n\n$$\nJ_{k, t_k} = \\frac{m_{left}}{m} G_{left} + \\frac{m_{right}}{m}G_{right}\n$$\n\n- G: impurity\\\n- m: number of instances\n\n- CART split training set recursively until: *purest*, *max_depth*, *min_samples_split*, *min_samples_leaf*, *min_weight_fraction_leaf*, and *max_leaf_nodes*. \n - Find optimal tree is `NP-complete problem`, **O(exp(m))** => Have to find 'resonably good' solution when training a decision tree\n - But making predictions is just **O(log2(m))**\n\n\n::: {.callout-tip collapse=\"true\"}\n### NP-complete problem\nP is the set of problems that can be solved in `polynomial time` (i.e., a polynomial of the dataset > size). NP is the set of problems whose solutions can be verified in polynomial time. An `NP-hard` problem is a problem that can be reduced to a known NP-hard problem in polynomial time. An `NP-complete` problem is both NP and NP-hard. A major open mathematical question is whether or not P = NP. If P ≠ NP (which seems likely), then no polynomial algorithm will ever be found for any NP-complete problem (except perhaps one day on a quantum computer).\n:::\n\n\n\n## Regularization\n\n- Parametric model (Linear Regression): predetermined parameters => degree of freedom is limited => reducing the risk of overfitting\n- Non-parametric model: parameters not determined prior to training => go freely => overfitting => **regularization**\n- Increasing `min_*` hyperparameters or reducing `max_*` hyperparameters will **regularize** the model:\n\t- max_depth, max_features, max_leaf_nodes\n\t- min_samples_split, min_samples_leaf, min_weight_fraction_leaf\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.datasets import make_moons\nX_moons, y_moons = make_moons(n_samples=150, noise=0.2, random_state=42)\n\ndt_clf1 = DecisionTreeClassifier(random_state=29)\ndt_clf2 = DecisionTreeClassifier(max_depth=5, min_samples_leaf=5, random_state=29)\ndt_clf1.fit(X_moons, y_moons)\ndt_clf2.fit(X_moons, y_moons)\n\nX_moons_test, y_moons_test = make_moons(n_samples=1000, noise=0.2, random_state=43)\n\nprint(f'Non-regularized decision tree: {dt_clf1.score(X_moons_test, y_moons_test):.4f}')\nprint(f'Regularized decision tree: {dt_clf2.score(X_moons_test, y_moons_test):.4f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNon-regularized decision tree: 0.8940\nRegularized decision tree: 0.9200\n```\n:::\n:::\n\n\n## Pruning Trees\nPruning (deleting) unnecessary nodes\nOther algorithms work by `first training` the decision tree `without restrictions`, `then pruning` (deleting) unnecessary nodes. `A node whose children are all leaf nodes` is considered `unnecessary` if the `purity improvement` it provides is `not statistically significant`. Standard statistical tests, such as the `χ2 test` (chi-squared test), are used to estimate the probability that the improvement is purely the result of chance (which is called the null hypothesis). If this probability, called the p-value, is higher than a given threshold (typically 5%, controlled by a hyperparameter), then the node is considered unnecessary and `its children are deleted`. The pruning continues until all unnecessary nodes have been pruned.\n\nThere are 3 types of Pruning Trees\\\n- Pre-Tuning\n- Post-Tuning\n- Combines\n\n\n\n## Regression\n\n- *DecisionTreeRegressor* splits each region in a way that makes most training instances `as close as possible` to that predicted value.\n- CART cost function for `regression`:\n$$\nJ_{k, t_k} = \\frac{m_{left}}{m} MSE_{left} + \\frac{m_{right}}{m} MSE_{right}\n$$\n\nMSE: mean squared error\nm: number of instances\n\n![[Pasted image 20240116111255.png]]\n![[Pasted image 20240116111728.png]]\n\n\n\n## Limitations of Decision Tree\n\n- Decision tree tends to make `orthogonal` decision boudaries => Sensitive to the data’s orientation\n\t- Solution: *Scale data* -> *PCA*: reduce dimensions but do not loss too much information, rotate data to reduce correlation between features\n- High variance: **randomly** select set of features to evaluate at each node => high variance, unstable\n\t- Solution: Ensemble methods (Random Forest, Boosting methods) averaging predictions over many trees\n\n",
    "supporting": [
      "decision-tree_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}