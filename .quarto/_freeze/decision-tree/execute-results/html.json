{
  "hash": "f66946da7b3d1b8a307069965b33b5f2",
  "result": {
    "markdown": "---\ntitle: Decision Tree\n---\n\n\n\n\n\nDecision trees are versatile machine learning algorithms that can perform both classification and regression tasks, and even multioutput tasks. It works by recursively partitioning the data into subsets based on **statements** of features, making decisions whether or not the statement is **True** or **False**.\n\n\n::: {.callout-note collapse=\"true\"}\n## Important notes to remember\n- Handle both numerical and categorical data.\\\n- Do not require feature scaling.\\\n- The algorithm selects the best feature at each decision point, aiming to maximize information gain (for classification) or variance reduction (for regression). Decision trees are interpretable, easy to visualize, and can handle both numerical and categorical data. However, they may be prone to overfitting, which can be addressed using techniques like pruning.\\\n- Ensemble methods, such as Random Forests and Gradient Boosting, often use multiple decision trees to enhance predictive performance and address the limitations of individual trees.\\\n- Decision Tree *(white box model, interpretable ML)* vs Random Forest, NNs *(black box model)*\n\n- Some DT algorithm:\n\t- ID3, C4.5 (Information Gain/ Information Gain Ratio)\\\n\t- CART (Gini Index)\\\n\t- SLIQ, SPRINT (Gini Index)\\\n:::\n\n\n\n### Training and Visualizing\n\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n## Train a decision tree\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\n\niris = load_iris(as_frame=True)\nX_iris = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\ny_iris = iris.target\nsome_flower = X_iris[100,:]\n\ndt_clf = DecisionTreeClassifier(max_depth=2, random_state=29)\ndt_clf.fit(X_iris, y_iris)\nprint(dt_clf.predict([some_flower]))\nprint(dt_clf.predict_proba([some_flower]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[2]\n[[0.         0.02173913 0.97826087]]\n```\n:::\n:::\n\n\n![Decsion tree decision boudaries](images/decision-tree1.png){#fig-decision-tree}\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n## Plot a decision tree\n\nfrom sklearn.tree import plot_tree\n\nplot_tree(dt_clf, feature_names=[\"petal length (cm)\", \"petal width (cm)\"], class_names=iris.target_names)\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\n[Text(0.4, 0.8333333333333334, 'petal width (cm) <= 0.8\\ngini = 0.667\\nsamples = 150\\nvalue = [50, 50, 50]\\nclass = setosa'),\n Text(0.2, 0.5, 'gini = 0.0\\nsamples = 50\\nvalue = [50, 0, 0]\\nclass = setosa'),\n Text(0.6, 0.5, 'petal width (cm) <= 1.75\\ngini = 0.5\\nsamples = 100\\nvalue = [0, 50, 50]\\nclass = versicolor'),\n Text(0.4, 0.16666666666666666, 'gini = 0.168\\nsamples = 54\\nvalue = [0, 49, 5]\\nclass = versicolor'),\n Text(0.8, 0.16666666666666666, 'gini = 0.043\\nsamples = 46\\nvalue = [0, 1, 45]\\nclass = virginica')]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](decision-tree_files/figure-html/cell-3-output-2.png){width=540 height=389}\n:::\n:::\n\n\n![Decision tree](images/decision-tree.png){#fig-decision-tree}\n\nThe structure of a decision tree:\n\n1. Root node: depth 0, at top\n2. Branch nodes: split data based on statements\n3. Leaf nodes: output, no child nodes \n\n@fig-decision-tree, in a node:\\\n- Samples: count instances in that node\\\n- Value: count instances of each class\\\n- Gini (Gini impurity): a measurement, measure a node is pure (0) or not (-> 1)\n$$G_{i}=1-\\sum_{k=1}^{n}p_{i,k}^{2}$$\n\n\tGi: Gini impurity of node i\\\n\tpi,k: probability of `class k instances` in total instances in node `i`\n\n**Other measurements**:\n\n- Entropy (Entropy impurity) : measure randomness or uncertainty\n$$H_{i} = -∑ p_{i,k}log_2(p_{i,k})$$\n\n- Information Gain = (Entropy before split) - (weighted entropy after split)\\\n- Information Gain Ratio = Information Gain / SplitInfo\\\n- SplitInfo (Split Information): potential worth of splitting a branch from a node\n$$SplitInfo(A) = -∑ p_{k}log_2(p_{k})$$\n\n::: {.callout-note}\nExample:\\\nGi = 1 – (0/54)2 – (49/54)2 – (5/54)2 ≈ 0.168\\\nHi = –(49/54) log2 (49/54) – (5/54) log2 (5/54) ≈ 0.445\n:::\n\n\n- Use cases:\n- `Gini` and `Entropy`: No big difference\n- `Gini` is faster => good default\n- `Gini`: isolate the most frequent class in its own branch\n- `Entropy`: produce slightly more balanced trees\n- `Information Gain` tends to prefer attributes that create many branches (e.g. 12 instances with 12 classes => Entropy = 0)\n- `Information Gain Ratio`: regularize Information Gain\n- Sometimes, attribute A is chosen because its Information Gain Ratio is really low => Set Information Gain threshold\n\n\n### CART algorithm\n\n- sklearn use *CART* algorithm produce *binary tree* (2 children only)\n- Other algorithms such as *ID3* have 2 or more children\n- Algorithm choose *feature k* and *threshold tk* producing purest subsets, weighted by their sizes. CART cost function for `classification`:\n\n$$\nJ_{k, t_k} = \\frac{m_{left}}{m} G_{left} + \\frac{m_{right}}{m}G_{right}\n$$\n\n- G: impurity\\\n- m: number of instances\n\n- CART split training set recursively until: *purest*, *max_depth*, *min_samples_split*, *min_samples_leaf*, *min_weight_fraction_leaf*, and *max_leaf_nodes*. \n - Find optimal tree is `NP-complete problem`, **O(exp(m))** => Have to find 'resonably good' solution when training a decision tree\n - But making predictions is just **O(log2(m))**\n\n\n::: {.callout-tip collapse=\"true\"}\n### NP-complete problem\nP is the set of problems that can be solved in `polynomial time` (i.e., a polynomial of the dataset > size). NP is the set of problems whose solutions can be verified in polynomial time. An `NP-hard` problem is a problem that can be reduced to a known NP-hard problem in polynomial time. An `NP-complete` problem is both NP and NP-hard. A major open mathematical question is whether or not P = NP. If P ≠ NP (which seems likely), then no polynomial algorithm will ever be found for any NP-complete problem (except perhaps one day on a quantum computer).\n:::\n\n\n\n## Regularization\n\n- Parametric model (Linear Regression): predetermined parameters => degree of freedom is limited => reducing the risk of overfitting\n- Non-parametric model: parameters not determined prior to training => go freely => overfitting => **regularization**\n- Increasing `min_*` hyperparameters or reducing `max_*` hyperparameters will **regularize** the model:\n\t- max_depth, max_features, max_leaf_nodes\n\t- min_samples_split, min_samples_leaf, min_weight_fraction_leaf\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.datasets import make_moons\n\nX_moons, y_moons = make_moons(n_samples=150, noise=0.2, random_state=42)\n\ndt_clf1 = DecisionTreeClassifier(random_state=29)\ndt_clf2 = DecisionTreeClassifier(max_depth=5, min_samples_leaf=5, random_state=29)\ndt_clf1.fit(X_moons, y_moons)\ndt_clf2.fit(X_moons, y_moons)\n\nX_moons_test, y_moons_test = make_moons(n_samples=1000, noise=0.2, random_state=43)\n\nprint(f'Non-regularized decision tree: {dt_clf1.score(X_moons_test, y_moons_test):.4f}')\nprint(f'Regularized decision tree: {dt_clf2.score(X_moons_test, y_moons_test):.4f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNon-regularized decision tree: 0.8940\nRegularized decision tree: 0.9200\n```\n:::\n:::\n\n\n![Decision boudaries of unregularized tree (left) and regularized tree (right)](images/regularized-decision-tree.png){#fig-regularized-decision-tree}\n\n::: {.callout-important}\n## Pruning Trees\n- Pruning: deleting unnecessary nodes\\\n- Algorithms work by first training the decision tree without restrictions, then pruning (deleting) unnecessary nodes. A node whose children are all leaf nodes is considered unnecessary if the purity improvement it provides is not statistically significant. Standard statistical tests, such as the χ2 test (chi-squared test), are used to estimate the probability that the improvement is purely the result of chance (which is called the null hypothesis). If this probability, called the p-value, is higher than a given threshold (typically 5%, controlled by a hyperparameter), then the node is considered unnecessary and its children are deleted. The pruning continues until all unnecessary nodes have been pruned.\\\n- There are 3 types of Pruning Trees\\\n    - Pre-Tuning\n    - Post-Tuning\n    - Combines\n:::\n\n\n\n## Regression\n\n- *DecisionTreeRegressor* splits each region in a way that makes most training instances `as close as possible` to that predicted value (average of instances in the region)\n- CART cost function for `regression`:\n$$\nJ_{k, t_k} = \\frac{m_{left}}{m} MSE_{left} + \\frac{m_{right}}{m} MSE_{right}\n$$\n\n- MSE: mean squared error\n- m: number of instances\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\n\nnp.random.seed(42)\nX_quad = np.random.rand(200, 1) - 0.5\ny_quad = X_quad ** 2 + 0.025 * np.random.randn(200, 1)\n\ndt_reg = DecisionTreeRegressor(max_depth=2, min_samples_leaf=5, random_state=29)\ndt_reg.fit(X_quad, y_quad)\n\nplot_tree(dt_reg)\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\n[Text(0.5, 0.8333333333333334, 'x[0] <= -0.303\\nsquared_error = 0.006\\nsamples = 200\\nvalue = 0.088'),\n Text(0.25, 0.5, 'x[0] <= -0.408\\nsquared_error = 0.002\\nsamples = 44\\nvalue = 0.172'),\n Text(0.125, 0.16666666666666666, 'squared_error = 0.001\\nsamples = 20\\nvalue = 0.213'),\n Text(0.375, 0.16666666666666666, 'squared_error = 0.001\\nsamples = 24\\nvalue = 0.138'),\n Text(0.75, 0.5, 'x[0] <= 0.272\\nsquared_error = 0.005\\nsamples = 156\\nvalue = 0.065'),\n Text(0.625, 0.16666666666666666, 'squared_error = 0.001\\nsamples = 110\\nvalue = 0.028'),\n Text(0.875, 0.16666666666666666, 'squared_error = 0.002\\nsamples = 46\\nvalue = 0.154')]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](decision-tree_files/figure-html/cell-5-output-2.png){width=540 height=389}\n:::\n:::\n\n\nIf we set *max_depth* larger, the model will predict more strictly. As same as @fig-different-max-depth, if we keep the default hypeparameters, the model will grow as much as it can @fig-regularized-tree.\n\n![Different max_depth](images/different-max-depth.png){#fig-different-max-depth}\n\n![Unregularized tree (left) and regularized tree (right](images/regularized-tree.png){#fig-regularized-tree}\n\n\n## Limitations of Decision Tree\n\n- Decision tree tends to make `orthogonal` decision boudaries.\\ => Sensitive to the data’s orientation.\n\n![Decision tree is sensitive to data's orientation](images/orientation.png){#fig-orientation}\n\n=> **Solution**: Scale data -> PCA: reduce dimensions but do not loss too much information, rotate data to reduce correlation between features, which often (not always) makes things easier for trees.\n\nCompare to @fig-decision-tree, the scaled and PCA-rotated iris dataset is separated easier.\n\n![Scaled and PCA-rotated data](images/scaled-decision-tree.png){#fig-scaled-decision-tree}\n\n- High variance: **randomly** select set of features to evaluate at each node, so that if we retrain model on the same dataset, it can behave really different => high variance, unstable\\\n=> **Solution**: Ensemble methods (Random Forest, Boosting methods) averaging predictions over many trees.\n\n",
    "supporting": [
      "decision-tree_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}