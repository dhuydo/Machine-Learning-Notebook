---
execute:
    freeze: auto
---

# Hands-on regression {#sec-regression}

::: {.callout-tip collapse="true"}
## Main Steps
1. Look at the big picture.
2. Get the data.
3. Explore and visualize the data to gain insights.
4. Prepare the data for machine learning algorithms. 
5. Select a model and train it.
6. Fine-tune model.
7. Present solution.
8. Launch, monitor, and maintain system.
:::



## Datasets

**Popular open data repositories**

- https://openml.org/  
- https://Kaggle.com
- https://PapersWithCode.com  
- [UC Irvine Machine Learning Repository — Amazon’s AWS datasets](https://archive.ics.uci.edu/)
- [TensorFlow datasets](https://www.tensorflow.org/datasets?hl=vi)

**Meta portals (they list open data repositories)**

- https://DataPortals.org  
- https://OpenDataMonitor.eu

**Other pages listing many popular open data repositories**

- [Wikipedia’s list of machine learning datasets](https://en.wikipedia.org/wiki/- List_of_datasets_for_machine-learning_research)
- https://Quora.com
- [The datasets subreddit](https://www.reddit.com/r/datasets/)

In this chapter we’ll use the California Housing Prices dataset from the StatLib repository. This dataset is based on data from the 1990 California census. It is not exactly recent but it has many qualities for learning.


## Look at the big picture

**Questions**

1. Business objective? Current solution (if any)?
2. How to use and benefit from the model? 
3. Data Pipelines?
4. Determine kind of model
5. Select preformance measures
6. Check the Assumptions


**Answers**

- Predict median housing price in any district. The results are used for another ML system for investment analysis. The current solution is to gather up-to-date information about a district, or to estimate manually using complex rules if no information.
- The current solution is costly and time-consuming, and it was often off by more than 30%. Therefore, a ML model could be more useful.
- Data pipelines: A sequence of data processing components. Pipelines are very common in machine learning systems, since there is a lot of data to manipulate and many data transformations to apply.
- This is a regression task (labeled data), batch learning (data is quite small and do not change too much) and model-based learning
- Metrics for regression:

::: {.callout-note collapse="true"}
## Expand to learn more about metrics
Both the RMSE and the MAE are ways to measure the distance between two vectors: the vector of predictions and the vector of target values. Various distance measures, or norms, are possible:\
	- Computing the root of a sum of squares (RMSE) corresponds to the Euclidean norm: this is the notion of distance we are all familiar with. It is also called the l2 norm, noted |·|₂ (or just |·|).\
	- Computing the sum of absolutes (MAE) corresponds to the l1 norm, noted |·|₁. This is sometimes called the Manhattan norm because it measures the distance between two points in a city if you can only travel along orthogonal city blocks.\
	- More generally, the lk norm of a vector v containing n elements is defined as ∥v∥k = (|v₁|ᵏ + |v₂|ᵏ + ... + |vₙ|ᵏ)¹/ᵏ. l0 gives the number of nonzero elements in the vector, and l∞ gives the maximum absolute value in the vector.\
The higher the norm index, the more it focuses on large values and neglects small ones. This is why the RMSE is more sensitive to outliers than the MAE. But when outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs very well and is generally preferred.
![L1 norm and L2 norm](images/l1_l2.png){#fig-l1-l2}
:::

RMSE (root mean squared error): l2 norm\
$$
RMSE(X, y)  = \sqrt{\frac{1}{m}\sum_{i=1}^{m}(y_{hat}^{(i)} - y^{(i)})^2}
$$

MAE (mean squared error): l1 norm\
$$
MAE(X, y)  = \frac{1}{m}\sum_{i=1}^{m}|y_{hat}^{(i)} - y^{(i)}|
$$

- Check with the team in charge of the downstream system that use out output whether it is suitable or not (e.g. it is terrible if after several months building model you realize that they need ordinal output not numerical one)


## Get the Data

### Download Data

```{python}
## Load data

from pathlib import Path
import pandas as pd
import tarfile
import urllib.request

def load_data(url):
	path = Path("datasets/housing.tgz")
	if not path.is_file():
		Path("datasets").mkdir(parents=True, exist_ok=True)
		urllib.request.urlretrieve(url, path)
		tarfile.open(path).extractall(path='datasets')
	return pd.read_csv('datasets/housing/housing.csv')

url = 'https://github.com/ageron/data/raw/main/housing.tgz'
data = load_data(url)
```

### Quick Look to Data Structure: head(), info(), describe(), value_counts(), histplot()

```{python}
## Quick look

pd.set_option('display.max_columns', None)		# display all columns
data.head()
```

There are total 10 features, each row represents a district observation

```{python}
data.info()
```

Data with 10 columns and 20640 rows
9 numerical features, 1 categorical feature
'total_bedrooms' has only 20433 non-null values

```{python}
data.describe(include='all')		# describe all type of data
```

```{python}
for ft in data.select_dtypes('object'):		# choose 'object' features only
	print(data[ft].value_counts())
	print(f'Number of classes: {data[ft].nunique()}')
```

Quite imbalanced classes

```{python}
## Plot histogram of numerical features

import matplotlib.pyplot as plt

data.hist(bins=50, figsize=(8,8))
plt.show()
```

*housing_median_age*: capped at range (1,52)\
*median_income*: scaled and capped at range ~ (0.5, 15)\
*median_house_value*: capped at top range of 500,000\
These features are `skewed` and have very `different scales` => Transforming and Feature Scaling


### Create train, val and test set: train_test_split()
`random sampling`: data must be large enough, otherwise there is a risk of sampling bias
`stratified sampling`: based on some very important features, help avoid *bias* and ensure train set and test set are *representative* to full dataset

*Here we assump that median_income is very important feature to predict household value*

```{python}
## Create new feature (income group)
import numpy as np

data['income_grp'] = pd.cut(data['median_income'], bins=[0,1.5,3,4.5,6,np.inf], labels=[1,2,3,4,5])

data.income_grp.value_counts().sort_index().plot.bar(rot=0, grid=True)
plt.title('Frequency of income group')
plt.xlabel('Income group')
plt.ylabel('Frequency')
plt.show()
```

```{python}
## Split data into train, val, test set

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(data.drop('median_house_value', axis=1), data.median_house_value, stratify=data['income_grp'], test_size=0.2, random_state=24)
```

```{python}
## Drop the 'income_grp' after using

for df in [x_train, x_test]:
	df.drop('income_grp', axis=1, inplace=True)
```


## Explore and visualize the data to gain insights

If the data is very large, we will sample an exploration set to manipulate faster and easier. On the other hand, just work directly on full set if the data is quite small.

```{python}
df_vis = data.copy()
```

### Visualize

**Plot**

```{python}
#| label: fig-geoplot
#| fig-cap: 'The geographical plot of data'
#| echo: false
#| fig-align: center
#| fig-width: 2

df_vis.plot.scatter(x='longitude', y='latitude', 
s=df_vis['population']/100, label='population', c='median_house_value', 
cmap='jet', colorbar=True, legend=True, figsize=(6,6))
# plt.title('The geographical plot of data')
plt.grid()

plt.show()
```

Large point size: larger population
Blue -> red: higher house price

**Correlation**

There are 2 ways to perform: heatmap and pairgrid map

```{python}
#| label: fig-corr
#| fig-cap: 'Correlation Plot'
#| echo: false
#| fig-align: center
#| fig-width: 2

## Heatmap

import seaborn as sns

corr_matrix = df_vis.corr(numeric_only=True)
sns.heatmap(corr_matrix, annot=True, fmt='.2f')
```

```{python}
#| label: fig-pairgrid
#| fig-cap: 'PairGrid Plot of numerical features'
#| echo: false
#| fig-align: center
#| fig-width: 1

sns.set(rc={'figure.figsize':(8,6)})
g = sns.PairGrid(df_vis[["median_house_value", "median_income", "total_rooms", "housing_median_age"]])		# plot some high correlated features
g.map_diag(sns.histplot, bins=10)
g.map_offdiag(sns.scatterplot, alpha=0.2)
```

*Try pd.plotting.scatter_matrix*

Look closely too the relation between 'median_house_value' and 'median_income', we see there is a strong positive correlation, but there are some clearly horizontal line at 500,000; 450,000; 350,000 and roughly 280,000. We should remove these instances to prevent the algorithms learning these patterns.

```{python}
#| label: fig-corr2
#| fig-cap: 'Median income versus median house value'
#| echo: false
#| fig-align: center

df_vis.plot(kind='scatter', x='median_income', y='median_house_value', grid=True, figsize=(8,6))

plt.show()
```


### Attributes combination

Useful when we want to find better features to predict

```{python}
df_vis['room_per_house'] = df_vis['total_rooms']/df_vis['households']
df_vis['bedroom_ratio'] = df_vis['total_bedrooms']/df_vis['total_rooms']
df_vis['people_per_house'] = df_vis['population']/df_vis['households']

corr_matrix = df_vis.corr(numeric_only=True)
corr_matrix['median_house_value'].sort_values(ascending=False)
```


## Explore and Visualize Data

- Visualize 
- Compute correlation (corr_matrix(), pandas.plotting.scatter_matrix() )
- Attributes combination


## Prepare Data

Benefits:
- Reproduce tranformations easily
- Build library of tranformations for future projects
- Use in live system to transform new data
- Try various transformations => Choose best combination of transformations


### Clean Data

Missing Values:
- Get rid of them
- Get rid of whole attributes
- Set new values (imputation): zero, mean, median, most_frequent, constant, etc. 
	- sklearn.impute.SimpleImputer()
	- Apply to all numerical variables cause we do not know there will not be any missing values in the future.
	- More powerful imputer: KnnImputer(), IterativeImputer()


### Handling Text and Categorical Attributes

-  OneHotEncoder, LabelEncoder, OrdinalEncoder
	- default: Scipy sparse matrix => set sparse=False or toarray()
	- pandas.get_dummies(): generate new columns for unknown categories
	- OneHotEncoder: detect unknown categories
-  If a categorical attribute has a large number of possible categories
	=> OneHotEncoder will be result in a large number of input features
	- Turn categorical -> numerical category
	- In *neural networks*: replace each category -> *embedding* (a learnable, low-dimensional vector)


### Feature Scaling and Tranformations

#### For input attributes

- Some ML algorithms don’t perform well when the input numerical attributes have very different scales.
- Without any scaling, most models will be biased toward ignoring the small values and focusing more on the larger values.

Two common feature scaling techniques: *min-max scaling* and *standardization*
- Use *fit* or *fit_transform* only on **training set**
- **Training set** will always be scaled to specific range, if new data contains outliers, these may end up scaled outside the range => Set clip hyperparameter to *True* to avoid.

- **Min-max scaling (normalization)**: 
- Default range: 0-1
- MinMaxScaler(feature_range=(-1,1)): change the prefered range

- **Standardization**: 
- Less affected by outliers
- StandardScaler(with_mean=False): only divide the data by the standard deviation, without subtracting the mean => scale sparse matrix without converting to dense matrix 

- *Heavy-tailed attributes*:
- Both min-max scaling and standardization will squash most values into a small range
- Solutions: square root (or power 0-1), logarithm, bucketizing
	- bucketizing: chop its distribution into equal-sized buckets => replace with the index (i.e. *percentile*, *categories (for multimodal distribution)*) of buckets.
		- *multimodal distribution*: 2 or more clear peaks (also called mode).
	- other options for *multimodal distribution*: Gaussian radial basic function (RBF) measure the similarity between that attribute and the modes.

#### For output

After feature scaling the target values to make predictions on new data, it has to be *inverse_transform()*, or just do TransformedTargetRegressor.
 
```{python}
from sklearn.compose import TransformedTargetRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler

model = TransformedTargetRegressor(LinearRegression(),
                                       transformer=StandardScaler())

# model.fit(housing[["median_income"]], housing_labels)
# predictions = model.predict(some_new_data)
```


### Custom Transformers

Write own custom transformers: custom transformations, cleanup operations, or combining specific attributes.

```{python}
from sklearn.preprocessing import FunctionTransformer
```

***For details, check p.79, 80, 81***

-  A transformer should contains:
	- fit(): self.n_features_in_ , return *self*
	- get_feature_names_out() => to create DataFrame after *transform*
	- inverse_transform()

-  This is a custom transformer using *KMeans clusterer* in the fit() method to identify the main clusters in the training data, and then uses *rbf_kernel()* in the transform() method to measure how similar each sample is to each cluster center:

```{python}
from sklearn.cluster import KMeans
from sklearn.base import BaseEstimator, TransformerMixin

class ClusterSimilarity(BaseEstimator, TransformerMixin):  
	def __init__(self, n_clusters=10, gamma=1.0, random_state=None):
		self.n_clusters = n_clusters
		self.gamma = gamma
		self.random_state = random_state

	def fit(self, X, y=None, sample_weight=None):  
		self.kmeans_ = KMeans(self.n_clusters, random_state=self.random_state) 
		self.kmeans_.fit(X, sample_weight=sample_weight)  
		return self # always return self!

	def transform(self, X):  
		return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)

	def get_feature_names_out(self, names=None):  
		return [f"Cluster {i} similarity" for i in range(self.n_clusters)]
```

***For details, check p.82***

- You can check whether your custom estimator respects *Scikit-Learn’s API* by passing an instance to check_estimator() from the sklearn.utils.estimator_checks package. For the full API, check out https://scikit-learn.org/stable/developers.


### Transformation Pipelines

- **Pipeline**: sequence of transformations => Take list of names/estimators

	- Names must be *unique* and do not contain *double underscores* 
	- First (n-1) names: *transformers*
		Last name: regardless *transformer* or *predictor*

2 ways: 

**Pipeline**

```{python}
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
num_pipeline = Pipeline([
    ("impute", SimpleImputer(strategy="median")),
    ("standardize", StandardScaler()),
])
```

**make_pipeline** (don't care naming estimators)

```{python}
from sklearn.pipeline import make_pipeline  

num_pipeline = make_pipeline(SimpleImputer(strategy="median"), StandardScaler())
```

- The pipeline exposes the same methods as the *final estimator* (transformer or predictor)

-  In a Jupyter notebook, if we import sklearn and run sklearn.set_config(display="diagram"), all Scikit-Learn estimators will be rendered as interactive diagrams. This is particularly useful for visualizing pipelines. To visualize *num_pipeline*, run a cell with num_pipeline as the last line. Clicking an estimator will show more details.


-  2 ways to apply transform for numerical attributes and categorical attributes *seperately*:

**ColumnTransformer**:

```{python}
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

num_attribs = ["longitude", "latitude", "housing_median_age", "total_rooms", "total_bedrooms", "population", "households", "median_income"]

cat_attribs = ["ocean_proximity"]

num_pipeline = Pipeline([
    ("impute", SimpleImputer(strategy="median")),
    ("standardize", StandardScaler()),

])
cat_pipeline = make_pipeline(
    SimpleImputer(strategy="most_frequent"),
    OneHotEncoder(handle_unknown="ignore"))

preprocessing = ColumnTransformer([
    ("num", num_pipeline, num_attribs),
    ("cat", cat_pipeline, cat_attribs),

])
```

**make_column_selector**, **make_column_transformer** (don't care naming estimators)

```{python}
from sklearn.compose import make_column_selector, make_column_transformer

preprocessing = make_column_transformer(
	(num_pipeline, make_column_selector(dtype_include=np.number)),
	(cat_pipeline, make_column_selector(dtype_include=object)),

)
```

- Drop or passthrough columns: ***For details, see p.86**


- **Recap Pipeline**

- Missing values in numerical features will be imputed by replacing them with the median, as most ML algorithms don’t expect missing values. In categorical features, missing values will be replaced by the most frequent category.
    
- The categorical feature will be one-hot encoded, as most ML algorithms only accept numerical inputs.
    
- A few ratio features will be computed and added: bedrooms_ratio, rooms_per_house, and people_per_house. Hopefully these will better correlate with the median house value, and thereby help the ML models.
    
- A few cluster similarity features will also be added. These will likely be more useful to the model than latitude and longitude.
    
- Features with a long tail will be replaced by their logarithm, as most models prefer features with roughly uniform or Gaussian distributions.
    
- All numerical features will be standardized, as most ML algorithms prefer when all features have roughly the same scale.


**Final Pipeline**:

```{python}
def column_ratio(X):  
	return X[:, [0]] / X[:, [1]]
    
def ratio_name(function_transformer, feature_names_in): 
	return ["ratio"] #feature names out
    
def ratio_pipeline(): 
	return make_pipeline(
		SimpleImputer(strategy="median"),
		FunctionTransformer(column_ratio, feature_names_out=ratio_name),
		StandardScaler())
    
log_pipeline = make_pipeline(
	SimpleImputer(strategy="median"),
	FunctionTransformer(np.log, feature_names_out="one-to-one"),
	StandardScaler())
    
cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)

default_num_pipeline = make_pipeline(SimpleImputer(strategy="median"),
	    StandardScaler())
    
preprocessing = ColumnTransformer([
	("bedrooms", ratio_pipeline(), ["total_bedrooms", "total_rooms"]),
	("rooms_per_house", ratio_pipeline(), ["total_rooms", "households"]),
	("people_per_house", ratio_pipeline(), ["population", "households"]),
	("log", log_pipeline, ["total_bedrooms", "total_rooms", "population",

							"households", "median_income"]),
	("geo", cluster_simil, ["latitude", "longitude"]),

	("cat", cat_pipeline, make_column_selector(dtype_include=object)),
	],
	remainder=default_num_pipeline) # one column remaining: housing_median_age
```








