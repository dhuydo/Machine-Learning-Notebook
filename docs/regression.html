<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Nguyen Cao Duc Huy">

<title>regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Regression</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="linear-regression">Linear Regression</h2>
<p>Suppose that we have a data set of demographic and healthcare cost for each individual in a city, and we want to predict the total healthcare cost based on age.</p>
<p>If we use linear regression method for this task, we will assump that the relationship between these features is <em>linear</em> and try to fit a <code>line</code> so that is closest to the data. The plot looks like this.</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">## Simple linear regression plot</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">29</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="dv">100</span>      <span class="co"># number of instances</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.randint(<span class="dv">18</span>,<span class="dv">80</span>,m)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.random.randint(<span class="op">-</span><span class="dv">200</span>,<span class="dv">200</span>,m) <span class="op">+</span> <span class="dv">20</span><span class="op">*</span>x</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>plt.plot(x,y,<span class="st">'b.'</span>, label<span class="op">=</span><span class="st">'True values'</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>plt.plot(x, <span class="dv">20</span><span class="op">*</span>x,<span class="st">'-'</span>,color<span class="op">=</span><span class="st">'r'</span>, label<span class="op">=</span><span class="st">'Linear regression'</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Age'</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Healthcare cost'</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="regression_files/figure-html/cell-2-output-1.png" width="602" height="429"></p>
</div>
</div>
<p>If you have another feature using to predict (e.g.&nbsp;weight), the plot will look like this. For â‰¥3 features, itâ€™s called â€˜Multiple linear regressionâ€™ and we will fit a <code>hyperplane</code> instead.</p>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">## Multiple linear regression plot</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mpl_toolkits.mplot3d <span class="im">import</span> Axes3D</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">"ignore"</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> np.random.randint(<span class="dv">20</span>,<span class="dv">30</span>,m)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.random.randint(<span class="op">-</span><span class="dv">200</span>,<span class="dv">200</span>,m) <span class="op">+</span> <span class="dv">20</span><span class="op">*</span>x <span class="op">+</span><span class="dv">30</span><span class="op">*</span>z</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> np.c_[x,z]</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>lm <span class="op">=</span> LinearRegression()</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>lm.fit(X_train, y)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Setting up the 3D plot</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">7</span>))</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> fig.add_subplot(<span class="dv">111</span>, projection<span class="op">=</span><span class="st">'3d'</span>)</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Scatter plot of actual data</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>ax.scatter(x, z, y, color<span class="op">=</span><span class="st">'blue'</span>, marker<span class="op">=</span><span class="st">'o'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'True values'</span>)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating a meshgrid for the plane</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>x_surf <span class="op">=</span> np.linspace(x.<span class="bu">min</span>(), x.<span class="bu">max</span>(), <span class="dv">100</span>)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>z_surf <span class="op">=</span> np.linspace(z.<span class="bu">min</span>(), z.<span class="bu">max</span>(), <span class="dv">100</span>)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>x_surf, z_surf <span class="op">=</span> np.meshgrid(x_surf, z_surf)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Predicting the values from the meshed grid</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>vals <span class="op">=</span> pd.DataFrame({<span class="st">'Age'</span>: x_surf.ravel(), <span class="st">'Weight'</span>: z_surf.ravel()})</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> lm.predict(vals)</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>ax.plot_surface(x_surf, z_surf, y_pred.reshape(x_surf.shape), color<span class="op">=</span><span class="st">'r'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, label<span class="op">=</span><span class="st">'Hyperplane'</span>)</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Labeling the axes</span></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Age'</span>)</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Weight'</span>)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>ax.set_zlabel(<span class="st">'Healthcare cost'</span>)</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a><span class="co">#ax.legend()</span></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="regression_files/figure-html/cell-3-output-1.png" width="554" height="537"></p>
</div>
</div>
<p>The formula of the line (n=1)/hyperplane (n&gt;1) is: <span class="math display">\[
\hat{y} = Î¸_o +Î¸_1x_1 +Î¸_2x_2+...+Î¸_nx_n
\]</span></p>
<ul>
<li>Å·: predicted value</li>
<li>n: number of features</li>
<li>x_i: the i_th feature value</li>
<li>Î¸_i: the i_th parameter value (Î¸_0: intercept; Î¸_1 - Î¸_n: weight of parameters)</li>
</ul>
<p>For linear algebra, this can be written much more concisely using a vectorized form like this: <span class="math display">\[\hat{y} = Î¸.X\]</span></p>
<ul>
<li>Î¸: vecto of weights (of parameters)</li>
<li>X: matrix of features</li>
</ul>
<p>So how can we find the best fitted line, the left or the right one?</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">29</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="dv">100</span>      <span class="co"># number of instances</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.randint(<span class="dv">18</span>,<span class="dv">80</span>,m)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.random.randint(<span class="op">-</span><span class="dv">200</span>,<span class="dv">200</span>,m) <span class="op">+</span> <span class="dv">20</span><span class="op">*</span>x</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">1</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>plt.plot(x,y,<span class="st">'b.'</span>, label<span class="op">=</span><span class="st">'True values'</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>plt.plot(x, <span class="dv">20</span><span class="op">*</span>x,<span class="st">'-'</span>,color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Age'</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Healthcare cost'</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>plt.plot(x,y,<span class="st">'b.'</span>, label<span class="op">=</span><span class="st">'True values'</span>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>plt.plot(x, <span class="dv">10</span><span class="op">+</span><span class="dv">18</span><span class="op">*</span>(x<span class="op">+</span><span class="dv">10</span>),<span class="st">'-'</span>,color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Age'</span>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="regression_files/figure-html/cell-4-output-1.png" width="603" height="429"></p>
</div>
</div>
<p>It turns out that we have 2 common strategy:<br>
- Linear algebra: using <code>normal equation</code><br>
- Optimization: using <code>gradient descent</code></p>
<section id="sec-normal-equation" class="level3">
<h3 class="anchored" data-anchor-id="sec-normal-equation">Normal Equation</h3>
<p><span class="math display">\[Î¸ = (X^{T}X)^{-1}X^{T}y\]</span></p>
<ul>
<li>Î¸: vecto of weights (of parameters)</li>
<li>X: matrix of features</li>
<li>y: vecto of target value</li>
</ul>
<p>Thatâ€™s all we need to compute the best weights (coefficients).</p>
<p>But in reality, not all cases matrix is invertible, so <code>LinearRegression</code> in <code>sklearn</code> compute <em>pseudoinverse (X+)</em> instead, using a standard matrix factorization technique called singular value decomposition (SVD) that decompose X into (UÎ£V^T): <span class="math display">\[
\begin{gather}
Î¸ = X^{+}Y\\
   X = UÎ£V^{T}\\
X^{+} = VÎ£^{+}U^{T}
\end{gather}
\]</span></p>
<p>Implement Linear regression using sklearn</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">29</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.randint(<span class="dv">18</span>,<span class="dv">80</span>,m)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.random.randint(<span class="op">-</span><span class="dv">200</span>,<span class="dv">200</span>,m) <span class="op">+</span> <span class="dv">20</span><span class="op">*</span>x</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> x.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>linear <span class="op">=</span> LinearRegression()</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>linear.fit(X_train,y)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> linear.predict(X_train)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Equation: </span><span class="sc">{</span>linear<span class="sc">.</span>intercept_<span class="sc">:.2f}</span><span class="ss"> + </span><span class="sc">{</span>linear<span class="sc">.</span>coef_[<span class="dv">0</span>]<span class="sc">:.2f}</span><span class="ss">*x'</span>)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y, <span class="st">'b.'</span>, label<span class="op">=</span><span class="st">'True values'</span>)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y_pred,<span class="st">'-'</span>,color<span class="op">=</span><span class="st">'r'</span>, label<span class="op">=</span><span class="st">'Linear regression'</span>)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Age'</span>)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Healthcare cost'</span>)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Equation: 1.41 + 20.21*x</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="regression_files/figure-html/cell-5-output-2.png" width="602" height="429"></p>
</div>
</div>
<p>Both the Normal equation and SVD approach scale well with the number of instances, but scale very badly with number of features. Therefore, we will look at another approach which is better suited for cases where there are a large number of features or too many training instances to fit in memory.</p>
</section>
<section id="gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="gradient-descent">Gradient Descent</h3>
<section id="how-does-gd-work" class="level4">
<h4 class="anchored" data-anchor-id="how-does-gd-work">How does GD work?</h4>
<p>In fact, the computer really like the term â€˜optimizationâ€™, which means we will take the result roughly equal to the correct one with the acceptable error. Gradient descent (GD) is that kind of method.</p>
<p>Generally, GD tweaks the weights iteratively in order to minimize a <code>cost function</code>. Steps to do Gradient Descent:</p>
<ol type="1">
<li>Take <code>Gradient (derivative)</code> of Loss Function</li>
<li>Random initialization (take random weights)<br>
Loop step 3-5 until <strong>converge</strong>:<br>
</li>
<li>Compute gradient</li>
<li>Compute <code>step size</code>: StepSize = Gradient * Learning_rate</li>
<li>Compute new weights: New = Old - StepSize</li>
</ol>
<ul>
<li>[i] Run single epoch:
<ul>
<li>partial_fit(): ignore (<em>max_iter</em>, <em>tol</em>) do not reset epoch counter</li>
<li>fit(warm_start = True)</li>
</ul></li>
</ul>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<ul>
<li><p>Loss function: also called <code>cost function</code>, is the amount that we have to pay if we use the specific set of weights. Of course we want to minimize it cause everyone want to pay less but gain more, right ðŸ˜†</p></li>
<li><p>Learning rate: the pace of changing the weights in respond to the estimated loss<br>
</p>
<ul>
<li>Too small: take a long time to converge</li>
<li>Too high: diverge</li>
</ul></li>
<li><p>Number of epochs: times that we update our weights</p>
<ul>
<li>Too low: canâ€™t get optimal solution</li>
<li>Too high: waste time (parameters do not change much)</li>
<li><code>Solution</code>: set large <em>epoch</em> and a <em>tolerance</em> to interrupt when <em>grandient</em> &lt; <em>tolerance</em></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="fig-learning-rates" class="quarto-layout-panel">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/learning-rate.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Suitable learning rate</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/high-lr.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Too low learning rate</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/low-lr.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Too high learning rate</figcaption>
</figure>
</div>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;1: Learning rate strategy</figcaption><p></p>
</figure>
</div>
</section>
<section id="gd-pitfalls" class="level4">
<h4 class="anchored" data-anchor-id="gd-pitfalls">GD pitfalls</h4>
<ul>
<li><em>Local minimum</em>: If we initialize weights from the left, we will reach local minimum instead of global minimum</li>
<li><em>Plateau</em>: if we initialize weights from the right, the gradient will change slowly and adding new instances to the training set doesnâ€™t make the average error much better or worse. If early stopping, we will never reach the global minimum</li>
</ul>
<div id="fig-gd-pitfalls" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/gd-pitfalls.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;2: Gradient descent pitfalls</figcaption>
</figure>
</div>
<p>Fortunately, the cost function of linear regression is a <em>convex function</em>, which means it has no local minimum/ just one global minimum, and its slope never changes abruptly</p>
<p><span class="math display">\[
MSE = \frac{1}{2m}\sum_{i=1}^{m}{(Î¸^{T}x_{i}-y_{i})}^2
\]</span></p>
<ul>
<li>Another pitfall of GD: features have very different scales. Therefore, when using gradient descent, you should ensure that all features have a similar scale (e.g., using Scikit-Learnâ€™s <em>StandardScaler</em> class), or else it will take much longer to converge.</li>
</ul>
<div id="fig-gd-scales.png" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/gd-scales.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;3: Gradient descent with (left) and without (right) feature scaling</figcaption>
</figure>
</div>
</section>
<section id="implement-gradient-descent-using-sklearn" class="level4">
<h4 class="anchored" data-anchor-id="implement-gradient-descent-using-sklearn">Implement gradient descent using sklearn</h4>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> SGDRegressor</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> make_pipeline</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>sgd <span class="op">=</span> make_pipeline(StandardScaler(),</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>SGDRegressor(max_iter<span class="op">=</span><span class="dv">1000</span>, tol<span class="op">=</span><span class="fl">1e-3</span>))</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>sgd.fit(X_train, y)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Equation: </span><span class="sc">%.2f</span><span class="st"> + </span><span class="sc">%.2f</span><span class="st">*x'</span> <span class="op">%</span> (sgd[<span class="st">'sgdregressor'</span>].intercept_[<span class="dv">0</span>], sgd[<span class="st">'sgdregressor'</span>].coef_[<span class="dv">0</span>]))</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> sgd.predict(X_train)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y, <span class="st">'b.'</span>, label<span class="op">=</span><span class="st">'True values'</span>)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y_pred,<span class="st">'-'</span>,color<span class="op">=</span><span class="st">'r'</span>, label<span class="op">=</span><span class="st">'Stochastic gradient descent regressor'</span>)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Age'</span>)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Healthcare cost'</span>)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Equation: 961.04 + 372.06*x</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="regression_files/figure-html/cell-6-output-2.png" width="602" height="429"></p>
</div>
</div>
<div class="callout callout-style-simple callout-warning">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>The intercept and coefficient in this equation are different from <a href="#sec-normal-equation">Section&nbsp;1.1</a> because they implement on scaled X_train</p>
</div>
</div>
</div>
<p>Learn more about <a href="./gradient-descent.html">Gradient descent</a>.</p>
</section>
</section>
</section>
<section id="polynomial-regression" class="level2">
<h2 class="anchored" data-anchor-id="polynomial-regression">Polynomial Regression</h2>
<p>If the data is more complex (non linear), what do we do? In that case, we just create new features by adding powers to existed features, and use them to fit to our linear model. This technique is called <em>polynomial regression</em>.</p>
<p>For example, we will use sklearnâ€™s <code>PolynomialFeatures</code> to transform our data to higher degree, and then fit it to <code>LinearRegression</code>.</p>
<div class="cell" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">29</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> <span class="dv">10</span><span class="op">*</span>np.random.rand(m, <span class="dv">1</span>)<span class="op">-</span><span class="dv">5</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">10</span><span class="op">+</span> <span class="fl">1.5</span><span class="op">*</span>X<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> X <span class="op">+</span> np.random.randn(m,<span class="dv">1</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> PolynomialFeatures</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> make_pipeline</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> make_pipeline(PolynomialFeatures(degree<span class="op">=</span><span class="dv">2</span>, include_bias<span class="op">=</span><span class="va">False</span>), LinearRegression())</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>pipe.fit(X, y)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> pipe.predict(X)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>X_new <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">100</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>y_pred_new <span class="op">=</span> pipe.predict(X_new)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>plt.plot(X, y, <span class="st">'b.'</span>, label<span class="op">=</span><span class="st">'True values'</span>)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>plt.plot(X_new, y_pred_new,<span class="st">'-'</span>,color<span class="op">=</span><span class="st">'r'</span>, label<span class="op">=</span><span class="st">'Stochastic gradient descent regressor'</span>)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'y'</span>)</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="regression_files/figure-html/cell-7-output-1.png" width="585" height="429"></p>
</div>
</div>
<div class="callout callout-style-simple callout-tip">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>If we have n features, d degree: <code>PolynomialFeatures</code> transform into <code>(n+d)! / (n!d!)</code> features</p>
</div>
</div>
</div>
</section>
<section id="learning-curve" class="level2">
<h2 class="anchored" data-anchor-id="learning-curve">Learning Curve</h2>
<blockquote class="blockquote">
<p>How complex polynomial should be?</p>
</blockquote>
<ul>
<li>Underfitting (1 dgree): too simple model, canâ€™t capture the pattern of data</li>
<li>Overfitting (300 degrees): too complex model, tend to remember data</li>
</ul>
<div id="fig-different-degrees" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/different-degrees.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;4: Different polynomial degree</figcaption>
</figure>
</div>
<blockquote class="blockquote">
<p>How can tell overfitting or underfitting? There are 2 strategies</p>
</blockquote>
<p><strong>Cross-validation</strong><br>
- Overfitting: model perform well on train set, generate poorly on validation set<br>
- Underfitting: perform poorly on both train and validation sets</p>
<p><strong>Learning Curve</strong> - Plot <code>training errors</code> and <code>validation errors</code> over training set sizes (using cross-validation)<br>
- Overfitting: <em>gap</em> between the curves - Underfitting: <em>Plateau</em> (adding more training samples do not help)</p>
<blockquote class="blockquote">
<p>So how do we handle the overfitting/underfitting model?</p>
</blockquote>
<ul>
<li>Overfitting: Change too simpler model, feeding more training data, constrain the weights of unimportant features<br>
</li>
<li>Underfitting: Change to more complex algorithm; better features</li>
</ul>
<div class="callout callout-style-simple callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Bias-Variation Trade-Off
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li><strong>Bias (underfitting)</strong>: wrong assumptions (e.g.&nbsp;assump linear while quadratic)</li>
<li><strong>Variation (overfitting)</strong>: remember data (sensitive to variations in data)<br>
=&gt; <code>Trade-Off</code>: Increase modelâ€™s complexity will increase variation and decrease bias<br>
</li>
<li><strong>Irreducible error</strong>: noisiness =&gt; clean up data</li>
</ol>
</div>
</div>
</section>
<section id="regularized-linear-models" class="level2">
<h2 class="anchored" data-anchor-id="regularized-linear-models">Regularized Linear Models</h2>
<p>As mentioned above, to reduce overfitting we constrain the weights of model. These techniques are called <code>regularization</code> including: Ridge regression, Lasso Regression and Elastic net.</p>
<div class="callout callout-style-simple callout-tip">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<ul>
<li><p>Regularized linear models: Sensitive to the scale<br>
=&gt; <em>StandardScaler</em> before regularize<br>
</p></li>
<li><p>In almost cases, we should avoid plain Linear regression</p></li>
<li><p>Use case of Regularized linear models:</p></li>
</ul>
<ol type="1">
<li>Elastic Net: when there are few useful features, (features &gt; instances, correlated features =&gt; Lasso tends to behave erratically)</li>
<li>Lasso: when there are few useful features</li>
<li>Ridge: good for default (a <em>warmstart</em>)</li>
</ol>
<ul>
<li>Find out more about <em>RidgeCV</em>, <em>LassoCV</em> and <em>ElasticNetCV</em></li>
</ul>
</div>
</div>
</div>
<p><strong>?@fig-l1-l2</strong></p>
<section id="ridge-regression" class="level3">
<h3 class="anchored" data-anchor-id="ridge-regression">Ridge Regression</h3>
<p>Add a <em>regularization term (L2 norm)</em> to the MSE cost function of Linear regression in order to keep the weights as small as possible</p>
<p>Ridge regression cost function <span class="math display">\[
\begin{equation}
\begin{split}
J(Î¸) &amp; = MSE(Î¸) + \frac{Î±}{2m}\sum_{i=1}^{m}w_i^2\\
    &amp; = MSE(Î¸) + \frac{Î±}{2m}Î¸^Î¤Î¸
\end{split}
\end{equation}
\]</span></p>
<p>Closed-form equation <span class="math display">\[Î¸ = (X^{T}X + Î±Î‘)^{-1}X^{T}Y\]</span></p>
<div class="callout callout-style-simple callout-tip">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>sklearn.linear_model.Ridge(solver=â€˜choleskyâ€™)</p>
</div>
</div>
</div>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Ridge</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">29</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> <span class="dv">3</span> <span class="op">*</span> np.random.rand(m, <span class="dv">1</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">1</span> <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> X <span class="op">+</span> np.random.randn(m, <span class="dv">1</span>) <span class="op">/</span> <span class="fl">1.5</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_plot(alphas):</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    plt.plot(X, y, <span class="st">'b.'</span>)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> alpha, style <span class="kw">in</span> <span class="bu">zip</span>(alphas, [<span class="st">'b:'</span>,<span class="st">'r--'</span>,<span class="st">'g-'</span>]):</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>        pipe <span class="op">=</span> make_pipeline(PolynomialFeatures(degree<span class="op">=</span><span class="dv">5</span>, include_bias<span class="op">=</span><span class="va">False</span>), Ridge(alpha<span class="op">=</span>alpha, solver<span class="op">=</span><span class="st">'cholesky'</span>))</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        pipe.fit(X, y)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>        X_new <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">100</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>        y_pred_new <span class="op">=</span> pipe.predict(X_new)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>        plt.plot(X_new, y_pred_new, style, label<span class="op">=</span><span class="st">'alpha = </span><span class="sc">%s</span><span class="st">'</span> <span class="op">%</span> alpha)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>    plt.axis([<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">0</span>, <span class="fl">3.5</span>])</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>make_plot([<span class="dv">0</span>,<span class="fl">0.1</span>,<span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="regression_files/figure-html/cell-8-output-1.png" width="581" height="416"></p>
</div>
</div>
<p>Gradient descent <span class="math display">\[
\begin{gather}
âˆ‡ = \frac{1}{m}X^{T}(XÎ¸ - y)+\frac{Î±}{m}Î¸\\
\\
Î¸ = Î¸ - Î»âˆ‡\\
\end{gather}
\]</span></p>
<p>These 2 models are equally, in which we have to set the lpha in the SGD to be alpha/m</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Ridge</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>ridge <span class="op">=</span> Ridge(alpha<span class="op">=</span><span class="fl">0.1</span>, random_state<span class="op">=</span><span class="dv">29</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>sgd <span class="op">=</span> SGDRegressor(penalty<span class="op">=</span><span class="st">'l2'</span>, alpha<span class="op">=</span><span class="fl">0.1</span><span class="op">/</span>m, random_state<span class="op">=</span><span class="dv">29</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="lasso-regression" class="level3">
<h3 class="anchored" data-anchor-id="lasso-regression">Lasso Regression</h3>
<p>Add a <em>regularization term (L1 norm)</em> to the MSE cost function of Linear regression, but tend to <code>eliminate</code> weights of least important features<br>
=&gt; Weights is <code>sparse matrix</code></p>
<p>Lasso regression cost function <span class="math display">\[
\begin{equation}
\begin{split}
J(Î¸) &amp; = MSE(Î¸) + Î±\sum_{i=1}^{m}|w|\\
    &amp; = MSE(Î¸) + Î±Î¸
\end{split}
\end{equation}
\]</span></p>
<p>Gradient descent</p>
<p>The L1 regularization is not differentiable at Î¸i = 0, but gradient descent still works if we use a <em>subgradient vector</em> g11 instead when any Î¸i = 0. Learn more about <a href="https://www.cs.cmu.edu/afs/cs/project/link-3/lafferty/www/ml-stat2/talks/YondaiKimGLasso-SLIDE-YD.pdf">gradient descent for lasso regression</a></p>
<p>These 2 models are equally, and we have to adjust the alpha as same as ridge regression</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Lasso</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>ridge <span class="op">=</span> Lasso(alpha<span class="op">=</span><span class="fl">0.1</span>, random_state<span class="op">=</span><span class="dv">29</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>sgd <span class="op">=</span> SGDRegressor(penalty<span class="op">=</span><span class="st">'l1'</span>, alpha<span class="op">=</span><span class="fl">0.1</span><span class="op">/</span>m, random_state<span class="op">=</span><span class="dv">29</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="elastic-net-regression" class="level3">
<h3 class="anchored" data-anchor-id="elastic-net-regression">Elastic Net Regression</h3>
<p>Elastic Net is weighted sum of Ridge and Lasso regression, change the weights by <em>r</em> rate: 0 (more Ridge) to 1 (more Lasso)</p>
<p><span class="math display">\[
J(Î¸) = MSE(Î¸) + r*\frac{Î±}{2m}\sum_{i=1}^{m}w_i^2 + (1-r)Î±\sum_{i=1}^{m}|w_i|
\]</span></p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> ElasticNet</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>elast <span class="op">=</span> ElasticNet(alpha<span class="op">=</span><span class="fl">0.01</span>, l1_ratio<span class="op">=</span><span class="fl">0.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="early-stopping" class="level2">
<h2 class="anchored" data-anchor-id="early-stopping">Early Stopping</h2>
<p>Another way to regularize iterative learning algorithms (e.g.&nbsp;GD): <em>partial_fit</em> for n epochs and save the model has the lowest validation error</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> copy <span class="im">import</span> deepcopy</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co">## Create data</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">29</span>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> <span class="dv">6</span> <span class="op">*</span> np.random.rand(m, <span class="dv">1</span>) <span class="op">-</span> <span class="dv">3</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> X <span class="op">**</span> <span class="dv">2</span> <span class="op">+</span> X <span class="op">+</span> <span class="dv">2</span> <span class="op">+</span> np.random.randn(m, <span class="dv">1</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>best_rmse <span class="op">=</span> np.inf</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>x_train, x_test, y_train, y_test <span class="op">=</span> train_test_split(X, y)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>preprocessing <span class="op">=</span> make_pipeline(PolynomialFeatures(degree<span class="op">=</span><span class="dv">90</span>, include_bias<span class="op">=</span><span class="va">False</span>), StandardScaler())</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> preprocessing.fit_transform(x_train)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> preprocessing.transform(x_test)</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>sgd <span class="op">=</span> SGDRegressor(penalty<span class="op">=</span><span class="st">'elasticnet'</span>, alpha<span class="op">=</span><span class="fl">0.01</span>, l1_ratio<span class="op">=</span><span class="fl">0.5</span>, eta0<span class="op">=</span><span class="fl">0.001</span>, random_state<span class="op">=</span><span class="dv">29</span>)</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>    sgd.partial_fit(X_train, y_train.ravel())</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> sgd.predict(X_test)</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>    rmse <span class="op">=</span> mean_squared_error(y_test, y_pred, squared<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> rmse <span class="op">&lt;</span> best_rmse:</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>        best_rmse <span class="op">=</span> rmse</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>        best_model <span class="op">=</span> deepcopy(sgd)</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> best_model.predict(X_test)</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a><span class="co">## Another way to apply early stopping</span></span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a><span class="co"># sgd = SGDRegressor(penalty='elasticnet', alpha=0.01, l1_ratio=0.5, max_iter = 2000, tol=0.00001, shuffle=True, random_state=29, learning_rate='invscaling', eta0=0.001, early_stopping=True, validation_fraction=0.25, n_iter_no_change=10)</span></span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a><span class="co"># sgd.fit(X_train, y_train.ravel())</span></span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a><span class="co"># y_pred = sgd.predict(X_test)</span></span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'RMSE: </span><span class="sc">%.2f</span><span class="st">'</span> <span class="op">%</span> mean_squared_error(y_test, y_pred))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>RMSE: 0.67</code></pre>
</div>
</div>
<div class="callout callout-style-simple callout-tip">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><em>partial_fit</em>: <em>max_iter=1</em> (fit 1 epoch per calling); learn <code>incrementally</code> from a mini-batch of instances =&gt; useful when data is not fit into memory</p>
<p><em>fit</em>: train model from scratch (all instances at once)</p>
<p><em>fit(warm_start=True)</em> = partial_fit: allow learning from the weights of previous fit</p>
<p>copy.deepcopy(): copies both the modelâ€™s <em>hyperparameters</em> and the <em>learned parameters</em></p>
<p>sklearn.base.clone() only copies the modelâ€™s <em>hyperparameters</em>.</p>
</div>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>