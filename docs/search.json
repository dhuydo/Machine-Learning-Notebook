[
  {
    "objectID": "decision-tree.html",
    "href": "decision-tree.html",
    "title": "Decision Tree",
    "section": "",
    "text": "Decision trees are versatile machine learning algorithms that can perform both classification and regression tasks, and even multioutput tasks. It works by recursively partitioning the data into subsets based on statements of features, making decisions whether or not the statement is True or False."
  },
  {
    "objectID": "decision-tree.html#regularization",
    "href": "decision-tree.html#regularization",
    "title": "Decision Tree",
    "section": "Regularization",
    "text": "Regularization\n\nParametric model (Linear Regression): predetermined parameters =&gt; degree of freedom is limited =&gt; reducing the risk of overfitting\nNon-parametric model: parameters not determined prior to training =&gt; go freely =&gt; overfitting =&gt; regularization\nIncreasing min_* hyperparameters or reducing max_* hyperparameters will regularize the model:\n\nmax_depth, max_features, max_leaf_nodes\nmin_samples_split, min_samples_leaf, min_weight_fraction_leaf\n\n\n\nfrom sklearn.datasets import make_moons\n\nX_moons, y_moons = make_moons(n_samples=150, noise=0.2, random_state=42)\n\ndt_clf1 = DecisionTreeClassifier(random_state=29)\ndt_clf2 = DecisionTreeClassifier(max_depth=5, min_samples_leaf=5, random_state=29)\ndt_clf1.fit(X_moons, y_moons)\ndt_clf2.fit(X_moons, y_moons)\n\nX_moons_test, y_moons_test = make_moons(n_samples=1000, noise=0.2, random_state=43)\n\nprint(f'Non-regularized decision tree: {dt_clf1.score(X_moons_test, y_moons_test):.4f}')\nprint(f'Regularized decision tree: {dt_clf2.score(X_moons_test, y_moons_test):.4f}')\n\nNon-regularized decision tree: 0.8940\nRegularized decision tree: 0.9200\n\n\n\n\n\nFigure 3: Decision boudaries of unregularized tree (left) and regularized tree (right)\n\n\n\n\n\n\n\n\nPruning Trees\n\n\n\n\nPruning: deleting unnecessary nodes\n\nAlgorithms work by first training the decision tree without restrictions, then pruning (deleting) unnecessary nodes. A node whose children are all leaf nodes is considered unnecessary if the purity improvement it provides is not statistically significant. Standard statistical tests, such as the χ2 test (chi-squared test), are used to estimate the probability that the improvement is purely the result of chance (which is called the null hypothesis). If this probability, called the p-value, is higher than a given threshold (typically 5%, controlled by a hyperparameter), then the node is considered unnecessary and its children are deleted. The pruning continues until all unnecessary nodes have been pruned.\n\nThere are 3 types of Pruning Trees\n\n\nPre-Tuning\nPost-Tuning\nCombines"
  },
  {
    "objectID": "decision-tree.html#regression",
    "href": "decision-tree.html#regression",
    "title": "Decision Tree",
    "section": "Regression",
    "text": "Regression\n\nDecisionTreeRegressor splits each region in a way that makes most training instances as close as possible to that predicted value (average of instances in the region)\nCART cost function for regression: \\[\nJ_{k, t_k} = \\frac{m_{left}}{m} MSE_{left} + \\frac{m_{right}}{m} MSE_{right}\n\\]\nMSE: mean squared error\nm: number of instances\n\n\nimport numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\n\nnp.random.seed(42)\nX_quad = np.random.rand(200, 1) - 0.5\ny_quad = X_quad ** 2 + 0.025 * np.random.randn(200, 1)\n\ndt_reg = DecisionTreeRegressor(max_depth=2, min_samples_leaf=5, random_state=29)\ndt_reg.fit(X_quad, y_quad)\n\nplot_tree(dt_reg)\n\n[Text(0.5, 0.8333333333333334, 'x[0] &lt;= -0.303\\nsquared_error = 0.006\\nsamples = 200\\nvalue = 0.088'),\n Text(0.25, 0.5, 'x[0] &lt;= -0.408\\nsquared_error = 0.002\\nsamples = 44\\nvalue = 0.172'),\n Text(0.125, 0.16666666666666666, 'squared_error = 0.001\\nsamples = 20\\nvalue = 0.213'),\n Text(0.375, 0.16666666666666666, 'squared_error = 0.001\\nsamples = 24\\nvalue = 0.138'),\n Text(0.75, 0.5, 'x[0] &lt;= 0.272\\nsquared_error = 0.005\\nsamples = 156\\nvalue = 0.065'),\n Text(0.625, 0.16666666666666666, 'squared_error = 0.001\\nsamples = 110\\nvalue = 0.028'),\n Text(0.875, 0.16666666666666666, 'squared_error = 0.002\\nsamples = 46\\nvalue = 0.154')]\n\n\n\n\n\nIf we set max_depth larger, the model will predict more strictly. As same as Figure 4, if we keep the default hypeparameters, the model will grow as much as it can Figure 5.\n\n\n\nFigure 4: Different max_depth\n\n\n\n\n\nFigure 5: Unregularized tree (left) and regularized tree (right"
  },
  {
    "objectID": "decision-tree.html#limitations-of-decision-tree",
    "href": "decision-tree.html#limitations-of-decision-tree",
    "title": "Decision Tree",
    "section": "Limitations of Decision Tree",
    "text": "Limitations of Decision Tree\n\nDecision tree tends to make orthogonal decision boudaries. =&gt; Sensitive to the data’s orientation.\n\n\n\n\nFigure 6: Decision tree is sensitive to data’s orientation\n\n\n=&gt; Solution: Scale data -&gt; PCA: reduce dimensions but do not loss too much information, rotate data to reduce correlation between features, which often (not always) makes things easier for trees.\nCompare to Figure 2, the scaled and PCA-rotated iris dataset is separated easier.\n\n\n\nFigure 7: Scaled and PCA-rotated data\n\n\n\nHigh variance: randomly select set of features to evaluate at each node, so that if we retrain model on the same dataset, it can behave really different =&gt; high variance, unstable\n=&gt; Solution: Ensemble methods (Random Forest, Boosting methods) averaging predictions over many trees."
  }
]