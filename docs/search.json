[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Notebook",
    "section": "",
    "text": "Welcome\nHello everyone, my name is Nguyen Cao Duc Huy. I am studying Master of Data Science at HCMUS.\nThis notebook covers some Machine Learning algorithms and practical exercises, has been written while I understanding ML concepts. Hopefully it will help you systemize knowledge and be able to work in this interesting and fast-evolving field.\nContents of this notebook are as follow:\n\nThe Machine Learning Landscape\nMachine Learning Project Workflow\nRegression algorithms\nGradient Descent\nHands-on Regression\nClassification algorithms\nHands-on Classification\nSupport Vector Machine\nDecision Tree\nDimensionality Reduction\nEnsemble Learning\nUnsupervised Learning\n\nIn Progress:\n\nFundamental Neural Networks (NNs)\nLoading and Preprocessing Data for NNs\nTraining Deep NNs\nCustomize NNs\nConvolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs)\nComputer Vision using NNs\nNatural Language Processing (NLP) using NNs\nAutoencoders, GANs, and Diffusion Models\nReinforcement Learning\nTraining and Deploying NNs models at Scale\n\nReferences\n\nAurélien Géron (2023). Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (3rd ed.). O’Reilly.\nJeremy Howard & Sylvain Gugger. Deep Learning for Coders with fastai & PyTorch. O’Reilly."
  },
  {
    "objectID": "landscape.html#what-is-machine-learning",
    "href": "landscape.html#what-is-machine-learning",
    "title": "1  Machine Learning Landscape",
    "section": "1.1 What is Machine Learning",
    "text": "1.1 What is Machine Learning\n\n“Machine learning is the field of study that gives computers the ability to learn without being explicitly programmed.” — Arthur Samuel, 1959\n\nTraditional Approach\nIf you have a task using data to make a prediction (e.g. classification), with traditional approach you might have to come up with a bunch of if-else clauses to handle it.\n\nFor example, to write a spam email filter program, you have to know some words or phrases that usually appear in a spam email, then your program would flag emails as spam if a number of these patterns were detected. Finally, you would test your program until it was good enough to launch.\n\n\n\nFigure 1.1: Traditional approach\n\n\nBut how is it if the spammers notice the rules that we set?\nThey will adjust these patterns a little bit that our program cannot detect, and if they keep working around our spam filter, we will need to keep writing new rules that gradually become a very long complex list. This is where the ML approach comes into play.\nThe Machine Learning Approach\nAs Arthur Samuel said, the ML system can learn patterns from the data without being explicitly programmed.\nIn this case, a spam filter based on ML techniques automatically notices unusually frequent patterns in spam flagged by users, and it starts flagging them without our intervention.\n\n\n\nFigure 1.2: Machine Learning approach"
  },
  {
    "objectID": "landscape.html#applications-of-machine-learning",
    "href": "landscape.html#applications-of-machine-learning",
    "title": "1  Machine Learning Landscape",
    "section": "1.2 Applications of Machine Learning",
    "text": "1.2 Applications of Machine Learning\nCommon tasks\n- Regression: Forecast revenue, weather, impact of certificate scores on admission, financial and logistical forecasting.\n- Classification: Anomaly detection.\n- Clustering: Segmenting clients.\n\nNatural Language Processing\nText classification (topic modeling, flagging offensive comments), text summarization, questions-answering modules (chatbot), speech recognition.\nComputer vision\nImage classification, semantic image segmentation, face recognition, image captioning.\nRecommendation system\nWeb search, product recommendations, home page layout.\nRobotics\nHandling objects, decision-making.\nMedicine\nFinding anomalies in radiology images, measuring features in ultrasounds.\nBiology\nClassifying cell/proteins, tumor-normal sequencing and classifying.\nPlaying games\nChess, Go, most Atari video games, and many real-time strategy games.\n\n\n\n\n\n\nTo summarize, machine learning is especially great for:\n- Problems for which existing solutions are complex and/or require a lot of work.\n- Fluctuating environments, hard/retrain to keep up to date.\n- Getting insights about complex problems and large amounts of data."
  },
  {
    "objectID": "landscape.html#types-of-machine-learning-system",
    "href": "landscape.html#types-of-machine-learning-system",
    "title": "1  Machine Learning Landscape",
    "section": "1.3 Types of Machine Learning System",
    "text": "1.3 Types of Machine Learning System\n\n\n\n\n\n\nMachine Learning Systems are divided into 3 common types based on:\n- Training supervision: supervised learning, unsupervised learning, reinforcement learning.\n- Learn on the fly or not: online learning versus batch learning.\n- Learn by heart or detect patterns: instance-based learning versus model-based learning.\n\n\n\n\n1.3.1 Training Supervision\nHow they supervised: supervised, unsupervised, semi-supervised, self-supervised, etc.\n\nSupervised: Classification and Regression.\nUnsupervised: Clustering, Dimensionality Reduction, Association Rule Learning.\nSemi-supervised: Partially labeled.\nSelf-supervised: Unlabeled -&gt; Fully labeled (learn to generate labels). Unlike unsupervised learning, self-supervised focuses on classification and regression, although it learns from fully unlabeled data\nReinforcement Learning: Optimize rewards -&gt; Find best strategy (called policy) itself.\n\n\n\n1.3.2 Learn on the fly\nWhether or not they can learn incrementally.\nBatch Learning (also called Offline Learning).\n- Learn offline using all available data, apply what it has learned without learning more.\n- Useful for areas that do not change too much.\n- Model’s performance will not decay fast.\n- Not useful for fast-evolving system (ie. financial market).\n- Model’s performance will decay rellay fast.\n- Have to train new model.\nOnline Learning (also called Incremental Learning).\n- Run and learn simultanously by feeding new data sequentially called mini-batches.\n- Suitable for rapidly changed system, limited computing resources.\n- Important parameter: learning rate.\n- Challenges: bad data.\n- Decline performance quickly.\n- Solutions: monitor system (switch learning off, anomaly detection).\n\n\n1.3.3 Instance-Based Versus Model-Based Learning\nCompare new data to known data/Detect patterns in data.\nInstance-based Learning\n\nLearn by heart, then generalize new data using a similarity measure (e.g. k-Nearest Neighbors Regression).\n\nModel-based Learning\n\n\nFeed data as examples to model\n\n=&gt; Able to make predictions (e.g. Linear Regression).\n\nMinimize cost function =&gt; Find best parameters."
  },
  {
    "objectID": "landscape.html#challenges-of-machine-learning",
    "href": "landscape.html#challenges-of-machine-learning",
    "title": "1  Machine Learning Landscape",
    "section": "1.4 Challenges of Machine Learning",
    "text": "1.4 Challenges of Machine Learning\n\n1.4.1 Insufficient Quantity of Data\nConsider the amount of data whether it’s sufficient or not rather than abandon the algorithm.\n\n\n\nFigure 1.3: The importance of data versus algorithms\n\n\n\n\n1.4.2 Non-representative Training Data\nThe training data must be representative of the new cases you want to generalize to.\n- If data is too small: sampling noise.\n- If large data but flawed sampling method: sampling bias.\n\n\n1.4.3 Poor-quality Data\n\nErrors, Outliers, Noises, etc.\n\n=&gt; Model perform badly.\n\nSolutions:\n\nRemove outliers/Fix errors.\nInstances have missing features =&gt;\n\nIgnore Instances.\nIgnore Features.\nFill in Missing Values.\nTrain model with/without features.\n\n\n\n\n\n1.4.4 Irrelevant Features\nFeature Engineering:\n\nFeature Selection: Select most useful features.\nFeature Extraction: Combine existing features to produce a more useful one.\nCreate new feature: from exist data/gather new data.\n\n\n\n1.4.5 Overfitting the Training Data\nModel tends to remember data.\n=&gt; Perform well on training Data, but generalize badly (e.g. Fit noisy/too small data to complex models (NNs)).\nSolutions:\n\nChoose simpler models (less parameters).\nContrain models (regularization).\nGather more data.\nReduct noise data.\n\n\n\n1.4.6 Underfitting the Training Data\nModel is too simple to learn underlying structure of data.\nSolutions:\n\nChoose more powerful model (more parameters).\nReduce the model’s contraints.\nFeature engineering.\n\n\n\n1.4.7 Summarise\n\nMachine learning is about making machines get better at some task by learning from data, instead of having to explicitly code rules.\nThere are many different types of ML systems: supervised or not, batch or online, instance-based or model-based.\nIn an ML project you gather data in a training set, and you feed the training set to a learning algorithm. If the algorithm is model-based, it tunes some parameters to fit the model to the training set (i.e., to make good predictions on the training set itself), and then hopefully it will be able to make good predictions on new cases as well. If the algorithm is instance-based, it just learns the examples by heart and generalizes to new instances by using a similarity measure to compare them to the learned instances.\nThe system will not perform well if your training set is too small, or if the data is not representative, is noisy, or is polluted with irrelevant features (garbage in, garbage out). Lastly, your model needs to be neither too simple (in which case it will underfit) nor too complex (in which case it will overfit)."
  },
  {
    "objectID": "landscape.html#testing-and-validating",
    "href": "landscape.html#testing-and-validating",
    "title": "1  Machine Learning Landscape",
    "section": "1.5 Testing and Validating",
    "text": "1.5 Testing and Validating\n\nTrain/val/test split.\nError rate on new cases: generalization error (also called out-of-sample error).\nCross-validation:\n\nusing many small validation sets (n).\nTrain models n times with (n-1) validation sets =&gt; Test with the rest set.\nAverage results =&gt; much more accurate measure.\nDrawback: the training time is multiplied by the number of validation sets.\n\n\n\n1.5.1 Data Mismatch\n\n\n\nFigure 1.4: Data Mismatch\n\n\n\n\n1.5.2 No Free Lunch Theorem\n\n\n\nFigure 1.5: No Free Lunch Theorem"
  },
  {
    "objectID": "landscape.html#the-machine-learning-workflow",
    "href": "landscape.html#the-machine-learning-workflow",
    "title": "1  Machine Learning Landscape",
    "section": "1.6 The Machine Learning Workflow",
    "text": "1.6 The Machine Learning Workflow\n\nFrame the problem and look at the big picture.\nGet the data.\nExplore the data to gain insights.\nPrepare the data to better expose the underlying data patterns to machine learn‐ ing algorithms.\nExplore many different models and shortlist the best ones.\nFine-tune your models and combine them into a great solution.\nPresent your solution.\nLaunch, monitor, and maintain your system.\n\nGo to Chapter 2 for details of each step"
  },
  {
    "objectID": "regression.html#linear-regression",
    "href": "regression.html#linear-regression",
    "title": "3  Regression",
    "section": "3.1 Linear Regression",
    "text": "3.1 Linear Regression\nSuppose that we have a data set of demographic and healthcare cost for each individual in a city, and we want to predict the total healthcare cost based on age.\nIf we use linear regression method for this task, we will assump that the relationship between these features is linear and try to fit a line so that is closest to the data. The plot looks like this.\n\n\nCode\n## Simple linear regression plot\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(29)\nm = 100      # number of instances\nx = np.random.randint(18,80,m)\ny = np.random.randint(-200,200,m) + 20*x\n\nplt.plot(x,y,'b.', label='True values')\nplt.plot(x, 20*x,'-',color='r', label='Linear regression')\nplt.xlabel('Age')\nplt.ylabel('Healthcare cost')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nIf you have another feature using to predict (e.g. weight), the plot will look like this. For ≥3 features, it’s called ‘Multiple linear regression’ and we will fit a hyperplane instead.\n\n\nCode\n## Multiple linear regression plot\n\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\nfrom mpl_toolkits.mplot3d import Axes3D\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nz = np.random.randint(20,30,m)\ny = np.random.randint(-200,200,m) + 20*x +30*z\n\nX_train = np.c_[x,z]\nlm = LinearRegression()\nlm.fit(X_train, y)\n\n\n# Setting up the 3D plot\nfig = plt.figure(figsize=(10, 7))\nax = fig.add_subplot(111, projection='3d')\n\n# Scatter plot of actual data\nax.scatter(x, z, y, color='blue', marker='o', alpha=0.5, label='True values')\n\n# Creating a meshgrid for the plane\nx_surf = np.linspace(x.min(), x.max(), 100)\nz_surf = np.linspace(z.min(), z.max(), 100)\nx_surf, z_surf = np.meshgrid(x_surf, z_surf)\n\n# Predicting the values from the meshed grid\nvals = pd.DataFrame({'Age': x_surf.ravel(), 'Weight': z_surf.ravel()})\ny_pred = lm.predict(vals)\nax.plot_surface(x_surf, z_surf, y_pred.reshape(x_surf.shape), color='r', alpha=0.3, label='Hyperplane')\n\n# Labeling the axes\nax.set_xlabel('Age')\nax.set_ylabel('Weight')\nax.set_zlabel('Healthcare cost')\n#ax.legend()\n\nplt.show()\n\n\n\n\n\nThe formula of the line (n=1)/hyperplane (n&gt;1) is: \\[\n\\hat{y} = θ_o +θ_1x_1 +θ_2x_2+...+θ_nx_n\n\\]\n\nŷ: predicted value\nn: number of features\nx_i: the i_th feature value\nθ_i: the i_th parameter value (θ_0: intercept; θ_1 - θ_n: weight of parameters)\n\nFor linear algebra, this can be written much more concisely using a vectorized form like this: \\[\\hat{y} = θ.X\\]\n\nθ: vecto of weights (of parameters)\nX: matrix of features\n\nSo how can we find the best fitted line, the left or the right one?\n\n\nCode\nnp.random.seed(29)\nm = 100      # number of instances\nx = np.random.randint(18,80,m)\ny = np.random.randint(-200,200,m) + 20*x\n\nplt.subplot(1,2,1)\nplt.plot(x,y,'b.', label='True values')\nplt.plot(x, 20*x,'-',color='r')\nplt.xlabel('Age')\nplt.ylabel('Healthcare cost')\n\nplt.subplot(1,2,2)\nplt.plot(x,y,'b.', label='True values')\nplt.plot(x, 10+18*(x+10),'-',color='r')\nplt.xlabel('Age')\n\nplt.show()\n\n\n\n\n\nIt turns out that we have 2 common strategy:\n- Linear algebra: using normal equation\n- Optimization: using gradient descent\n\n3.1.1 Normal Equation\n\\[θ = (X^{T}X)^{-1}X^{T}y\\]\n\nθ: vecto of weights (of parameters)\nX: matrix of features\ny: vecto of target value\n\nThat’s all we need to compute the best weights (coefficients).\nBut in reality, not all cases matrix is invertible, so LinearRegression in sklearn compute pseudoinverse (X+) instead, using a standard matrix factorization technique called singular value decomposition (SVD) that decompose X into (UΣV^T): \\[\n\\begin{gather}\nθ = X^{+}Y\\\\\n   X = UΣV^{T}\\\\\nX^{+} = VΣ^{+}U^{T}\n\\end{gather}\n\\]\nImplement Linear Regression using sklearn\n\nfrom sklearn.linear_model import LinearRegression\n\nnp.random.seed(29)\nx = np.random.randint(18,80,m)\ny = np.random.randint(-200,200,m) + 20*x\nX_train = x.reshape(-1,1)\n\nlinear = LinearRegression()\nlinear.fit(X_train,y)\ny_pred = linear.predict(X_train)\nprint(f'Equation: {linear.intercept_:.2f} + {linear.coef_[0]:.2f}*x')\n\nplt.plot(x, y, 'b.', label='True values')\nplt.plot(x, y_pred,'-',color='r', label='Linear regression')\nplt.xlabel('Age')\nplt.ylabel('Healthcare cost')\nplt.legend()\n\nplt.show()\n\nEquation: 1.41 + 20.21*x\n\n\n\n\n\nBoth the Normal equation and SVD approach scale well with the number of instances, but scale very badly with number of features. Therefore, we will look at another approach which is better suited for cases where there are a large number of features or too many training instances to fit in memory.\n\n\n3.1.2 Gradient Descent\n\n3.1.2.1 How does GD work?\nIn fact, the computer really like the term ‘optimization’, which means we will take the result roughly equal to the correct one with the acceptable error. Gradient descent (GD) is that kind of method.\nGenerally, GD tweaks the weights iteratively in order to minimize a cost function. Steps to do Gradient Descent:\n\nTake Gradient (derivative) of Loss Function\nRandom initialization (take random weights)\nLoop step 3-5 until converge:\n\nCompute gradient\nCompute step size: StepSize = Gradient * Learning_rate\nCompute new weights: New = Old - StepSize\n\n\nRun single epoch:\n\npartial_fit(): ignore (max_iter, tol) do not reset epoch counter\nfit(warm_start = True)\n\n\n\n\n\n\n\n\n\nLoss function: also called cost function, is the amount that we have to pay if we use the specific set of weights. Of course we want to minimize it cause everyone want to pay less but gain more, right 😆\nLearning rate: the pace of changing the weights in respond to the estimated loss\n\n\nToo small: take a long time to converge\nToo high: diverge\n\nNumber of epochs: times that we update our weights\n\nToo low: can’t get optimal solution\nToo high: waste time (parameters do not change much)\nSolution: set large epoch and a tolerance to interrupt when grandient &lt; tolerance\n\n\n\n\n\n\n\n\n\n\n\n\nSuitable learning rate\n\n\n\n\n\n\n\nToo low learning rate\n\n\n\n\n\n\n\nToo high learning rate\n\n\n\n\nFigure 3.1: Learning rate strategy\n\n\n\n\n3.1.2.2 GD pitfalls\n\nLocal minimum: If we initialize weights from the left, we will reach local minimum instead of global minimum\nPlateau: if we initialize weights from the right, the gradient will change slowly and adding new instances to the training set doesn’t make the average error much better or worse. If early stopping, we will never reach the global minimum\n\n\n\n\nFigure 3.2: Gradient descent pitfalls\n\n\nFortunately, the cost function of linear regression is a convex function, which means it has no local minimum/ just one global minimum, and its slope never changes abruptly\n\\[\nMSE = \\frac{1}{2m}\\sum_{i=1}^{m}{(θ^{T}x_{i}-y_{i})}^2\n\\]\n\nAnother pitfall of GD: features have very different scales. Therefore, when using gradient descent, you should ensure that all features have a similar scale (e.g., using Scikit-Learn’s StandardScaler class), or else it will take much longer to converge.\n\n\n\n\nFigure 3.3: Gradient descent with (left) and without (right) feature scaling\n\n\n\n\n3.1.2.3 Implement gradient descent using sklearn\n\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\nsgd = make_pipeline(StandardScaler(),\nSGDRegressor(max_iter=1000, tol=1e-3))\nsgd.fit(X_train, y)\nprint('Equation: %.2f + %.2f*x' % (sgd['sgdregressor'].intercept_[0], sgd['sgdregressor'].coef_[0]))\n\ny_pred = sgd.predict(X_train)\n\nplt.plot(x, y, 'b.', label='True values')\nplt.plot(x, y_pred,'-',color='r', label='Stochastic gradient descent regressor')\nplt.xlabel('Age')\nplt.ylabel('Healthcare cost')\nplt.legend()\n\nplt.show()\n\nEquation: 961.04 + 372.06*x\n\n\n\n\n\n\n\n\n\n\n\nThe intercept and coefficient in this equation are different from Section 3.1.1 because they implement on scaled X_train\n\n\n\nLearn more about Gradient descent."
  },
  {
    "objectID": "regression.html#polynomial-regression",
    "href": "regression.html#polynomial-regression",
    "title": "3  Regression",
    "section": "3.2 Polynomial Regression",
    "text": "3.2 Polynomial Regression\nIf the data is more complex (non linear), what do we do? In that case, we just create new features by adding powers to existed features, and use them to fit to our linear model. This technique is called polynomial regression.\nFor example, we will use sklearn’s PolynomialFeatures to transform our data to higher degree, and then fit it to LinearRegression.\n\n\nCode\nnp.random.seed(29)\nm = 100\nX = 10*np.random.rand(m, 1)-5\ny = 10+ 1.5*X**2 + X + np.random.randn(m,1)\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\npipe = make_pipeline(PolynomialFeatures(degree=2, include_bias=False), LinearRegression())\n\npipe.fit(X, y)\ny_pred = pipe.predict(X)\n\nX_new = np.linspace(-5, 5, 100).reshape(-1, 1)\ny_pred_new = pipe.predict(X_new)\n\nplt.plot(X, y, 'b.', label='True values')\nplt.plot(X_new, y_pred_new,'-',color='r', label='Stochastic gradient descent regressor')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nIf we have n features, d degree: PolynomialFeatures transform into (n+d)! / (n!d!) features"
  },
  {
    "objectID": "regression.html#learning-curve",
    "href": "regression.html#learning-curve",
    "title": "3  Regression",
    "section": "3.3 Learning Curve",
    "text": "3.3 Learning Curve\n\nHow complex polynomial should be?\n\n\nUnderfitting (1 dgree): too simple model, can’t capture the pattern of data\nOverfitting (300 degrees): too complex model, tend to remember data\n\n\n\n\nFigure 3.4: Different polynomial degree\n\n\n\nHow can tell overfitting or underfitting? There are 2 strategies\n\nCross-validation\n- Overfitting: model perform well on train set, generate poorly on validation set\n- Underfitting: perform poorly on both train and validation sets\nLearning Curve - Plot training errors and validation errors over training set sizes (using cross-validation)\n- Overfitting: gap between the curves.\n- Underfitting: Plateau (adding more training samples do not help).\nSo how do we handle the overfitting/underfitting model?\n\nOverfitting: Change too simpler model, feeding more training data, constrain the weights of unimportant features.\n\nUnderfitting: Change to more complex algorithm; better features.\n\n\n\n\n\n\n\nBias-Variation Trade-Off\n\n\n\n\nBias (underfitting): wrong assumptions (e.g. assump linear while quadratic)\nVariation (overfitting): remember data (sensitive to variations in data)\n=&gt; Trade-Off: Increase model’s complexity will increase variation and decrease bias\n\nIrreducible error: noisiness =&gt; clean up data"
  },
  {
    "objectID": "regression.html#regularized-linear-models",
    "href": "regression.html#regularized-linear-models",
    "title": "3  Regression",
    "section": "3.4 Regularized Linear Models",
    "text": "3.4 Regularized Linear Models\nAs mentioned above, to reduce overfitting we constrain the weights of model. These techniques are called regularization including: Ridge regression, Lasso Regression and Elastic net.\n\n\n\n\n\n\n\nRegularized linear models: Sensitive to the scale\n=&gt; StandardScaler before regularize\n\nIn almost cases, we should avoid plain Linear regression\nUse case of Regularized linear models:\n\n\nElastic Net: when there are few useful features, (features &gt; instances, correlated features =&gt; Lasso tends to behave erratically)\nLasso: when there are few useful features\nRidge: good for default (a warmstart)\n\n\nFind out more about RidgeCV, LassoCV and ElasticNetCV\n\n\n\n\n\n\n\nFigure 3.5: L1-L2 norm\n\n\n\n3.4.1 Ridge Regression\nAdd a regularization term (L2 norm) to the MSE cost function of Linear regression in order to keep the weights as small as possible\nRidge regression cost function \\[\n\\begin{equation}\n\\begin{split}\nJ(θ) & = MSE(θ) + \\frac{α}{2m}\\sum_{i=1}^{m}w_i^2\\\\\n    & = MSE(θ) + \\frac{α}{2m}θ^Τθ\n\\end{split}\n\\end{equation}\n\\]\nClosed-form equation \\[θ = (X^{T}X + αΑ)^{-1}X^{T}Y\\]\n\n\n\n\n\n\nsklearn.linear_model.Ridge(solver=‘cholesky’)\n\n\n\n\nfrom sklearn.linear_model import Ridge\n\nnp.random.seed(29)\nm = 50\nX = 3 * np.random.rand(m, 1)\ny = 1 + 0.5 * X + np.random.randn(m, 1) / 1.5\n\ndef make_plot(alphas):\n    plt.plot(X, y, 'b.')\n    for alpha, style in zip(alphas, ['b:','r--','g-']):\n        pipe = make_pipeline(PolynomialFeatures(degree=5, include_bias=False), Ridge(alpha=alpha, solver='cholesky'))\n        pipe.fit(X, y)\n        X_new = np.linspace(0, 3, 100).reshape(-1, 1)\n        y_pred_new = pipe.predict(X_new)\n\n        plt.plot(X_new, y_pred_new, style, label='alpha = %s' % alpha)\n    plt.axis([0, 3, 0, 3.5])\n    plt.legend()\n    plt.show()\n\nmake_plot([0,0.1,1])\n\n\n\n\nGradient descent \\[\n\\begin{gather}\n∇ = \\frac{1}{m}X^{T}(Xθ - y)+\\frac{α}{m}θ\\\\\n\\\\\nθ = θ - λ∇\\\\\n\\end{gather}\n\\]\nThese 2 models are equally, in which we have to set the lpha in the SGD to be alpha/m\n\nfrom sklearn.linear_model import Ridge\n\nalpha = 0.01\n\nridge = Ridge(alpha=0.1, random_state=29)\nsgd = SGDRegressor(penalty='l2', alpha=0.1/m, random_state=29)\n\n\n\n3.4.2 Lasso Regression\nAdd a regularization term (L1 norm) to the MSE cost function of Linear regression, but tend to eliminate weights of least important features\n=&gt; Weights is sparse matrix\nLasso regression cost function \\[\n\\begin{equation}\n\\begin{split}\nJ(θ) & = MSE(θ) + α\\sum_{i=1}^{m}|w|\\\\\n    & = MSE(θ) + αθ\n\\end{split}\n\\end{equation}\n\\]\nGradient descent\nThe L1 regularization is not differentiable at θi = 0, but gradient descent still works if we use a subgradient vector g11 instead when any θi = 0. Learn more about gradient descent for lasso regression\nThese 2 models are equally, and we have to adjust the alpha as same as ridge regression\n\nfrom sklearn.linear_model import Lasso\n\nalpha = 0.01\n\nridge = Lasso(alpha=0.1, random_state=29)\nsgd = SGDRegressor(penalty='l1', alpha=0.1/m, random_state=29)\n\n\n\n3.4.3 Elastic Net Regression\nElastic Net is weighted sum of Ridge and Lasso regression, change the weights by r rate: 0 (more Ridge) to 1 (more Lasso)\n\\[\nJ(θ) = MSE(θ) + r*\\frac{α}{2m}\\sum_{i=1}^{m}w_i^2 + (1-r)α\\sum_{i=1}^{m}|w_i|\n\\]\n\nfrom sklearn.linear_model import ElasticNet\n\nelast = ElasticNet(alpha=0.01, l1_ratio=0.5)"
  },
  {
    "objectID": "regression.html#early-stopping",
    "href": "regression.html#early-stopping",
    "title": "3  Regression",
    "section": "3.5 Early Stopping",
    "text": "3.5 Early Stopping\nAnother way to regularize iterative learning algorithms (e.g. GD): partial_fit for n epochs and save the model has the lowest validation error\n\nfrom copy import deepcopy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n\n## Create data\n\nnp.random.seed(29)\nm = 100\nX = 6 * np.random.rand(m, 1) - 3\ny = 0.5 * X ** 2 + X + 2 + np.random.randn(m, 1)\nepochs = 500\nbest_rmse = np.inf\n\nx_train, x_test, y_train, y_test = train_test_split(X, y)\n\npreprocessing = make_pipeline(PolynomialFeatures(degree=90, include_bias=False), StandardScaler())\nX_train = preprocessing.fit_transform(x_train)\nX_test = preprocessing.transform(x_test)\n\n\nsgd = SGDRegressor(penalty='elasticnet', alpha=0.01, l1_ratio=0.5, eta0=0.001, random_state=29)\n\nfor epoch in range(epochs):\n    sgd.partial_fit(X_train, y_train.ravel())\n    y_pred = sgd.predict(X_test)\n    rmse = mean_squared_error(y_test, y_pred, squared=False)\n    if rmse &lt; best_rmse:\n        best_rmse = rmse\n        best_model = deepcopy(sgd)\n\ny_pred = best_model.predict(X_test)\n\n\n## Another way to apply early stopping\n\n# sgd = SGDRegressor(penalty='elasticnet', alpha=0.01, l1_ratio=0.5, max_iter = 2000, tol=0.00001, shuffle=True, random_state=29, learning_rate='invscaling', eta0=0.001, early_stopping=True, validation_fraction=0.25, n_iter_no_change=10)\n# sgd.fit(X_train, y_train.ravel())\n# y_pred = sgd.predict(X_test)\nprint('RMSE: %.2f' % mean_squared_error(y_test, y_pred))\n\nRMSE: 0.67\n\n\n\n\n\n\n\n\npartial_fit: max_iter=1 (fit 1 epoch per calling); learn incrementally from a mini-batch of instances =&gt; useful when data is not fit into memory\nfit: train model from scratch (all instances at once)\nfit(warm_start=True) = partial_fit: allow learning from the weights of previous fit\ncopy.deepcopy(): copies both the model’s hyperparameters and the learned parameters\nsklearn.base.clone() only copies the model’s hyperparameters."
  },
  {
    "objectID": "hands-on-regression.html#datasets",
    "href": "hands-on-regression.html#datasets",
    "title": "5  Hands-on regression",
    "section": "5.1 Datasets",
    "text": "5.1 Datasets\nPopular open data repositories\n\nhttps://openml.org/\n\nhttps://Kaggle.com\nhttps://PapersWithCode.com\n\nUC Irvine Machine Learning Repository — Amazon’s AWS datasets\nTensorFlow datasets\n\nMeta portals (they list open data repositories)\n\nhttps://DataPortals.org\n\nhttps://OpenDataMonitor.eu\n\nOther pages listing many popular open data repositories\n\nWikipedia’s list of machine learning datasets\nhttps://Quora.com\nThe datasets subreddit\n\nIn this chapter we’ll use the California Housing Prices dataset from the StatLib repository. This dataset is based on data from the 1990 California census. It is not exactly recent but it has many qualities for learning."
  },
  {
    "objectID": "hands-on-regression.html#look-at-the-big-picture",
    "href": "hands-on-regression.html#look-at-the-big-picture",
    "title": "5  Hands-on regression",
    "section": "5.2 Look at the big picture",
    "text": "5.2 Look at the big picture\nQuestions\n\nBusiness objective? Current solution (if any)?\nHow to use and benefit from the model?\nData Pipelines?\nDetermine kind of model\nSelect preformance measures\nCheck the Assumptions\n\nAnswers\n\nPredict median housing price in any district. The results are used for another ML system for investment analysis. The current solution is to gather up-to-date information about a district, or to estimate manually using complex rules if no information.\nThe current solution is costly and time-consuming, and it was often off by more than 30%. Therefore, a ML model could be more useful.\nData pipelines: A sequence of data processing components. Pipelines are very common in machine learning systems, since there is a lot of data to manipulate and many data transformations to apply.\nThis is a regression task (labeled data), batch learning (data is quite small and do not change too much) and model-based learning\nMetrics for regression:\n\n\n\n\n\n\n\nExpand to learn more about metrics\n\n\n\n\n\nBoth the RMSE and the MAE are ways to measure the distance between two vectors: the vector of predictions and the vector of target values. Various distance measures, or norms, are possible:\n- Computing the root of a sum of squares (RMSE) corresponds to the Euclidean norm: this is the notion of distance we are all familiar with. It is also called the l2 norm, noted |·|₂ (or just |·|).\n- Computing the sum of absolutes (MAE) corresponds to the l1 norm, noted |·|₁. This is sometimes called the Manhattan norm because it measures the distance between two points in a city if you can only travel along orthogonal city blocks.\n- More generally, the lk norm of a vector v containing n elements is defined as ∥v∥k = (|v₁|ᵏ + |v₂|ᵏ + … + |vₙ|ᵏ)¹/ᵏ. l0 gives the number of nonzero elements in the vector, and l∞ gives the maximum absolute value in the vector.\nThe higher the norm index, the more it focuses on large values and neglects small ones. This is why the RMSE is more sensitive to outliers than the MAE. But when outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs very well and is generally preferred. \n\n\n\nRMSE (root mean squared error): l2 norm\n\\[\nRMSE(X, y)  = \\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}(y_{hat}^{(i)} - y^{(i)})^2}\n\\]\nMAE (mean squared error): l1 norm\n\\[\nMAE(X, y)  = \\frac{1}{m}\\sum_{i=1}^{m}|y_{hat}^{(i)} - y^{(i)}|\n\\]\n\nCheck with the team in charge of the downstream system that use out output whether it is suitable or not (e.g. it is terrible if after several months building model you realize that they need ordinal output not numerical one)"
  },
  {
    "objectID": "hands-on-regression.html#get-the-data",
    "href": "hands-on-regression.html#get-the-data",
    "title": "5  Hands-on regression",
    "section": "5.3 Get the Data",
    "text": "5.3 Get the Data\n\n5.3.1 Download Data\n\n## Load data\n\nfrom pathlib import Path\nimport pandas as pd\nimport tarfile\nimport urllib.request\n\ndef load_data(url):\n    path = Path(\"datasets/housing.tgz\")\n    if not path.is_file():\n        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n        urllib.request.urlretrieve(url, path)\n        tarfile.open(path).extractall(path='datasets')\n    return pd.read_csv('datasets/housing/housing.csv')\n\nurl = 'https://github.com/ageron/data/raw/main/housing.tgz'\ndata = load_data(url)\n\n\n\n5.3.2 Quick Look to Data Structure: head(), info(), describe(), value_counts(), histplot()\n\n## Quick look\n\npd.set_option('display.max_columns', None)      # display all columns\ndata.head()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n0\n-122.23\n37.88\n41.0\n880.0\n129.0\n322.0\n126.0\n8.3252\n452600.0\nNEAR BAY\n\n\n1\n-122.22\n37.86\n21.0\n7099.0\n1106.0\n2401.0\n1138.0\n8.3014\n358500.0\nNEAR BAY\n\n\n2\n-122.24\n37.85\n52.0\n1467.0\n190.0\n496.0\n177.0\n7.2574\n352100.0\nNEAR BAY\n\n\n3\n-122.25\n37.85\n52.0\n1274.0\n235.0\n558.0\n219.0\n5.6431\n341300.0\nNEAR BAY\n\n\n4\n-122.25\n37.85\n52.0\n1627.0\n280.0\n565.0\n259.0\n3.8462\n342200.0\nNEAR BAY\n\n\n\n\n\n\n\nThere are total 10 features, each row represents a district observation\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 10 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   longitude           20640 non-null  float64\n 1   latitude            20640 non-null  float64\n 2   housing_median_age  20640 non-null  float64\n 3   total_rooms         20640 non-null  float64\n 4   total_bedrooms      20433 non-null  float64\n 5   population          20640 non-null  float64\n 6   households          20640 non-null  float64\n 7   median_income       20640 non-null  float64\n 8   median_house_value  20640 non-null  float64\n 9   ocean_proximity     20640 non-null  object \ndtypes: float64(9), object(1)\nmemory usage: 1.6+ MB\n\n\nData with 10 columns and 20640 rows 9 numerical features, 1 categorical feature ‘total_bedrooms’ has only 20433 non-null values\n\ndata.describe(include='all')        # describe all type of data\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\ncount\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20433.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640\n\n\nunique\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n5\n\n\ntop\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n&lt;1H OCEAN\n\n\nfreq\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n9136\n\n\nmean\n-119.569704\n35.631861\n28.639486\n2635.763081\n537.870553\n1425.476744\n499.539680\n3.870671\n206855.816909\nNaN\n\n\nstd\n2.003532\n2.135952\n12.585558\n2181.615252\n421.385070\n1132.462122\n382.329753\n1.899822\n115395.615874\nNaN\n\n\nmin\n-124.350000\n32.540000\n1.000000\n2.000000\n1.000000\n3.000000\n1.000000\n0.499900\n14999.000000\nNaN\n\n\n25%\n-121.800000\n33.930000\n18.000000\n1447.750000\n296.000000\n787.000000\n280.000000\n2.563400\n119600.000000\nNaN\n\n\n50%\n-118.490000\n34.260000\n29.000000\n2127.000000\n435.000000\n1166.000000\n409.000000\n3.534800\n179700.000000\nNaN\n\n\n75%\n-118.010000\n37.710000\n37.000000\n3148.000000\n647.000000\n1725.000000\n605.000000\n4.743250\n264725.000000\nNaN\n\n\nmax\n-114.310000\n41.950000\n52.000000\n39320.000000\n6445.000000\n35682.000000\n6082.000000\n15.000100\n500001.000000\nNaN\n\n\n\n\n\n\n\n\nfor ft in data.select_dtypes('object'):     # choose 'object' features only\n    print(data[ft].value_counts())\n    print(f'Number of classes: {data[ft].nunique()}')\n\nocean_proximity\n&lt;1H OCEAN     9136\nINLAND        6551\nNEAR OCEAN    2658\nNEAR BAY      2290\nISLAND           5\nName: count, dtype: int64\nNumber of classes: 5\n\n\nQuite imbalanced classes\n\n## Plot histogram of numerical features\n\nimport matplotlib.pyplot as plt\n\ndata.hist(bins=50, figsize=(8,8))\nplt.show()\n\n\n\n\nhousing_median_age: capped at range (1,52)\nmedian_income: scaled and capped at range ~ (0.5, 15)\nmedian_house_value: capped at top range of 500,000\nThese features are skewed and have very different scales =&gt; Transforming and Feature Scaling\n\n\n5.3.3 Create train, val and test set: train_test_split()\nrandom sampling: data must be large enough, otherwise there is a risk of sampling bias stratified sampling: based on some very important features, help avoid bias and ensure train set and test set are representative to full dataset\nHere we assump that median_income is very important feature to predict household value\n\n## Create new feature (income group)\nimport numpy as np\n\ndata['income_grp'] = pd.cut(data['median_income'], bins=[0,1.5,3,4.5,6,np.inf], labels=[1,2,3,4,5])\n\ndata.income_grp.value_counts().sort_index().plot.bar(rot=0, grid=True)\nplt.title('Frequency of income group')\nplt.xlabel('Income group')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n## Split data into train, val, test set\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(data.drop('median_house_value', axis=1), data.median_house_value, stratify=data['income_grp'], test_size=0.2, random_state=24)\n\n\n## Drop the 'income_grp' after using\n\nfor df in [x_train, x_test]:\n    df.drop('income_grp', axis=1, inplace=True)"
  },
  {
    "objectID": "hands-on-regression.html#explore-and-visualize-the-data-to-gain-insights",
    "href": "hands-on-regression.html#explore-and-visualize-the-data-to-gain-insights",
    "title": "5  Hands-on regression",
    "section": "5.4 Explore and visualize the data to gain insights",
    "text": "5.4 Explore and visualize the data to gain insights\nIf the data is very large, we will sample an exploration set to manipulate faster and easier. On the other hand, just work directly on full set if the data is quite small.\n\ndf_vis = data.copy()\n\n\n5.4.1 Visualize\nPlot\n\n\n\n\n\nFigure 5.1: The geographical plot of data\n\n\n\n\nLarge point size: larger population Blue -&gt; red: higher house price\nCorrelation\nThere are 2 ways to perform: heatmap and pairgrid map\n\n\n\n\n&lt;Axes: &gt;\n(a) Correlation Plot\n\n\n\n\n\n\n(b)\n\n\n\nFigure 5.2: ?(caption)\n\n\n\n\n\n\n\nFigure 5.3: PairGrid Plot of numerical features\n\n\n\n\nTry pd.plotting.scatter_matrix\nLook closely too the relation between ‘median_house_value’ and ‘median_income’, we see there is a strong positive correlation, but there are some clearly horizontal line at 500,000; 450,000; 350,000 and roughly 280,000. We should remove these instances to prevent the algorithms learning these patterns.\n\n\n\n\n\nFigure 5.4: Median income versus median house value\n\n\n\n\n\n\n5.4.2 Attributes combination\nUseful when we want to find better features to predict\n\ndf_vis['room_per_house'] = df_vis['total_rooms']/df_vis['households']\ndf_vis['bedroom_ratio'] = df_vis['total_bedrooms']/df_vis['total_rooms']\ndf_vis['people_per_house'] = df_vis['population']/df_vis['households']\n\ncorr_matrix = df_vis.corr(numeric_only=True)\ncorr_matrix['median_house_value'].sort_values(ascending=False)\n\nmedian_house_value    1.000000\nmedian_income         0.688075\nroom_per_house        0.151948\ntotal_rooms           0.134153\nhousing_median_age    0.105623\nhouseholds            0.065843\ntotal_bedrooms        0.049686\npeople_per_house     -0.023737\npopulation           -0.024650\nlongitude            -0.045967\nlatitude             -0.144160\nbedroom_ratio        -0.255880\nName: median_house_value, dtype: float64"
  },
  {
    "objectID": "hands-on-regression.html#explore-and-visualize-data",
    "href": "hands-on-regression.html#explore-and-visualize-data",
    "title": "5  Hands-on regression",
    "section": "5.5 Explore and Visualize Data",
    "text": "5.5 Explore and Visualize Data\n\nVisualize\nCompute correlation (corr_matrix(), pandas.plotting.scatter_matrix() )\nAttributes combination"
  },
  {
    "objectID": "hands-on-regression.html#prepare-data",
    "href": "hands-on-regression.html#prepare-data",
    "title": "5  Hands-on regression",
    "section": "5.6 Prepare Data",
    "text": "5.6 Prepare Data\nBenefits: - Reproduce tranformations easily - Build library of tranformations for future projects - Use in live system to transform new data - Try various transformations =&gt; Choose best combination of transformations\n\n5.6.1 Clean Data\nMissing Values: - Get rid of them - Get rid of whole attributes - Set new values (imputation): zero, mean, median, most_frequent, constant, etc. - sklearn.impute.SimpleImputer() - Apply to all numerical variables cause we do not know there will not be any missing values in the future. - More powerful imputer: KnnImputer(), IterativeImputer()\n\n\n5.6.2 Handling Text and Categorical Attributes\n\nOneHotEncoder, LabelEncoder, OrdinalEncoder\n\ndefault: Scipy sparse matrix =&gt; set sparse=False or toarray()\npandas.get_dummies(): generate new columns for unknown categories\nOneHotEncoder: detect unknown categories\n\nIf a categorical attribute has a large number of possible categories =&gt; OneHotEncoder will be result in a large number of input features\n\nTurn categorical -&gt; numerical category\nIn neural networks: replace each category -&gt; embedding (a learnable, low-dimensional vector)\n\n\n\n\n5.6.3 Feature Scaling and Tranformations\n\n5.6.3.1 For input attributes\n\nSome ML algorithms don’t perform well when the input numerical attributes have very different scales.\nWithout any scaling, most models will be biased toward ignoring the small values and focusing more on the larger values.\n\nTwo common feature scaling techniques: min-max scaling and standardization - Use fit or fit_transform only on training set - Training set will always be scaled to specific range, if new data contains outliers, these may end up scaled outside the range =&gt; Set clip hyperparameter to True to avoid.\n\nMin-max scaling (normalization):\nDefault range: 0-1\nMinMaxScaler(feature_range=(-1,1)): change the prefered range\nStandardization:\nLess affected by outliers\nStandardScaler(with_mean=False): only divide the data by the standard deviation, without subtracting the mean =&gt; scale sparse matrix without converting to dense matrix\nHeavy-tailed attributes:\nBoth min-max scaling and standardization will squash most values into a small range\nSolutions: square root (or power 0-1), logarithm, bucketizing\n\nbucketizing: chop its distribution into equal-sized buckets =&gt; replace with the index (i.e. percentile, categories (for multimodal distribution)) of buckets.\n\nmultimodal distribution: 2 or more clear peaks (also called mode).\n\nother options for multimodal distribution: Gaussian radial basic function (RBF) measure the similarity between that attribute and the modes.\n\n\n\n\n5.6.3.2 For output\nAfter feature scaling the target values to make predictions on new data, it has to be inverse_transform(), or just do TransformedTargetRegressor.\n\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\n\nmodel = TransformedTargetRegressor(LinearRegression(),\n                                       transformer=StandardScaler())\n\n# model.fit(housing[[\"median_income\"]], housing_labels)\n# predictions = model.predict(some_new_data)\n\n\n\n\n5.6.4 Custom Transformers\nWrite own custom transformers: custom transformations, cleanup operations, or combining specific attributes.\n\nfrom sklearn.preprocessing import FunctionTransformer\n\nFor details, check p.79, 80, 81\n\nA transformer should contains:\n\nfit(): self.n_features_in_ , return self\nget_feature_names_out() =&gt; to create DataFrame after transform\ninverse_transform()\n\nThis is a custom transformer using KMeans clusterer in the fit() method to identify the main clusters in the training data, and then uses rbf_kernel() in the transform() method to measure how similar each sample is to each cluster center:\n\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass ClusterSimilarity(BaseEstimator, TransformerMixin):  \n    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):\n        self.n_clusters = n_clusters\n        self.gamma = gamma\n        self.random_state = random_state\n\n    def fit(self, X, y=None, sample_weight=None):  \n        self.kmeans_ = KMeans(self.n_clusters, random_state=self.random_state) \n        self.kmeans_.fit(X, sample_weight=sample_weight)  \n        return self # always return self!\n\n    def transform(self, X):  \n        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)\n\n    def get_feature_names_out(self, names=None):  \n        return [f\"Cluster {i} similarity\" for i in range(self.n_clusters)]\n\nFor details, check p.82\n\nYou can check whether your custom estimator respects Scikit-Learn’s API by passing an instance to check_estimator() from the sklearn.utils.estimator_checks package. For the full API, check out https://scikit-learn.org/stable/developers.\n\n\n\n5.6.5 Transformation Pipelines\n\nPipeline: sequence of transformations =&gt; Take list of names/estimators\n\nNames must be unique and do not contain double underscores\nFirst (n-1) names: transformers Last name: regardless transformer or predictor\n\n\n2 ways:\nPipeline\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nnum_pipeline = Pipeline([\n    (\"impute\", SimpleImputer(strategy=\"median\")),\n    (\"standardize\", StandardScaler()),\n])\n\nmake_pipeline (don’t care naming estimators)\n\nfrom sklearn.pipeline import make_pipeline  \n\nnum_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n\n\nThe pipeline exposes the same methods as the final estimator (transformer or predictor)\nIn a Jupyter notebook, if we import sklearn and run sklearn.set_config(display=“diagram”), all Scikit-Learn estimators will be rendered as interactive diagrams. This is particularly useful for visualizing pipelines. To visualize num_pipeline, run a cell with num_pipeline as the last line. Clicking an estimator will show more details.\n2 ways to apply transform for numerical attributes and categorical attributes seperately:\n\nColumnTransformer:\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\nnum_attribs = [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \"population\", \"households\", \"median_income\"]\n\ncat_attribs = [\"ocean_proximity\"]\n\nnum_pipeline = Pipeline([\n    (\"impute\", SimpleImputer(strategy=\"median\")),\n    (\"standardize\", StandardScaler()),\n\n])\ncat_pipeline = make_pipeline(\n    SimpleImputer(strategy=\"most_frequent\"),\n    OneHotEncoder(handle_unknown=\"ignore\"))\n\npreprocessing = ColumnTransformer([\n    (\"num\", num_pipeline, num_attribs),\n    (\"cat\", cat_pipeline, cat_attribs),\n\n])\n\nmake_column_selector, make_column_transformer (don’t care naming estimators)\n\nfrom sklearn.compose import make_column_selector, make_column_transformer\n\npreprocessing = make_column_transformer(\n    (num_pipeline, make_column_selector(dtype_include=np.number)),\n    (cat_pipeline, make_column_selector(dtype_include=object)),\n\n)\n\n\nDrop or passthrough columns: *For details, see p.86\nRecap Pipeline\nMissing values in numerical features will be imputed by replacing them with the median, as most ML algorithms don’t expect missing values. In categorical features, missing values will be replaced by the most frequent category.\nThe categorical feature will be one-hot encoded, as most ML algorithms only accept numerical inputs.\nA few ratio features will be computed and added: bedrooms_ratio, rooms_per_house, and people_per_house. Hopefully these will better correlate with the median house value, and thereby help the ML models.\nA few cluster similarity features will also be added. These will likely be more useful to the model than latitude and longitude.\nFeatures with a long tail will be replaced by their logarithm, as most models prefer features with roughly uniform or Gaussian distributions.\nAll numerical features will be standardized, as most ML algorithms prefer when all features have roughly the same scale.\n\nFinal Pipeline:\n\ndef column_ratio(X):  \n    return X[:, [0]] / X[:, [1]]\n    \ndef ratio_name(function_transformer, feature_names_in): \n    return [\"ratio\"] #feature names out\n    \ndef ratio_pipeline(): \n    return make_pipeline(\n        SimpleImputer(strategy=\"median\"),\n        FunctionTransformer(column_ratio, feature_names_out=ratio_name),\n        StandardScaler())\n    \nlog_pipeline = make_pipeline(\n    SimpleImputer(strategy=\"median\"),\n    FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),\n    StandardScaler())\n    \ncluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\n\ndefault_num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"),\n        StandardScaler())\n    \npreprocessing = ColumnTransformer([\n    (\"bedrooms\", ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]),\n    (\"rooms_per_house\", ratio_pipeline(), [\"total_rooms\", \"households\"]),\n    (\"people_per_house\", ratio_pipeline(), [\"population\", \"households\"]),\n    (\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\",\n\n                            \"households\", \"median_income\"]),\n    (\"geo\", cluster_simil, [\"latitude\", \"longitude\"]),\n\n    (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)),\n    ],\n    remainder=default_num_pipeline) # one column remaining: housing_median_age"
  },
  {
    "objectID": "ensemble-learning.html#bagging",
    "href": "ensemble-learning.html#bagging",
    "title": "11  Ensemble Learning",
    "section": "11.1 Bagging",
    "text": "11.1 Bagging\n\n11.1.1 Voting Classifier\nThe simpliest bagging algorithm with no boostrapping and only aggregating. It aggregates predictions by choosing most voted class (hard voting) or class with highest probability (soft voting).\nIn sklearn.ensemble.VotingClassifier, it performs hard voting by default.\n\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier, RandomForestClassifier\nfrom sklearn.svm import SVC\n\nX, y = make_moons(n_samples=500, noise=0.30, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\nvoting_clf = VotingClassifier(\n        estimators=[\n            ('lr', LogisticRegression(random_state=42)),\n            ('rf', RandomForestClassifier(random_state=42)),\n            ('svc', SVC(random_state=42))\n] )\nvoting_clf.fit(X_train, y_train)\n\nprint('Accuracy of individual predictor:')\nfor name,clf in voting_clf.named_estimators_.items():\n    print(f'{name} = {clf.score(X_test,y_test)}')\n\nprint(f'Accuracy of ensemble: {voting_clf.score(X_test, y_test)}')\n\nAccuracy of individual predictor:\nlr = 0.864\nrf = 0.896\nsvc = 0.896\nAccuracy of ensemble: 0.912\n\n\nChange to soft voting\n\nvoting_clf.voting = 'soft'\nvoting_clf.named_estimators['svc'].probability = True\nvoting_clf.fit(X_train, y_train)\nvoting_clf.score(X_test, y_test)\n\n0.92\n\n\n\n\n11.1.2 Bagging and Pasting\n\nTraining multiple learner parallely =&gt; Scale very well.\nSampling with replacement (Bagging); sampling without replacement (pasting).\n\n\n\n\nFigure 11.1: Bagging and Pasting\n\n\n\nSampling instances : max_samples indicates number/proportion of instances used to train model; bootstrap=True/False.\nSampling features: max_features and bootstrap_features work the same.\n\n\nRandom patches: sampling instances and features\n\nRandom subspaces: sampling only features.\n\n\nThe randomness trades a higher bias for lower variance.\n\n\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nbagging_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, max_samples=100, random_state=29)\n\nbagging_clf.fit(X_train, y_train)\nbagging_clf.score(X_train, y_train)\n\n0.9413333333333334\n\n\n\n\n\nFigure 11.2: Bagging decision trees\n\n\n\nMeasure performance by Out-Of-Bag Error: set oob_score=True; the score made on the remaining not sampled instances.\n\n\nbagging_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, oob_score=True, max_samples=100, random_state=29)\n\nbagging_clf.fit(X_train, y_train)\nbagging_clf.oob_score_\n\n0.9226666666666666\n\n\n\n\n11.1.3 Random Forest\nRandom forest is ensemble of decision trees using bagging on full dataset. RandomForestClassifier has all the hyperparameters of DecisionTreeClassifier and BaggingClassifier to control ensemble.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, min_samples_leaf=10, random_state=29)\nrf_clf.fit(X_train, y_train)\nrf_clf.score(X_test, y_test)\n\n0.92\n\n\nAdd more randomness:\n\nDecision tree: searching for the best possible thresholds of each feature.\nExtra-trees: search for best feature using random thresholds for each feature. Use RandomForestClassifier(splitter=“random”) or ExtraTreesClassifier.\n\n\nbagging_clf = BaggingClassifier(DecisionTreeClassifier(splitter='random'), n_estimators=500, random_state=29)\nbagging_clf.fit(X_train, y_train)\nprint(f'Random forest: {bagging_clf.score(X_test, y_test)}')\n\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nextra_tree = ExtraTreesClassifier(n_estimators=500, random_state=29)\nextra_tree.fit(X_train, y_train)\nprint(f'Extra trees: {extra_tree.score(X_test, y_test)}')\n\nRandom forest: 0.888\n\n\nExtra trees: 0.88\n\n\n\n\n11.1.4 Feature importance\nAlthough we have state that bagging method treat all features equally, there is still a method to get the weighted average after training by look at the proportion of training samples used to reduce impurity. This can be used to perform features selection.\n\nfrom sklearn.datasets import load_iris\niris = load_iris(as_frame=True)\nrf_clf = RandomForestClassifier(n_estimators=500, random_state=42)\nrf_clf.fit(iris.data, iris.target)\n\nfor score,feature in zip(rf_clf.feature_importances_, iris.data.columns):\n    print(f'{feature}: {score:.3f}')\n\nsepal length (cm): 0.112\nsepal width (cm): 0.023\npetal length (cm): 0.441\npetal width (cm): 0.423"
  },
  {
    "objectID": "ensemble-learning.html#boosting",
    "href": "ensemble-learning.html#boosting",
    "title": "11  Ensemble Learning",
    "section": "11.2 Boosting",
    "text": "11.2 Boosting\nWe will talk about AdaBoost (adaptive boosting) and Gradient boosting.\n\n11.2.1 AdaBoost\nThe algorithm work by training predictors sequentially. First it trains the base model and make predictions, then train the new pridictor weighting more on misclassified instances of the previous one, and so on. This is one of the most powerful model, and its main drawback is just do not scale really well.\n\n\n\nFigure 11.3: AdaBoost\n\n\nFundamentals:\n\nInitialize weights w(i) of each instance equal to 1/m\nTrain base predictor\nPredict and get weighted error rate r(j) Weighted error rate of the j(th) predictor \\[r_j = \\sum_{i=1}^{m}w^{(i)}\\;\\;\\;with\\;yhat_{j}^{(i)}\\;≠\\;y^{(i)}\\]\nCompute predictor’s weight α(j). The more accurate the predictor, the higher its α(j)\n\n\\[α_j = ηlog(\\frac{1-r_j}{r_j})\\]\n\nUpdate instances’s weights. This give more weights on misclassified instances.\n\n\\[\n\\begin{equation}\n\\begin{split}\nw^{(i)} &= w^{(i)}\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;if\\;yhat_{j}^{(i)} = y^{(i)}\\\\\n&=w^{(i)}exp(α_j)\\;\\;if\\;yhat_{j}^{(i)} ≠ y^{(i)}\n\\end{split}\n\\end{equation}\n\\]\n\nNormalize all instances’s weights.\n\n\\[w^{(i)} = \\frac{w^{(i)}}{\\sum_{i=1}^{m}w^{(i)}}\\]\n\nTrain new predictor using these weights\nStop training when number of predictors is reached, or when a perfect predictor is found.\nPredict by majority vote.\n\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\nada_clf = AdaBoostClassifier(estimator=DecisionTreeClassifier(), learning_rate=0.001, n_estimators=100, random_state=29)\n\nada_clf.fit(X_train, y_train)\nada_clf.score(X_test, y_test)\n\n0.824\n\n\n\n\n11.2.2 Gradient Boosting\nAs same as AdaBoost, but instead of tweaking the instance weights, this method tries to fit the new predictor to the log loss (classification)/residual errors (regression) made by the previous predictor.\nRegularization technique: shrinkage, adjust the learning rate.\nSampling instances: Stochastic Gradient Boosting; set subsample hyperparameter.\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngb_clf = GradientBoostingClassifier(n_estimators=50, max_depth=2, learning_rate=0.05, n_iter_no_change=10)\ngb_clf.fit(X_train, y_train)\ngb_clf.score(X_test, y_test)\n\n0.912\n\n\n\n\n11.2.3 Histogram-Based Gradient Boosting\nOptimize for large dataset. It works by binning the input features, replacing them with integers, in which max_bins ≤ 255. It’s more faster but causes a precision loss =&gt; Risk of underfitting.\n\n\n\n\n\n\nLearn more about XGBoost, CatBoost and LightGBM"
  },
  {
    "objectID": "ensemble-learning.html#stacking-stacked-generalization",
    "href": "ensemble-learning.html#stacking-stacked-generalization",
    "title": "11  Ensemble Learning",
    "section": "11.3 Stacking (Stacked Generalization)",
    "text": "11.3 Stacking (Stacked Generalization)\nStacking method works by training a model to aggregate the predictions instead of majority voting like bagging method. This model, also called blender or meta learner, uses the prediction of weak learners (out-of-sample) as input and makes final prediction.\n\n\n\nFigure 11.4: Stacking method\n\n\n\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.svm import LinearSVC\n\nstacking_clf = StackingClassifier(\n    estimators=[\n        ('logis', LogisticRegression(random_state=29)),\n        ('rf', RandomForestClassifier(random_state=29)),\n        ('svc', LinearSVC(random_state=29))\n    ],\n    final_estimator=RandomForestClassifier(random_state=29),\n    cv=5\n)\nstacking_clf.fit(X_train, y_train)\nstacking_clf.score(X_test, y_test)\n\n/usr/local/anaconda3/envs/dhuy/lib/python3.11/site-packages/sklearn/svm/_classes.py:32: FutureWarning:\n\nThe default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n\n\n\n/usr/local/anaconda3/envs/dhuy/lib/python3.11/site-packages/sklearn/svm/_classes.py:32: FutureWarning:\n\nThe default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n\n/usr/local/anaconda3/envs/dhuy/lib/python3.11/site-packages/sklearn/svm/_classes.py:32: FutureWarning:\n\nThe default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n\n/usr/local/anaconda3/envs/dhuy/lib/python3.11/site-packages/sklearn/svm/_classes.py:32: FutureWarning:\n\nThe default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n\n/usr/local/anaconda3/envs/dhuy/lib/python3.11/site-packages/sklearn/svm/_classes.py:32: FutureWarning:\n\nThe default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n\n/usr/local/anaconda3/envs/dhuy/lib/python3.11/site-packages/sklearn/svm/_classes.py:32: FutureWarning:\n\nThe default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n\n\n\n0.896\n\n\nIt can be created with layer of blenders such like this\n\n\n\nFigure 11.5: Multilayer stacking\n\n\nIf you don’t provide a final estimator, StackingClassifier will use LogisticRegression and StackingRegressor will use RidgeCV."
  },
  {
    "objectID": "unsupervised-learning.html",
    "href": "unsupervised-learning.html",
    "title": "12  Unsupervised Learning",
    "section": "",
    "text": "Clustering: k-means, DBSCAN Density estimation: Gaussian Mixture model\nIn progress"
  },
  {
    "objectID": "metrics.html#sklearn-documents",
    "href": "metrics.html#sklearn-documents",
    "title": "13  Metrics and scoring",
    "section": "13.1 Sklearn documents",
    "text": "13.1 Sklearn documents\nsklearn.metrics"
  },
  {
    "objectID": "metrics.html#classification",
    "href": "metrics.html#classification",
    "title": "13  Metrics and scoring",
    "section": "13.2 Classification",
    "text": "13.2 Classification\n\n13.2.1 Accuracy\n\naccuracy(y_true, y_pred)\nProportion of exact match prediction of the model\nAs this case, the result showed that with the presence of class immbalance, accuracy score is not useful metric\n\n\n\n13.2.2 Confusion matrix\n\nIn progress"
  },
  {
    "objectID": "hands-on-classification.html#describe-the-used-dataset",
    "href": "hands-on-classification.html#describe-the-used-dataset",
    "title": "7  Hands-on Classification",
    "section": "7.1 Describe the used dataset",
    "text": "7.1 Describe the used dataset\n\nName: MNIST\nAuthor: Yann LeCun, Corinna Cortes, Christopher J.C. Burges\nContent: 70,000 images of digits handwritten\nSource: MNIST Website"
  },
  {
    "objectID": "hands-on-classification.html#get-data",
    "href": "hands-on-classification.html#get-data",
    "title": "7  Hands-on Classification",
    "section": "7.2 Get data",
    "text": "7.2 Get data\n\n7.2.1 Download data\n\nfrom sklearn.datasets import fetch_openml\n\nmnist = fetch_openml('mnist_784', as_frame=False)       # as_frame=False: get data as Numpy Array instead of Pandas DataFrame\nmnist.DESCR\n\n/usr/local/anaconda3/envs/dhuy/lib/python3.11/site-packages/sklearn/datasets/_openml.py:1022: FutureWarning:\n\nThe default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n\n\n\n\"**Author**: Yann LeCun, Corinna Cortes, Christopher J.C. Burges  \\n**Source**: [MNIST Website](http://yann.lecun.com/exdb/mnist/) - Date unknown  \\n**Please cite**:  \\n\\nThe MNIST database of handwritten digits with 784 features, raw data available at: http://yann.lecun.com/exdb/mnist/. It can be split in a training set of the first 60,000 examples, and a test set of 10,000 examples  \\n\\nIt is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image. It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting. The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.  \\n\\nWith some classification methods (particularly template-based methods, such as SVM and K-nearest neighbors), the error rate improves when the digits are centered by bounding box rather than center of mass. If you do this kind of pre-processing, you should report it in your publications. The MNIST database was constructed from NIST's NIST originally designated SD-3 as their training set and SD-1 as their test set. However, SD-3 is much cleaner and easier to recognize than SD-1. The reason for this can be found on the fact that SD-3 was collected among Census Bureau employees, while SD-1 was collected among high-school students. Drawing sensible conclusions from learning experiments requires that the result be independent of the choice of training set and test among the complete set of samples. Therefore it was necessary to build a new database by mixing NIST's datasets.  \\n\\nThe MNIST training set is composed of 30,000 patterns from SD-3 and 30,000 patterns from SD-1. Our test set was composed of 5,000 patterns from SD-3 and 5,000 patterns from SD-1. The 60,000 pattern training set contained examples from approximately 250 writers. We made sure that the sets of writers of the training set and test set were disjoint. SD-1 contains 58,527 digit images written by 500 different writers. In contrast to SD-3, where blocks of data from each writer appeared in sequence, the data in SD-1 is scrambled. Writer identities for SD-1 is available and we used this information to unscramble the writers. We then split SD-1 in two: characters written by the first 250 writers went into our new training set. The remaining 250 writers were placed in our test set. Thus we had two sets with nearly 30,000 examples each. The new training set was completed with enough examples from SD-3, starting at pattern # 0, to make a full set of 60,000 training patterns. Similarly, the new test set was completed with SD-3 examples starting at pattern # 35,000 to make a full set with 60,000 test patterns. Only a subset of 10,000 test images (5,000 from SD-1 and 5,000 from SD-3) is available on this site. The full 60,000 sample training set is available.\\n\\nDownloaded from openml.org.\"\n\n\n\n\n7.2.2 Quick Look\n\n## Size of dataset\n\nX,y = mnist.data, mnist.target\nprint(X.shape, y.shape)\n\n(70000, 784) (70000,)\n\n\n\n## Quick look\n\nimport matplotlib.pyplot as plt\n\ndef plot_digit(data):\n    image = data.reshape(28,28)\n    plt.imshow(image, cmap='binary')   # binary: grayscale color map from 0 (white) to 255 (black)\n    \nsome_digit = X[0]    # Look at first digit\nplot_digit(some_digit)\nplt.show()\n\n\n\n\n\n\n7.2.3 Create train, test set\n\n## Split dataset into train set and test set as its describe (train: first 60000 images, test: last 10000 images)\n\nX_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\nprint(X_train.shape)\n\n(60000, 784)"
  },
  {
    "objectID": "hands-on-classification.html#create-a-binary-classfier5-or-non-5",
    "href": "hands-on-classification.html#create-a-binary-classfier5-or-non-5",
    "title": "7  Hands-on Classification",
    "section": "7.3 Create a Binary Classfier(5 or non-5)",
    "text": "7.3 Create a Binary Classfier(5 or non-5)\n\n## Target labels\n\ny_train_5 = (y_train == '5')\ny_test_5 = (y_test == '5')\n\n\n7.3.1 Stochastic Gradient Descent\n\n7.3.1.1 Train model\n\nfrom sklearn.linear_model import SGDClassifier\n\nsgd_clf = SGDClassifier(random_state=42)\nsgd_clf.fit(X_train, y_train_5)\n\nsgd_clf.predict([some_digit])\n\narray([ True])\n\n\n\n\n7.3.1.2 Evaluate model\n\n\n\n\n\n\nMetrics:\n- Accuracy\n- Confusion matrix: Precision, Recall (TPR), FPR, ROC, ROC AUC\n- Plot: Precision-Recall Curve, ROC Curve\nUse case:\n- Precision-Recall Curve: aim to care more about false positives than the false negatives\n- Otherwise: ROC Curve\n\n\n\nAccuracy\n\nfrom sklearn.model_selection import cross_val_score\n\ncross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring='accuracy')\n\narray([0.95035, 0.96035, 0.9604 ])\n\n\n\n\n\n\n\n\nThe accuracy scores are pretty good, but it may be due to the class imbalance. Let take a look at a Dummy Model which always classify as the most frequent class\n\n\n\n\n## Dummy classifier\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import cross_val_score\n\ndummy_model = DummyClassifier(random_state=248)\ncross_val_score(dummy_model, X_train, y_train_5, cv=3, scoring='accuracy')\n\narray([0.90965, 0.90965, 0.90965])\n\n\n\n\n\n\n\n\nThe accuracy scores are over 90% because there’s only about 10% of training set are 5 digit\n=&gt; With class imbalance, accuracy score is not a useful metric\n=&gt; We will use other metrics such as Precision, Recall, ROC Curve, AUC\n\n\n\nConfusion Matrix\n\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\n\ny_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\nconfusion_matrix(y_train_5, y_train_pred)\n\narray([[53892,   687],\n       [ 1891,  3530]])\n\n\n\n## Precision and Recall\n\nfrom sklearn.metrics import precision_score, recall_score\n\nprint(f'Precision scores: {precision_score(y_train_5, y_train_pred):.4f}')\nprint(f'Recall scores: {recall_score(y_train_5, y_train_pred):.4f}')\n\nPrecision scores: 0.8371\nRecall scores: 0.6512\n\n\n\n## F1-score\n\nfrom sklearn.metrics import f1_score\n\nprint(f'F1-score: {f1_score(y_train_5, y_train_pred):.4f}')\n\nF1-score: 0.7325\n\n\nPrecision-Recall Trade-off\n\nCompute the scores of all instances in the training using decision_function\nChange the threshold to see the difference\n\n\ny_score = sgd_clf.decision_function([some_digit])\n\nthreshold = [0, 1000, 3000]\nfor thr in threshold:\n    print(f'With threshold of {thr:4d}: predicted value is {y_score&gt;thr}')\n\nWith threshold of    0: predicted value is [ True]\nWith threshold of 1000: predicted value is [ True]\nWith threshold of 3000: predicted value is [False]\n\n\n\n\n\n\n\n\nHow to choose the suitable threshold?\n\nUse Precision-Recall Curve\nprecision_recall_curve: require scores computed from decision_function or probabilities from predict_proba\n\n\n\n\n\n## Precision-Recall Curve\n\n\n### Compute scores by decision_function\n\ny_scores = cross_val_predict(sgd_clf, X_train, y_train_5, method='decision_function')\n\n### Plot Precision-Recall Curve vs Threshold\n\nfrom sklearn.metrics import precision_recall_curve\n\nprecisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n\nplt.plot(thresholds, precisions[:-1], label='Precision', color='darkslateblue')\nplt.plot(thresholds, recalls[:-1], label='Recall', color='crimson')\nplt.grid()\nplt.legend(loc='center left')\nplt.xlim([-100000,40000])\nplt.title('Precision and Recall versus Threshold')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nThe higher Precision, the lower Recall and vice versa\n\n\n\n\n## Plot Precision versus Recall\n\nplt.plot(recalls, precisions)\nplt.title('Precision versus Recall')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.grid()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nDepend on your project, you would trade between precision and recall\n\n\n\n\n## Find Threshold of over 0.90 Precision\n\nidx_90_precision = (precisions &gt;= 0.90).argmax()\nthreshold_90_precision = thresholds[idx_90_precision]\nthreshold_90_precision\n\n3045.9258227053647\n\n\n\ny_train_90_precision = (y_scores &gt; threshold_90_precision)\n\nfrom sklearn.metrics import accuracy_score\nprint(f'Accuracy score: {accuracy_score(y_train_5, y_train_90_precision):.4f}')\nprint(f'Precision score: {precision_score(y_train_5, y_train_90_precision):.4f}')\nprint(f'Recall score: {recall_score(y_train_5, y_train_90_precision):.4f}')\nprint(f'F1 score: {f1_score(y_train_5, y_train_90_precision):.4f}')\n\nAccuracy score: 0.9626\nPrecision score: 0.9002\nRecall score: 0.6587\nF1 score: 0.7608\n\n\n\n## ROC AUC\n\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\nprint(f'AUC score: {roc_auc_score(y_train_5, y_scores):.4f}')       \n\nAUC score: 0.9648\n\n\n\n## ROC Curve\n\nfpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\nidx_threshold_90_precision = (thresholds&lt;=threshold_90_precision).argmax()      # thresholds listed decreasing =&gt; use (&lt;=)\nfpr_90, tpr_90 = fpr[idx_threshold_90_precision], tpr[idx_threshold_90_precision]\n\nplt.plot(fpr, tpr, label='ROC Curve', color='darkslateblue')\nplt.plot([fpr_90], [tpr_90], 'o', label='Threshold for 90% precision', color='crimson')\nplt.title('ROC Curve')\nplt.xlabel('False Positive Rate (Fall-out)')\nplt.ylabel('True Positive Rate (Recall)')\nplt.legend(loc='center right')\nplt.grid()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nAnother trade-off: The higher TPR, the lower FPR and vice versa\n\n\n\n\n\n\n7.3.2 Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\n\nlogistic = LogisticRegression(random_state=29)\n\ny_pred_logis = cross_val_predict(logistic, X_train, y_train_5, cv=3, method='predict_proba')[:,1]\n\n/usr/local/anaconda3/envs/dhuy/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning:\n\nlbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n\n\n\n/usr/local/anaconda3/envs/dhuy/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning:\n\nlbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n\n\n\n/usr/local/anaconda3/envs/dhuy/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning:\n\nlbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n\n\n\n\n## Measure performance\n\nthreshold = 0.5\nf1_logis = f1_score(y_train_5, y_pred_logis&gt;=threshold)\nauc_logis = roc_auc_score(y_train_5, y_pred_logis&gt;=threshold)\n\nprint(f'F1 score Random Forest: {f1_logis:.4f}')\nprint(f'AUC Random Forest: {auc_logis:.4f}')\n\nF1 score Random Forest: 0.8487\nAUC Random Forest: 0.9004\n\n\n\n\n7.3.3 Random Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier(random_state=42)\n\ny_train_pred_rf = cross_val_predict(rf_clf, X_train, y_train_5, cv=3, method='predict_proba')[:,1]\n\n\n## Measure performance\n\nthreshold = 0.5\nf1_rf = f1_score(y_train_5, y_train_pred_rf&gt;=threshold)\nauc_rf = roc_auc_score(y_train_5, y_train_pred_rf&gt;=threshold)\n\nprint(f'F1 score Random Forest: {f1_rf:.4f}')\nprint(f'AUC Random Forest: {auc_rf:.4f}')\n\nF1 score Random Forest: 0.9275\nAUC Random Forest: 0.9358\n\n\n\n## PR Curve\n\nprecisions_rf, recalls_rf, thresholds_rf = precision_recall_curve(y_train_5, y_train_pred_rf)\n\nplt.plot(recalls, precisions, \"-\", label='SGD')\nplt.plot(recalls_rf, precisions_rf, label='Random Forest')\nplt.title('Precision versus Recall')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.legend()\nplt.grid()\n\nplt.show()\n\n\n\n\n\n## ROC Curve\n\nfpr_rf, tpr_rf, thresholds = roc_curve(y_train_5, y_train_pred_rf)\n\nplt.plot(fpr, tpr, label='SGD', color='darkslateblue')\nplt.plot(fpr_rf, tpr_rf, label='Random Forest', color='crimson')\nplt.title('ROC Curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.grid()\n\nplt.show()"
  },
  {
    "objectID": "hands-on-classification.html#multiclass-classification",
    "href": "hands-on-classification.html#multiclass-classification",
    "title": "7  Hands-on Classification",
    "section": "7.4 Multiclass Classification",
    "text": "7.4 Multiclass Classification\n\nLogisticRegression, RandomForestClassifier, GaussianNB: natively handle Multiclass Classification\nSGDClassifier and SVC: strictly binary classifiers\n\novo: one versus one strategy, preferred with scale poorly algorithms (i.e. SVC)\novr: one versus rest strategy, preferred for almost algorithms\n\n\n\n7.4.1 SVC\n\n7.4.1.1 Default: ovo strategy\n\nfrom sklearn.svm import SVC\n\nsvc_clf = SVC(random_state=42)\nsvc_clf.fit(X_train[:1000], y_train[:1000])\nsvc_clf.predict([some_digit])\n\narray(['5'], dtype=object)\n\n\n\n## Scores from decision_function\n\nsome_digit_svc = svc_clf.decision_function([some_digit])\nsome_digit_svc.round(4)\n\narray([[ 1.7583,  2.7496,  6.1381,  8.2854, -0.2873,  9.3012,  0.7423,\n         3.7926,  7.2085,  4.8576]])\n\n\n\n## Class of highest score\n\nidx_svc = some_digit_svc.argmax()\nidx_svc\n\n5\n\n\n\n## Classes of prediction\nsvc_clf.classes_[idx_svc]\n\n'5'\n\n\n\n\n7.4.1.2 Force: ovr strategy\n\n## Train model\n\nfrom sklearn.multiclass import OneVsRestClassifier\n\novr_svc_clf = OneVsRestClassifier(SVC(random_state=42))\novr_svc_clf.fit(X[:1000], y_train[:1000])\novr_svc_clf.predict([some_digit])\n\narray(['5'], dtype='&lt;U1')\n\n\n\n## Compute scores\n\nsome_digit_ovr_svc = ovr_svc_clf.decision_function([some_digit])\nsome_digit_ovr_svc.round(4)\n\narray([[-1.3439, -1.5195, -1.221 , -0.9294, -2.0057,  0.6077, -1.6226,\n        -0.9998, -1.2764, -1.7031]])\n\n\n\n## Class of hishest score\n\nsome_digit_ovr_svc.argmax()\n\n5\n\n\n\n## Extract classes\n\novr_svc_clf.classes_\n\narray(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype='&lt;U1')\n\n\n\n\n\n7.4.2 SGD\n\n## Train model\n\nfrom sklearn.linear_model import SGDClassifier\n\nsgd_clf = SGDClassifier(random_state=42)\nsgd_clf.fit(X_train, y_train)\nsgd_clf.predict([some_digit])\n\narray(['3'], dtype='&lt;U1')\n\n\nThat’s incorrect. As we can see,The Classifier is not very confident about its prediction.\n\n## Compute scores\n\nsgd_clf.decision_function([some_digit])\n\narray([[-31893.03095419, -34419.69069632,  -9530.63950739,\n          1823.73154031, -22320.14822878,  -1385.80478895,\n        -26188.91070951, -16147.51323997,  -4604.35491274,\n        -12050.767298  ]])\n\n\nWe will use cross validation to evaluate our model\n\ncross_val_score(sgd_clf, X_train, y_train, cv=3, scoring='accuracy')\n\narray([0.87365, 0.85835, 0.8689 ])\n\n\nWe can scale the data to get better result\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train.astype('float64'))\ncross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring='accuracy')\n\narray([0.8983, 0.891 , 0.9018])\n\n\nLet’s look at the confusion matrix of our prediction\n\n## Predict using cross_val_predict\n\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\ny_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\n\nConfusion matrix with (right) and without (left) normalization.\n\nfig,ax = plt.subplots(1,2,figsize=(9, 4))\n\nConfusionMatrixDisplay.from_predictions(y_train, y_train_pred, ax=ax[0])\nax[0].set_title(\"Confusion matrix\")\nConfusionMatrixDisplay.from_predictions(y_train, y_train_pred, ax=ax[1], normalize='true', values_format='.0%')\nax[1].set_title(\"CM normalized by row\")\n\nplt.show()\n\n\n\n\nIn row #5 and column #8 on the left plot, it’s means 10% of true 5s is misclassified as 8s. Kinda hard to see the errors made by model. Therefore, we will put 0 weight on correct prediction (error plot).\nConfustion matrix with error normalized by row (left) and by column (right) (normalize=[‘true’,‘pred’])\n\nfig,ax = plt.subplots(1,2,figsize=(9, 4))\n\nsample_weight = (y_train != y_train_pred)\n\nConfusionMatrixDisplay.from_predictions(y_train, y_train_pred, ax=ax[0],sample_weight=sample_weight, normalize='true', values_format='.0%')\nax[0].set_title(\"Confusion matrix\")\nConfusionMatrixDisplay.from_predictions(y_train, y_train_pred, ax=ax[1],sample_weight=sample_weight, normalize='pred', values_format='.0%')\nax[1].set_title(\"CM normalized by row\")\n\nplt.show()\n\n\n\n\nIn row #5 and column #8 on the left plot, it’s means 55% of errors made on true 5s is misclassified as 8s.\nIn row #5 and column #8 on the right plot, it’s means 19% of misclassified 8s are actually 5s.\nAnalyzing the made errors can help us gain insights and why the classifier failing"
  },
  {
    "objectID": "hands-on-classification.html#multilabel-classification",
    "href": "hands-on-classification.html#multilabel-classification",
    "title": "7  Hands-on Classification",
    "section": "7.5 Multilabel Classification",
    "text": "7.5 Multilabel Classification\nOutput is multilabel for each instances. For example, we will classify whether the digit is large (&gt;7) and is odd\n\n7.5.1 K Nearest Neighbors\n\n## Train model\n\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsClassifier\n\ny_train_large = (y_train &gt;= '7')\ny_train_odd = (y_train.astype('int8') % 2 == 1)\ny_train_multilabel = np.c_[y_train_large, y_train_odd]\n\nknn = KNeighborsClassifier()\nknn.fit(X_train_scaled, y_train_multilabel)\nknn.predict([some_digit])\n\narray([[False,  True]])\n\n\nCompute average F1 score across all labels (equally important)\n\n## Evaluate model\n\ny_train_pred_knn = cross_val_predict(knn, X_train_scaled, y_train, cv=3)\nf1_score(y_train, y_train_pred_knn, average='macro')\n\n0.9396793112547043\n\n\nAnother approach is to give each label a weight equal to its number of instances\n\nf1_score(y_train, y_train_pred_knn, average='weighted')\n\n0.940171964265114\n\n\n\n\n7.5.2 SVC\n\nSVC does not natively support multilabel classification. Therefore, there are 2 strategies:\n\n\nTrain one model per label. It turns out that it’s hard to capture the dependencies between labels\nTrain models sequentially (ChainClassifier): using input features and all predictions of previous models in the chain\n\n\nfrom sklearn.multioutput import ClassifierChain\n\nchain_clf = ClassifierChain(SVC(), cv=3, random_state=42)\nchain_clf.fit(X_train_scaled[:2000], y_train_multilabel[:2000])\nchain_clf.predict([some_digit])\n\narray([[0., 1.]])"
  },
  {
    "objectID": "hands-on-classification.html#multioutput-classification",
    "href": "hands-on-classification.html#multioutput-classification",
    "title": "7  Hands-on Classification",
    "section": "7.6 Multioutput Classification",
    "text": "7.6 Multioutput Classification\n\nMulticlass-multilabel classification\nFor example, we will build a model that removes noise from an digit image\nOutput is a clean image 28x28: multilabel (one label per pixel) and multiclass (pixel intensity range from 0-255 per label)\n\n\n## Create a noisy train set\n\nnp.random.seed(42)\n\nnoise = np.random.randint(0,100,(len(X_train), 28*28))\nX_train_noise = X_train + noise\ny_train_noise = X_train\n\nnoise = np.random.randint(0,100,(len(X_test), 28*28))\nX_test_noise = X_test + noise\ny_test_noise = X_test\n\nLet’s look at sample images\n\nplt.subplot(1,2,1)\nplot_digit(X_train_noise[0])\nplt.subplot(1,2,2)\nplot_digit(y_train_noise[0])\n\nplt.show()\n\n\n\n\n\nknn.fit(X_train_noise, y_train_noise)\ny_pred_noise = knn.predict([X_train_noise[0]])\nplot_digit(y_pred_noise)"
  },
  {
    "objectID": "svm.html#linear-svm-classification-support-vector-classification---svc",
    "href": "svm.html#linear-svm-classification-support-vector-classification---svc",
    "title": "8  Support Vector Machine",
    "section": "8.1 Linear SVM Classification (Support Vector Classification - SVC)",
    "text": "8.1 Linear SVM Classification (Support Vector Classification - SVC)\nSupport Vector Machine (Large Margin Classification): Fitting the widest street between classes, supported by support vector instances on the street.\n\n\n\nFigure 8.1: Large margin classification\n\n\nSVMs are sensitive to feature scaling. As we can see, SVM seperate the data better with scaled data.\n\n\n\nFigure 8.2: SVMs are sensitive to feature scaling\n\n\nHard margin/Soft margin classification\n- Hard margin: All instances must be off the street, only work with linearly seperable data and sensitive to outliers\n- Soft margin: improve weakness of hard margin by allow limiting margin violations\n\n\n\n\n\n\nHyperparameter C: the penalty on any misclassified data point.\n- High: high penalty, stricter classification, narrower street and tends to overfit\n- Low: low penalty, allow larger number of misclassifications, wider street and tends to underfit\n\n\n\n\n\n\nFigure 8.3: Different C parameters\n\n\nImplement SVC\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\n\ndata = load_iris(as_frame=True)\nX = data.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\ny = (data.target == 2) # Iris virginica\nsome_flower = X[2,:]\nsvc = make_pipeline(StandardScaler(),\nLinearSVC(random_state=29))\nsvc.fit(X, y)\nprint(svc.predict([some_flower]))\nprint(svc.decision_function([some_flower]))\n\n[False]\n[-6.34263777]\n\n\n/usr/local/anaconda3/envs/dhuy/lib/python3.11/site-packages/sklearn/svm/_classes.py:32: FutureWarning:\n\nThe default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning."
  },
  {
    "objectID": "svm.html#sec-nonlinear-svm",
    "href": "svm.html#sec-nonlinear-svm",
    "title": "8  Support Vector Machine",
    "section": "8.2 Non-linear SVM Classification",
    "text": "8.2 Non-linear SVM Classification\nWith non-linearly seperable datasets in low dimensions, we want to transform them to a higher dimension where they will be linearly sepparable. Imagine “raising” the green points, then you can sepparate them from the red points with a plane (hyperplane).\n\n\n\nFigure 8.4: Non-linearly seperable\n\n\nTo do that, we can use more complex models (Random forest, etc.) or add more features (Polynomial features, similarity features using Gaussian RBF, etc.), but this will lead to a huge bunch of new features and computationally expensive.\nTherefore, SVM supply a powerful technique called kernel trick, allow us to get the same result as if add many polynomial/similarity features, without actually having to add them.\nPolynomial kernel\n\nfrom sklearn.svm import SVC\n\npoly_svc = make_pipeline(StandardScaler(),\nSVC(kernel='poly', degree=3, C=10, coef0=1))\n\nGaussian RBF kernel\n\nrbf_svc = make_pipeline(StandardScaler(),\nSVC(kernel='rbf', C=10, gamma=5))\n\n\n\n\n\n\n\n\ncoef0 (poly kernel): controls how much the model is influenced by high-degree terms versus low-degree terms.\n\ngamma (RBF kernel): high =&gt; overfitting, low =&gt; underfitting\n\ngamma: controls the shape of the “peaks” where you raise the points\n\nHigh: pointed bump (narrow bell-shaped curve), each instance’s range of influence is smaller, tend to wiggling around individual instances\nLow: softer, broader bump (wide bell-shaped curve), vice versa.\n\n\n\n\n\n\n\n\nFigure 8.5: Different C and gamma parameters"
  },
  {
    "objectID": "svm.html#svms-classes-computational-complexity",
    "href": "svm.html#svms-classes-computational-complexity",
    "title": "8  Support Vector Machine",
    "section": "8.3 SVMs Classes Computational Complexity",
    "text": "8.3 SVMs Classes Computational Complexity\n\n\n\nFigure 8.6: BigO of SVM classification"
  },
  {
    "objectID": "svm.html#svm-regression-support-vector-regression---svr",
    "href": "svm.html#svm-regression-support-vector-regression---svr",
    "title": "8  Support Vector Machine",
    "section": "8.4 SVM Regression (Support Vector Regression - SVR)",
    "text": "8.4 SVM Regression (Support Vector Regression - SVR)\nOpposed to SVC, SVR tries to fit as many instances as possible on the street while limiting margin violations (instances off the street)\nHyperparameter epsilon: control the width of the street - Low: narrow street, more support vector, tend to too complex - High: wide street, less support vector, tend to too simple\n\nimport numpy as np\nfrom sklearn.svm import SVR\n\nnp.random.seed(29)\nm = 100\nX = 6 * np.random.rand(m, 1) - 3\ny = 0.5 * X ** 2 + X + 2 + np.random.randn(m, 1)\n\nsvr = make_pipeline(StandardScaler(),\nSVR(kernel='poly', degree=5, C=0.001, epsilon=0.1))\nsvr.fit(X,y.ravel())\nsvr.predict([[3]])\n\narray([3.31601563])\n\n\n\n\n\nFigure 8.7: Different epsilons"
  },
  {
    "objectID": "svm.html#understand-the-fundamentals-of-svm",
    "href": "svm.html#understand-the-fundamentals-of-svm",
    "title": "8  Support Vector Machine",
    "section": "8.5 Understand the Fundamentals of SVM",
    "text": "8.5 Understand the Fundamentals of SVM\nTo predict the class of an instance, SVM compute decision function, then compare to the margin of the street to predict.\n\\[y = θ_{0} + θ^{Τ}X\\]\nSuppose that the margin is (-1,1). With the same margin, to make the wider street, we have to make the θ smaller.\n\n\n\nFigure 8.8: A smaller weights results in a larger margin\n\n\n\n8.5.1 Quadratic Programming Problem (QP solver)\nHard margin classification\nTo avoid the margin violations, we have to minimize the θ while making the decision function ≥1 for positive instances and ≤-1 for negative instances. This constraint can be written using t = 1 or t = -1 repectively:\n\\[\n\\begin{gather}\nminimize(θ,θ_{0})\\;\\;\\frac{1}{2}θ^{Τ}θ\\\\\nsubject\\;to\\;\\;t(θ_{0} + θ^{Τ}X) ≥ 1;\\;\\;t_{i} = [-1;1]\n\\end{gather}\n\\]\nSoft margin classification\nTo perform soft margin classification, we add a slack variable ζ(i) ≥ 0 for each instance: ζ measure how much the instance is allowed to violate the margin.\nExpectedly, we want to keep ζ as small as possible to reduce margin violations, but we also want the margin as wide as possible (too greedy 😆). Don’t worry, this is where the C parameter comes into play.\n\\[\n\\begin{gather}\nminimize(θ,θ_{0})\\;\\;\\frac{1}{2}θ^{Τ}θ + Cζ\\\\\nsubject\\;to\\;\\;t(θ_{0} + θ^{Τ}X) ≥ 1 - ζ;\\;\\;t_{i} = [-1;1];\\;ζ_{i}≥0\n\\end{gather}\n\\]\n\n\n8.5.2 Gradient Descent\nCost function: hinge loss or the squared hinge loss (loss hyperparameter)\nDecision function:\n- ≥ 1: true label is positive =&gt; loss = 0\n- ≤-1: true label is negative =&gt; loss = 0\nBy default: LinearSVC use squared hinge loss, while SGDClassifier use hinge loss\n\n\n\nFigure 8.9: The hinge loss and squared hinge loss\n\n\n\n\n8.5.3 Kernelized SVMs\nAs mentioned in Section 8.2, when we want to perform on more complex model like polynomial or RBF, kernel trick can compute the dot product in the minimization work directly on the original vectors a and b, without even know about the transformation. The Figure 8.10 illustrate the kernel trick for a second-degree polynomial\n\n\n\nFigure 8.10: Kernel trick for a second-degree polynomial\n\n\nThese are the common kernels, in which K is the kernel function:\n\n\n\nFigure 8.11: Common kernels\n\n\n\nd: degree\nr: coef0\nγ: gamma, ≥ 0\n\n\n\n\n\n\n\nLearn more about Dual problem, equation to make predictions with kernel trick"
  },
  {
    "objectID": "dimensionality-reduction.html#the-curse-of-dimensionality",
    "href": "dimensionality-reduction.html#the-curse-of-dimensionality",
    "title": "10  Dimensionality Reduction",
    "section": "10.1 The Curse of Dimensionality",
    "text": "10.1 The Curse of Dimensionality\nBecause of the huge number of spaces in high dimensions, high-dimensional datasets are at risk of being very sparse: most training instances are likely to be far away from each other. Therefore, a new instance will likely be far away from any training instance, making predictions much less reliable than in lower dimensions, since they will be based on much larger extrapolations. In short, the more dimensions the training set has, the greater the risk of overfitting it."
  },
  {
    "objectID": "dimensionality-reduction.html#projection",
    "href": "dimensionality-reduction.html#projection",
    "title": "10  Dimensionality Reduction",
    "section": "10.2 Projection",
    "text": "10.2 Projection\nSuppose that we want to turn a 3D dataset into 2D (called subspace), we will project all instances perpendicularly onto this subspace.\n\n\n\nFigure 10.1: Turn a 3D dataset into 2D\n\n\n\n10.2.1 PCA\nPCA first identifies the righ hyperplane that lies closest to the data, and then it projects the data onto it. This hyperplane preserves maximum variance and minimizes the mean squared distance between the original dataset and its projection.\n\n\n\nFigure 10.2: Select the right hyperplane (c1)\n\n\nHow does PCA do that?\n\nCenter the data (minus mean).\nFind an axis (principle component) accounts for the largest amount of variance.\nEach next priciple components orthogonal to the previous one accounting for the largest amount of the remaining variance.\n\nTo find the principle components, PCA use SVD technique (singular value decomposition) that decompose data X into UΣV⊺ where V⊺ contains the unit vectors that define all the principal components.\n\\[X = UΣV^T\\]\nThen, we multiply matrix X with V⊺ to get the lower-dimension data.\n\\[X_d = XV^T\\]\n\n\n\n\n\n\nHyperparameter: n_components, svd_solver\nAttribute of PCA: components_, explained_variance_ratio_, n_components_\n\n\n\nChoose the right number of dimensions by setting the n_components hyperparameter.\n\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.decomposition import PCA\n\n# mnist = fetch_openml('mnist_784', as_frame=False)\n# X_train, y_train = mnist.data[:600], mnist.target[:600] \n# X_test, y_test = mnist.data[600:1200], mnist.target[600:1200]\n\n# pca = PCA(n_components=0.95)    # choose n_components that preserve ~ 95% variance of data\n# X_reduced = pca.fit_transform(X_train)\n# print(f'Number of components: {pca.n_components_}')\n\nTuning the n_components hyperparameter to compress the data.\n\nimport numpy as np\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\npipe = Pipeline([\n    ('pca', PCA(random_state=29)),\n    ('classifier', RandomForestClassifier(random_state=29))\n])\nparams = {\n    'pca__n_components': np.arange(10,15),\n    'classifier__n_estimators': [50,60,70]\n}\ngrid = RandomizedSearchCV(estimator=pipe, param_distributions=params, cv=5, scoring='accuracy', random_state=29)\n# grid.fit(X_train, y_train)\n# print(f'Best params: {grid.best_params_}')\n\nDecompress the transformed data.\n\\[X_{recovered} = X_dV^T\\]\n\n# X_recovered = pca.inverse_transform(X_reduced)\n\nRandomized PCA: svd_solver=‘random’; quickly find an approximation of d principle components, auto if max(m,n)&gt;500 and n_components &lt; 80% of min(m,n).\n\n# rand_pca = PCA(n_components=154, svd_solver='randomized', random_state=29)\n# X_reduced = rand_pca.fit_transform(X_train)\n\nIncremental PCA: use np.array_split, partial_fit; fit mini-batch of data\n\nfrom sklearn.decomposition import IncrementalPCA\n\nn_batches = 100\ninc_pca = IncrementalPCA(n_components=154)\n# for batch in np.array_split(X_train, n_batches):\n#     inc_pca.fit(batch)\n# X_reduced = inc_pca.transform(X_train)\n\n\n\n10.2.2 Random Projection\nFor very high-dimension dataset, PCA can be too slow. Therefore, random projection is solution. It works by implementing random linear projection.\n\nChoose the optimal number of dimensions by sklearn.random_projection.johnson_lindenstrauss_min_dim: compute minimum number of dimensions to ensure the squared distance between any two instances to change by more than a tolerance.\n\n\\[d ≥ 4log(m)\\frac{1}{(\\frac{1}{2}ε2 - \\frac{1}{3}ε3)}\\]\nd: target dimension\nm: number of instances\nε: tolerance\n2. Generate a random matrix P of shape [d,n] (n: number of features), from a Gaussian distribution with mean 0 and variance 1/d.\n3. Reduced matrix = X @ P.T\n\nfrom sklearn.random_projection import GaussianRandomProjection\n\ngauss_rand_prj = GaussianRandomProjection(eps=0.1, random_state=29)\n# X_reduced = gauss_rand_prj.fit_transform(X_train)\n\n\n\n10.2.3 Sparse Random Projection\nWork as same as random projection, except the random matrix is sparse, so that it use much less memory and train much faster and is preferred."
  },
  {
    "objectID": "dimensionality-reduction.html#manifold-learning",
    "href": "dimensionality-reduction.html#manifold-learning",
    "title": "10  Dimensionality Reduction",
    "section": "10.3 Manifold Learning",
    "text": "10.3 Manifold Learning\nIn many cases the subspace may twist and turn, projection is not useful Figure 10.3. Simply projecting onto a plane (e.g., by dropping x3) would squash different layers together (Figure 10.4, left). Instead, we will unroll to obtain the 2D dataset (Figure 10.4, right).\n\n\n\nFigure 10.3: Roll dataset\n\n\n\n\n\nFigure 10.4: Projecting and unrolling the roll\n\n\nThe roll is an example of 2D manifold. Generally, a d-dimensional manifold is a part of an n-dimensional space (where d &lt; n) that locally resembles a d-dimensional hyperplane.\nHowever, manifold learning may not always lead to a better or simpler solution, it all depends on dataset.\n\n\n\nFigure 10.5: Good use case (upper) and not good use case (lower) of manifold learning\n\n\n\n10.3.1 LLE (Locally Linear Embedding)\nLLE is used for nonlinear task, do not scale well.\n\nMeasuring how each training instance linearly relates to its nearest neighbors (k-nearest neighbors).\nFind low-dimensional data where these local relationships are best preserved.\n\n\nfrom sklearn.datasets import make_swiss_roll\nfrom sklearn.manifold import LocallyLinearEmbedding\n\nX_swiss, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\nlle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)\n# X_unrolled = lle.fit_transform(X_swiss)"
  },
  {
    "objectID": "project-checklist.html#frame-problem",
    "href": "project-checklist.html#frame-problem",
    "title": "2  Machine Learning Project Workflow",
    "section": "2.1 Frame problem",
    "text": "2.1 Frame problem\n\nObjective and current solutions?\nNew solutions: how to use\nDepend on type of problems: possible models, performance measuring\nMinimum needed performance\nComparable problems -&gt; Can reuse experiment, tools?\nList and verify assumptions (if available)"
  },
  {
    "objectID": "project-checklist.html#get-data",
    "href": "project-checklist.html#get-data",
    "title": "2  Machine Learning Project Workflow",
    "section": "2.2 Get data",
    "text": "2.2 Get data\n\nList data: where to get, how much (features, instances), storage space\nGet and convert data if necessary\nAnonymize sensitive information\nRecheck data"
  },
  {
    "objectID": "project-checklist.html#gain-insights",
    "href": "project-checklist.html#gain-insights",
    "title": "2  Machine Learning Project Workflow",
    "section": "2.3 Gain Insights",
    "text": "2.3 Gain Insights\n\nCreate a copy of the data for exploration (sampling it down to a manageable size if necessary).\nCreate a notebook to keep a record of data exploration.\nStudy each attribute and its characteristics:\n\nName\nType (categorical, int/float, bounded/unbounded, text, structured, etc.)\n% of missing values\nNoisiness and type of noise (stochastic, outliers, rounding errors, etc.)\nUsefulness for the task\nType of distribution (Gaussian, uniform, logarithmic, etc.)\n\nFor supervised learning tasks, identify the target attribute(s).\nVisualize the data.\nStudy the correlations/mututal information\nIdentify the promising transformations/feature engineering\nIdentify extra data that would be useful.\nDocument what we have learned."
  },
  {
    "objectID": "project-checklist.html#prepare-data",
    "href": "project-checklist.html#prepare-data",
    "title": "2  Machine Learning Project Workflow",
    "section": "2.4 Prepare data",
    "text": "2.4 Prepare data\n\n\n\n\n\n\n\nWork on copies of the data (keep the original dataset intact).\nWrite functions for all data transformations we apply, for 3 reasons:\n— Easily prepare the data the next time we get a fresh dataset\n— Easily to apply transformations for test set/new instances once solution is live\n— Treat preparation choices as hyperparameters\n\n\n\n\n\nClean the data\n\nFix or remove outliers (optional).\nFill in missing values (e.g., with zero, mean, median…) or drop their rows (or columns).\n\nPerform feature selection (optional)\n\nDrop the attributes that provide no useful information for the task.\n\nPerform feature engineering, where appropriate\n\nDiscretize continuous features.\nDecompose features (e.g., categorical, date/time, etc.).\nAdd promising transformations of features (e.g., log(x), sqrt(x), x2, etc.).\nAggregate features into promising new features.\n\nPerform feature scaling\n\nStandardize or normalize features."
  },
  {
    "objectID": "project-checklist.html#choose-models",
    "href": "project-checklist.html#choose-models",
    "title": "2  Machine Learning Project Workflow",
    "section": "2.5 Choose models",
    "text": "2.5 Choose models\n\n\n\n\n\n\nIf the data is very large, it might be better to sample smaller training set to train many different models in a reasonable time, but this will affect performance of complex models such as Random Forest, Neural Networks, etc.\n\n\n\n\nTrain many quick-and-dirty models from different categories (e.g., Linear, Naive Bayes, SVM, Random Forest, Neural Networks, etc.) using standard parameters.\nMeasure and compare their performance: For each model, use N-fold cross-validation and compute the mean and standard deviation of the performance measures.\nAnalyze the most significant variables for each algorithm.\nAnalyze the types of errors the models make: What data would a human have used to avoid these errors?\nPerform a quick round of feature selection and engineering.\nPerform one or two more quick iterations of the five previous steps.\nShortlist the top three to five most promising models, preferring models that make different types of errors."
  },
  {
    "objectID": "project-checklist.html#fine-tune-and-combine-models",
    "href": "project-checklist.html#fine-tune-and-combine-models",
    "title": "2  Machine Learning Project Workflow",
    "section": "2.6 Fine-tune and combine models",
    "text": "2.6 Fine-tune and combine models\n\n\n\n\n\n\n\nImplementing on full training set.\nDon’t tweak the model after measuring the generalization error: it would just start overfitting the test set.\n\n\n\n\n\nFine-tune the hyperparameters using cross-validation:\n\nTreat data transformation choices as hyperparameters, especially when we are not sure about them (e.g., if we are not sure whether to replace missing values with zeros or with the median value, or to just drop the rows).\nUnless there are very few hyperparameter values to explore, prefer random search over grid search. If the training is very long, prefer a Bayesian optimization approach (e.g., using Gaussian process priors)\n\nTry ensemble methods. Combining our best models will often produce better performance than running them individually.\nOnce we are confident about final model, measure performance on the test set to estimate the generalization error."
  },
  {
    "objectID": "project-checklist.html#present-solutions",
    "href": "project-checklist.html#present-solutions",
    "title": "2  Machine Learning Project Workflow",
    "section": "2.7 Present solutions",
    "text": "2.7 Present solutions\n\nDocument what we have done.\nCreate a nice presentation: Make sure to highlight the big picture first.\nExplain why solution achieves the business objective.\nDon’t forget to present interesting points noticed along the way:\n\nDescribe what worked and what did not.\nList our assumptions and system’s limitations.\n\nEnsure the key findings are communicated through beautiful visualizations or easy-to-remember statements (e.g., “the median income is the number-one predictor of housing prices”)."
  },
  {
    "objectID": "project-checklist.html#launch-monitor-and-maintain-system",
    "href": "project-checklist.html#launch-monitor-and-maintain-system",
    "title": "2  Machine Learning Project Workflow",
    "section": "2.8 Launch, monitor and maintain system",
    "text": "2.8 Launch, monitor and maintain system\n\nGet the solution ready for production (plug into production data inputs, write unit tests, etc.).\nWrite monitoring code to check the system’s live performance at regular intervals and trigger alerts when it drops:\n\nBeware of slow degradation: models tend to “rot” as data evolves.\nMeasuring performance may require a human pipeline (e.g., via a crowdsourcing service).\nAlso monitor the inputs’ quality (e.g., a malfunctioning sensor sending random values, or another team’s output becoming stale). This is particularly important for online learning systems.\n\nRetrain the models on a regular basis on fresh data (automate as much as possible)."
  },
  {
    "objectID": "gradient-descent.html#batch-gradient-descent",
    "href": "gradient-descent.html#batch-gradient-descent",
    "title": "4  Gradient Descent",
    "section": "4.1 Batch Gradient Descent",
    "text": "4.1 Batch Gradient Descent\n\n[i] Full Gradient Descent =&gt; terribly slow\n[i] Gradient: \\[∇ = \\frac{1}{m} X^{T}(Χθ-y)\\]\nScale well with number of features"
  },
  {
    "objectID": "gradient-descent.html#stochastic-gradient-descent",
    "href": "gradient-descent.html#stochastic-gradient-descent",
    "title": "4  Gradient Descent",
    "section": "4.2 Stochastic Gradient Descent",
    "text": "4.2 Stochastic Gradient Descent\n\nPick a random instance at every step (not epoch) to compute gradient\nOut-of-core algorithm\nCost function: cost function is erratic, continue bounch around when get to the global minimum\n\nCan jump out local minimum\nWeights are good, not optimal =&gt; Improve by set gradually reduce learning_rate (called learning schedule)\nRandomness =&gt; Improve by shuffling to ensure pick every instance"
  },
  {
    "objectID": "gradient-descent.html#mini-batch-gradient-descent",
    "href": "gradient-descent.html#mini-batch-gradient-descent",
    "title": "4  Gradient Descent",
    "section": "4.3 Mini-Batch Gradient Descent",
    "text": "4.3 Mini-Batch Gradient Descent\n\nCompute gradient on small random sets called mini-batches (boost by GPUs)\nLess erratic"
  },
  {
    "objectID": "classification.html#binary-classification",
    "href": "classification.html#binary-classification",
    "title": "6  Classification",
    "section": "6.1 Binary Classification",
    "text": "6.1 Binary Classification\n\n6.1.1 Logistic Regression\nPretty much same as method using in Linear regression, Logistic regression use sigmoid function to the same equation using in Linear regression to turn the output into probabilities (range (0,1)).\n\\[\n\\begin{gather}\nLinear Regression: y = θ^{T}X\\\\\nLogistic Regression: p = sigmoid(θ^{T}X)\\\\\nsigmoid(t) = \\frac{1}{1-e^{-t}}\\\\\nlogit(p) = log\\left(\\frac{p}{1-p}\\right)= t\n\\end{gather}\n\\]\nCost function for 1 instance \\[\\begin{equation}\n\\begin{split}\nJ(θ) & = -log(p)\\quad \\quad \\quad if\\;\\;\\; y=1\\\\\n& = -log(1-p) \\quad \\; if\\;\\;\\;  y=0\n\\end{split}\n\\end{equation}\n\\]\n\n\n\n\n\n\nCost function penalizes the model when it estimates the loew probability for the real target class\n\n-log(p) -&gt; inf when p -&gt; 0 for y = 1 instance\n\n-log(1-p) -&gt; inf when p -&gt; 1 for y = 0 instance\n\n\n\n\nThere is no closed-form equation to compute θ. We will use gradient descent to find the best weights.\nCost function for whole training set (log loss): convex function \\[J(θ) = \\frac{−1}{m} \\sum [y_ilog(p_i) + (1−y_i)log(1−p_i)]\\]\nGradient \\[∇ = \\frac{1}{m}X^{T}[sigmoid(Xθ) - y]\\]\n\n\n\n\n\n\n\nLog loss assumption: the instances follow a Gaussian distribution around the mean of their class\n\nMSE assumption: data is purely linear\n\nThe more wrong assumption, the more biased the model\n\n\n\n\nDecision boudaries:\n\n\n\n\n\n\nRegularization in Logistic Regression: l1, l2 using C parameter (inverse of alpha)\n\n\n\nImplement Linear regression using sklearn: Logistic regression\n\n\n6.1.2 Softmax Regression (Multinomial Logistic Regression)\nThe Logistic regression can be generalized to support multipleclass classification directly. It is called softmax regression.\nThe strategy when given an instance x is described like this:\n\nCompute score for each class using softmax score function\nCompute probability for each class using softmax function to each score\nChoose the class with the highest probability. The instance x is belong to this class\n\nSoftmax score for class k \\[s_k(x) = (θ^{(k)})^{T}X\\]\n\n\n\n\n\n\nEach class has own parameter vecto θ(k). Parameter matrix Θ contains all parameter vectors of all classes\n\n\n\nSoftmax function for class k: \\[p_k = σ(s(x))_k = \\frac{exp(s_k(x))}{\\sum\\limits exp(s_j(x))}\\]\n\nK is the number of classes\n\ns(x) is a vector containing the scores of each class for the instance x\n\nσ(s(x))k is the estimated probability that the instance x belongs to class k, given the scores of each class for that instance\n\n\nChoose the class with the highest probability \\[y= argmax\\; σ(s(x))_k= argmax\\;s_k(x) = argmax\\; (θ^{k})^{T}X\\]\nJust like Logistic regression, softmax regression has the cost function called Cross entropy\nCross entropy cost function\n\\[\nJ(Θ) = −\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}y_{k}^{(i)}log(p_{k}^{(i)})\n\\]\n\n\n\n\n\n\n\nyk(i): the label of the target class\n\nWhen k=2, softmax regression is equivalent to logistic regression\n\n\n\n\nCross entropy gradient vector for class k\n\\[\n∇_{θ}k = \\frac{1}{m}\\sum(p_{k}^{(i)} − y_{k}^{i})x^{(i)}\n\\]\nImplement Linear regression using sklearn\n\nfrom sklearn.datasets import load_iris\n\niris = load_iris(as_frame=True)\niris.target_names\n\narray(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10')\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nX = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\ny = iris[\"target\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nsoftmax = LogisticRegression(max_iter=1000, C=30)\nsoftmax.fit(X_train, y_train)\nprint(softmax.predict([X_test[0]]))\nprint(softmax.predict_proba([X_test[0]]).round(4))\n\n[1]\n[[0.     0.9827 0.0173]]"
  },
  {
    "objectID": "classification.html#multiclass-classification",
    "href": "classification.html#multiclass-classification",
    "title": "6  Classification",
    "section": "6.2 Multiclass Classification",
    "text": "6.2 Multiclass Classification"
  },
  {
    "objectID": "classification.html#multilabel-classification",
    "href": "classification.html#multilabel-classification",
    "title": "6  Classification",
    "section": "6.3 Multilabel Classification",
    "text": "6.3 Multilabel Classification"
  },
  {
    "objectID": "classification.html#multioutput-classification",
    "href": "classification.html#multioutput-classification",
    "title": "6  Classification",
    "section": "6.4 Multioutput Classification",
    "text": "6.4 Multioutput Classification"
  }
]