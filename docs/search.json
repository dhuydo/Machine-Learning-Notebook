[
  {
    "objectID": "classification.html",
    "href": "classification.html",
    "title": "Classification",
    "section": "",
    "text": "Classification is a supervised learning problem whose output are categorical, different from Linear regression of which are numerical.\nThe algorithms used to handle classification problems are divided into the following groups: Binary classification, multiclass classification, multilabel classification, multioutput classification, etc."
  },
  {
    "objectID": "classification.html#binary-classification",
    "href": "classification.html#binary-classification",
    "title": "Classification",
    "section": "Binary Classification",
    "text": "Binary Classification\n\nLogistic Regression\nPretty much same as method using in Linear regression, Logistic regression use sigmoid function to the same equation using in Linear regression to turn the output into probabilities (range (0,1)).\n\\[\n\\begin{gather}\nLinear Regression: y = θ^{T}X\\\\\nLogistic Regression: p = sigmoid(θ^{T}X)\\\\\nsigmoid(t) = \\frac{1}{1-e^{-t}}\\\\\nlogit(p) = log\\left(\\frac{p}{1-p}\\right)= t\n\\end{gather}\n\\]\nCost function for 1 instance \\[\\begin{equation}\n\\begin{split}\nJ(θ) & = -log(p)\\quad \\quad \\quad if\\;\\;\\; y=1\\\\\n& = -log(1-p) \\quad \\; if\\;\\;\\;  y=0\n\\end{split}\n\\end{equation}\n\\]\n\n\n\n\n\n\nCost function penalizes the model when it estimates the loew probability for the real target class\n\n-log(p) -&gt; inf when p -&gt; 0 for y = 1 instance\n\n-log(1-p) -&gt; inf when p -&gt; 1 for y = 0 instance\n\n\n\n\nThere is no closed-form equation to compute θ. We will use gradient descent to find the best weights.\nCost function for whole training set (log loss): convex function \\[J(θ) = \\frac{−1}{m} \\sum [y_ilog(p_i) + (1−y_i)log(1−p_i)]\\]\nGradient \\[∇ = \\frac{1}{m}X^{T}[sigmoid(Xθ) - y]\\]\n\n\n\n\n\n\n\nLog loss assumption: the instances follow a Gaussian distribution around the mean of their class\n\nMSE assumption: data is purely linear\n\nThe more wrong assumption, the more biased the model\n\n\n\n\nDecision boudaries:\n\n\n\n\n\n\nRegularization in Logistic Regression: l1, l2 using C parameter (inverse of alpha)\n\n\n\nImplement Linear regression using sklearn: Logistic regression\n\n\nSoftmax Regression (Multinomial Logistic Regression)\nThe Logistic regression can be generalized to support multipleclass classification directly. It is called softmax regression.\nThe strategy when given an instance x is described like this:\n\nCompute score for each class using softmax score function\nCompute probability for each class using softmax function to each score\nChoose the class with the highest probability. The instance x is belong to this class\n\nSoftmax score for class k \\[s_k(x) = (θ^{(k)})^{T}X\\]\n\n\n\n\n\n\nEach class has own parameter vecto θ(k). Parameter matrix Θ contains all parameter vectors of all classes\n\n\n\nSoftmax function for class k: \\[p_k = σ(s(x))_k = \\frac{exp(s_k(x))}{\\sum\\limits exp(s_j(x))}\\]\n\nK is the number of classes\n\ns(x) is a vector containing the scores of each class for the instance x\n\nσ(s(x))k is the estimated probability that the instance x belongs to class k, given the scores of each class for that instance\n\n\nChoose the class with the highest probability \\[y= argmax\\; σ(s(x))_k= argmax\\;s_k(x) = argmax\\; (θ^{k})^{T}X\\]\nJust like Logistic regression, softmax regression has the cost function called Cross entropy\nCross entropy cost function\n\\[\nJ(Θ) = −\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}y_{k}^{(i)}log(p_{k}^{(i)})\n\\]\n\n\n\n\n\n\n\nyk(i): the label of the target class\n\nWhen k=2, softmax regression is equivalent to logistic regression\n\n\n\n\nCross entropy gradient vector for class k\n\\[\n∇_{θ}k = \\frac{1}{m}\\sum(p_{k}^{(i)} − y_{k}^{i})x^{(i)}\n\\]\nImplement Linear regression using sklearn\n\nfrom sklearn.datasets import load_iris\n\niris = load_iris(as_frame=True)\niris.target_names\n\narray(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10')\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nX = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\ny = iris[\"target\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nsoftmax = LogisticRegression(max_iter=1000, C=30)\nsoftmax.fit(X_train, y_train)\nprint(softmax.predict([X_test[0]]))\nprint(softmax.predict_proba([X_test[0]]).round(4))\n\n[1]\n[[0.     0.9827 0.0173]]"
  },
  {
    "objectID": "classification.html#multiclass-classification",
    "href": "classification.html#multiclass-classification",
    "title": "Classification",
    "section": "Multiclass Classification",
    "text": "Multiclass Classification"
  },
  {
    "objectID": "classification.html#multilabel-classification",
    "href": "classification.html#multilabel-classification",
    "title": "Classification",
    "section": "Multilabel Classification",
    "text": "Multilabel Classification"
  },
  {
    "objectID": "classification.html#multioutput-classification",
    "href": "classification.html#multioutput-classification",
    "title": "Classification",
    "section": "Multioutput Classification",
    "text": "Multioutput Classification"
  },
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "Regression",
    "section": "",
    "text": "Suppose that we have a data set of demographic and healthcare cost for each individual in a city, and we want to predict the total healthcare cost based on age.\nIf we use linear regression method for this task, we will assump that the relationship between these features is linear and try to fit a line so that is closest to the data. The plot looks like this.\n\n\nCode\n## Simple linear regression plot\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(29)\nm = 100      # number of instances\nx = np.random.randint(18,80,m)\ny = np.random.randint(-200,200,m) + 20*x\n\nplt.plot(x,y,'b.', label='True values')\nplt.plot(x, 20*x,'-',color='r', label='Linear regression')\nplt.xlabel('Age')\nplt.ylabel('Healthcare cost')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nIf you have another feature using to predict (e.g. weight), the plot will look like this. For ≥3 features, it’s called ‘Multiple linear regression’ and we will fit a hyperplane instead.\n\n\nCode\n## Multiple linear regression plot\n\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\nfrom mpl_toolkits.mplot3d import Axes3D\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nz = np.random.randint(20,30,m)\ny = np.random.randint(-200,200,m) + 20*x +30*z\n\nX_train = np.c_[x,z]\nlm = LinearRegression()\nlm.fit(X_train, y)\n\n\n# Setting up the 3D plot\nfig = plt.figure(figsize=(10, 7))\nax = fig.add_subplot(111, projection='3d')\n\n# Scatter plot of actual data\nax.scatter(x, z, y, color='blue', marker='o', alpha=0.5, label='True values')\n\n# Creating a meshgrid for the plane\nx_surf = np.linspace(x.min(), x.max(), 100)\nz_surf = np.linspace(z.min(), z.max(), 100)\nx_surf, z_surf = np.meshgrid(x_surf, z_surf)\n\n# Predicting the values from the meshed grid\nvals = pd.DataFrame({'Age': x_surf.ravel(), 'Weight': z_surf.ravel()})\ny_pred = lm.predict(vals)\nax.plot_surface(x_surf, z_surf, y_pred.reshape(x_surf.shape), color='r', alpha=0.3, label='Hyperplane')\n\n# Labeling the axes\nax.set_xlabel('Age')\nax.set_ylabel('Weight')\nax.set_zlabel('Healthcare cost')\n#ax.legend()\n\nplt.show()\n\n\n\n\n\nThe formula of the line (n=1)/hyperplane (n&gt;1) is: \\[\n\\hat{y} = θ_o +θ_1x_1 +θ_2x_2+...+θ_nx_n\n\\]\n\nŷ: predicted value\nn: number of features\nx_i: the i_th feature value\nθ_i: the i_th parameter value (θ_0: intercept; θ_1 - θ_n: weight of parameters)\n\nFor linear algebra, this can be written much more concisely using a vectorized form like this: \\[\\hat{y} = θ.X\\]\n\nθ: vecto of weights (of parameters)\nX: matrix of features\n\nSo how can we find the best fitted line, the left or the right one?\n\n\nCode\nnp.random.seed(29)\nm = 100      # number of instances\nx = np.random.randint(18,80,m)\ny = np.random.randint(-200,200,m) + 20*x\n\nplt.subplot(1,2,1)\nplt.plot(x,y,'b.', label='True values')\nplt.plot(x, 20*x,'-',color='r')\nplt.xlabel('Age')\nplt.ylabel('Healthcare cost')\n\nplt.subplot(1,2,2)\nplt.plot(x,y,'b.', label='True values')\nplt.plot(x, 10+18*(x+10),'-',color='r')\nplt.xlabel('Age')\n\nplt.show()\n\n\n\n\n\nIt turns out that we have 2 common strategy:\n- Linear algebra: using normal equation\n- Optimization: using gradient descent\n\n\n\\[θ = (X^{T}X)^{-1}X^{T}y\\]\n\nθ: vecto of weights (of parameters)\nX: matrix of features\ny: vecto of target value\n\nThat’s all we need to compute the best weights (coefficients).\nBut in reality, not all cases matrix is invertible, so LinearRegression in sklearn compute pseudoinverse (X+) instead, using a standard matrix factorization technique called singular value decomposition (SVD) that decompose X into (UΣV^T): \\[\n\\begin{gather}\nθ = X^{+}Y\\\\\n   X = UΣV^{T}\\\\\nX^{+} = VΣ^{+}U^{T}\n\\end{gather}\n\\]\nImplement Linear regression using sklearn\n\nfrom sklearn.linear_model import LinearRegression\n\nnp.random.seed(29)\nx = np.random.randint(18,80,m)\ny = np.random.randint(-200,200,m) + 20*x\nX_train = x.reshape(-1,1)\n\nlinear = LinearRegression()\nlinear.fit(X_train,y)\ny_pred = linear.predict(X_train)\nprint(f'Equation: {linear.intercept_:.2f} + {linear.coef_[0]:.2f}*x')\n\nplt.plot(x, y, 'b.', label='True values')\nplt.plot(x, y_pred,'-',color='r', label='Linear regression')\nplt.xlabel('Age')\nplt.ylabel('Healthcare cost')\nplt.legend()\n\nplt.show()\n\nEquation: 1.41 + 20.21*x\n\n\n\n\n\nBoth the Normal equation and SVD approach scale well with the number of instances, but scale very badly with number of features. Therefore, we will look at another approach which is better suited for cases where there are a large number of features or too many training instances to fit in memory.\n\n\n\n\n\nIn fact, the computer really like the term ‘optimization’, which means we will take the result roughly equal to the correct one with the acceptable error. Gradient descent (GD) is that kind of method.\nGenerally, GD tweaks the weights iteratively in order to minimize a cost function. Steps to do Gradient Descent:\n\nTake Gradient (derivative) of Loss Function\nRandom initialization (take random weights)\nLoop step 3-5 until converge:\n\nCompute gradient\nCompute step size: StepSize = Gradient * Learning_rate\nCompute new weights: New = Old - StepSize\n\n\n[i] Run single epoch:\n\npartial_fit(): ignore (max_iter, tol) do not reset epoch counter\nfit(warm_start = True)\n\n\n\n\n\n\n\n\n\nLoss function: also called cost function, is the amount that we have to pay if we use the specific set of weights. Of course we want to minimize it cause everyone want to pay less but gain more, right 😆\nLearning rate: the pace of changing the weights in respond to the estimated loss\n\n\nToo small: take a long time to converge\nToo high: diverge\n\nNumber of epochs: times that we update our weights\n\nToo low: can’t get optimal solution\nToo high: waste time (parameters do not change much)\nSolution: set large epoch and a tolerance to interrupt when grandient &lt; tolerance\n\n\n\n\n\n\n\n\n\n\n\n\nSuitable learning rate\n\n\n\n\n\n\n\nToo low learning rate\n\n\n\n\n\n\n\nToo high learning rate\n\n\n\n\nFigure 1: Learning rate strategy\n\n\n\n\n\n\nLocal minimum: If we initialize weights from the left, we will reach local minimum instead of global minimum\nPlateau: if we initialize weights from the right, the gradient will change slowly and adding new instances to the training set doesn’t make the average error much better or worse. If early stopping, we will never reach the global minimum\n\n\n\n\nFigure 2: Gradient descent pitfalls\n\n\nFortunately, the cost function of linear regression is a convex function, which means it has no local minimum/ just one global minimum, and its slope never changes abruptly\n\\[\nMSE = \\frac{1}{2m}\\sum_{i=1}^{m}{(θ^{T}x_{i}-y_{i})}^2\n\\]\n\nAnother pitfall of GD: features have very different scales. Therefore, when using gradient descent, you should ensure that all features have a similar scale (e.g., using Scikit-Learn’s StandardScaler class), or else it will take much longer to converge.\n\n\n\n\nFigure 3: Gradient descent with (left) and without (right) feature scaling\n\n\n\n\n\n\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\nsgd = make_pipeline(StandardScaler(),\nSGDRegressor(max_iter=1000, tol=1e-3))\nsgd.fit(X_train, y)\nprint('Equation: %.2f + %.2f*x' % (sgd['sgdregressor'].intercept_[0], sgd['sgdregressor'].coef_[0]))\n\ny_pred = sgd.predict(X_train)\n\nplt.plot(x, y, 'b.', label='True values')\nplt.plot(x, y_pred,'-',color='r', label='Stochastic gradient descent regressor')\nplt.xlabel('Age')\nplt.ylabel('Healthcare cost')\nplt.legend()\n\nplt.show()\n\nEquation: 961.04 + 372.06*x\n\n\n\n\n\n\n\n\n\n\n\nThe intercept and coefficient in this equation are different from Section 1.1 because they implement on scaled X_train\n\n\n\nLearn more about Gradient descent."
  },
  {
    "objectID": "regression.html#linear-regression",
    "href": "regression.html#linear-regression",
    "title": "Regression",
    "section": "",
    "text": "Suppose that we have a data set of demographic and healthcare cost for each individual in a city, and we want to predict the total healthcare cost based on age.\nIf we use linear regression method for this task, we will assump that the relationship between these features is linear and try to fit a line so that is closest to the data. The plot looks like this.\n\n\nCode\n## Simple linear regression plot\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(29)\nm = 100      # number of instances\nx = np.random.randint(18,80,m)\ny = np.random.randint(-200,200,m) + 20*x\n\nplt.plot(x,y,'b.', label='True values')\nplt.plot(x, 20*x,'-',color='r', label='Linear regression')\nplt.xlabel('Age')\nplt.ylabel('Healthcare cost')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nIf you have another feature using to predict (e.g. weight), the plot will look like this. For ≥3 features, it’s called ‘Multiple linear regression’ and we will fit a hyperplane instead.\n\n\nCode\n## Multiple linear regression plot\n\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\nfrom mpl_toolkits.mplot3d import Axes3D\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nz = np.random.randint(20,30,m)\ny = np.random.randint(-200,200,m) + 20*x +30*z\n\nX_train = np.c_[x,z]\nlm = LinearRegression()\nlm.fit(X_train, y)\n\n\n# Setting up the 3D plot\nfig = plt.figure(figsize=(10, 7))\nax = fig.add_subplot(111, projection='3d')\n\n# Scatter plot of actual data\nax.scatter(x, z, y, color='blue', marker='o', alpha=0.5, label='True values')\n\n# Creating a meshgrid for the plane\nx_surf = np.linspace(x.min(), x.max(), 100)\nz_surf = np.linspace(z.min(), z.max(), 100)\nx_surf, z_surf = np.meshgrid(x_surf, z_surf)\n\n# Predicting the values from the meshed grid\nvals = pd.DataFrame({'Age': x_surf.ravel(), 'Weight': z_surf.ravel()})\ny_pred = lm.predict(vals)\nax.plot_surface(x_surf, z_surf, y_pred.reshape(x_surf.shape), color='r', alpha=0.3, label='Hyperplane')\n\n# Labeling the axes\nax.set_xlabel('Age')\nax.set_ylabel('Weight')\nax.set_zlabel('Healthcare cost')\n#ax.legend()\n\nplt.show()\n\n\n\n\n\nThe formula of the line (n=1)/hyperplane (n&gt;1) is: \\[\n\\hat{y} = θ_o +θ_1x_1 +θ_2x_2+...+θ_nx_n\n\\]\n\nŷ: predicted value\nn: number of features\nx_i: the i_th feature value\nθ_i: the i_th parameter value (θ_0: intercept; θ_1 - θ_n: weight of parameters)\n\nFor linear algebra, this can be written much more concisely using a vectorized form like this: \\[\\hat{y} = θ.X\\]\n\nθ: vecto of weights (of parameters)\nX: matrix of features\n\nSo how can we find the best fitted line, the left or the right one?\n\n\nCode\nnp.random.seed(29)\nm = 100      # number of instances\nx = np.random.randint(18,80,m)\ny = np.random.randint(-200,200,m) + 20*x\n\nplt.subplot(1,2,1)\nplt.plot(x,y,'b.', label='True values')\nplt.plot(x, 20*x,'-',color='r')\nplt.xlabel('Age')\nplt.ylabel('Healthcare cost')\n\nplt.subplot(1,2,2)\nplt.plot(x,y,'b.', label='True values')\nplt.plot(x, 10+18*(x+10),'-',color='r')\nplt.xlabel('Age')\n\nplt.show()\n\n\n\n\n\nIt turns out that we have 2 common strategy:\n- Linear algebra: using normal equation\n- Optimization: using gradient descent\n\n\n\\[θ = (X^{T}X)^{-1}X^{T}y\\]\n\nθ: vecto of weights (of parameters)\nX: matrix of features\ny: vecto of target value\n\nThat’s all we need to compute the best weights (coefficients).\nBut in reality, not all cases matrix is invertible, so LinearRegression in sklearn compute pseudoinverse (X+) instead, using a standard matrix factorization technique called singular value decomposition (SVD) that decompose X into (UΣV^T): \\[\n\\begin{gather}\nθ = X^{+}Y\\\\\n   X = UΣV^{T}\\\\\nX^{+} = VΣ^{+}U^{T}\n\\end{gather}\n\\]\nImplement Linear regression using sklearn\n\nfrom sklearn.linear_model import LinearRegression\n\nnp.random.seed(29)\nx = np.random.randint(18,80,m)\ny = np.random.randint(-200,200,m) + 20*x\nX_train = x.reshape(-1,1)\n\nlinear = LinearRegression()\nlinear.fit(X_train,y)\ny_pred = linear.predict(X_train)\nprint(f'Equation: {linear.intercept_:.2f} + {linear.coef_[0]:.2f}*x')\n\nplt.plot(x, y, 'b.', label='True values')\nplt.plot(x, y_pred,'-',color='r', label='Linear regression')\nplt.xlabel('Age')\nplt.ylabel('Healthcare cost')\nplt.legend()\n\nplt.show()\n\nEquation: 1.41 + 20.21*x\n\n\n\n\n\nBoth the Normal equation and SVD approach scale well with the number of instances, but scale very badly with number of features. Therefore, we will look at another approach which is better suited for cases where there are a large number of features or too many training instances to fit in memory.\n\n\n\n\n\nIn fact, the computer really like the term ‘optimization’, which means we will take the result roughly equal to the correct one with the acceptable error. Gradient descent (GD) is that kind of method.\nGenerally, GD tweaks the weights iteratively in order to minimize a cost function. Steps to do Gradient Descent:\n\nTake Gradient (derivative) of Loss Function\nRandom initialization (take random weights)\nLoop step 3-5 until converge:\n\nCompute gradient\nCompute step size: StepSize = Gradient * Learning_rate\nCompute new weights: New = Old - StepSize\n\n\n[i] Run single epoch:\n\npartial_fit(): ignore (max_iter, tol) do not reset epoch counter\nfit(warm_start = True)\n\n\n\n\n\n\n\n\n\nLoss function: also called cost function, is the amount that we have to pay if we use the specific set of weights. Of course we want to minimize it cause everyone want to pay less but gain more, right 😆\nLearning rate: the pace of changing the weights in respond to the estimated loss\n\n\nToo small: take a long time to converge\nToo high: diverge\n\nNumber of epochs: times that we update our weights\n\nToo low: can’t get optimal solution\nToo high: waste time (parameters do not change much)\nSolution: set large epoch and a tolerance to interrupt when grandient &lt; tolerance\n\n\n\n\n\n\n\n\n\n\n\n\nSuitable learning rate\n\n\n\n\n\n\n\nToo low learning rate\n\n\n\n\n\n\n\nToo high learning rate\n\n\n\n\nFigure 1: Learning rate strategy\n\n\n\n\n\n\nLocal minimum: If we initialize weights from the left, we will reach local minimum instead of global minimum\nPlateau: if we initialize weights from the right, the gradient will change slowly and adding new instances to the training set doesn’t make the average error much better or worse. If early stopping, we will never reach the global minimum\n\n\n\n\nFigure 2: Gradient descent pitfalls\n\n\nFortunately, the cost function of linear regression is a convex function, which means it has no local minimum/ just one global minimum, and its slope never changes abruptly\n\\[\nMSE = \\frac{1}{2m}\\sum_{i=1}^{m}{(θ^{T}x_{i}-y_{i})}^2\n\\]\n\nAnother pitfall of GD: features have very different scales. Therefore, when using gradient descent, you should ensure that all features have a similar scale (e.g., using Scikit-Learn’s StandardScaler class), or else it will take much longer to converge.\n\n\n\n\nFigure 3: Gradient descent with (left) and without (right) feature scaling\n\n\n\n\n\n\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\nsgd = make_pipeline(StandardScaler(),\nSGDRegressor(max_iter=1000, tol=1e-3))\nsgd.fit(X_train, y)\nprint('Equation: %.2f + %.2f*x' % (sgd['sgdregressor'].intercept_[0], sgd['sgdregressor'].coef_[0]))\n\ny_pred = sgd.predict(X_train)\n\nplt.plot(x, y, 'b.', label='True values')\nplt.plot(x, y_pred,'-',color='r', label='Stochastic gradient descent regressor')\nplt.xlabel('Age')\nplt.ylabel('Healthcare cost')\nplt.legend()\n\nplt.show()\n\nEquation: 961.04 + 372.06*x\n\n\n\n\n\n\n\n\n\n\n\nThe intercept and coefficient in this equation are different from Section 1.1 because they implement on scaled X_train\n\n\n\nLearn more about Gradient descent."
  },
  {
    "objectID": "regression.html#polynomial-regression",
    "href": "regression.html#polynomial-regression",
    "title": "Regression",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression\nIf the data is more complex (non linear), what do we do? In that case, we just create new features by adding powers to existed features, and use them to fit to our linear model. This technique is called polynomial regression.\nFor example, we will use sklearn’s PolynomialFeatures to transform our data to higher degree, and then fit it to LinearRegression.\n\n\nCode\nnp.random.seed(29)\nm = 100\nX = 10*np.random.rand(m, 1)-5\ny = 10+ 1.5*X**2 + X + np.random.randn(m,1)\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\npipe = make_pipeline(PolynomialFeatures(degree=2, include_bias=False), LinearRegression())\n\npipe.fit(X, y)\ny_pred = pipe.predict(X)\n\nX_new = np.linspace(-5, 5, 100).reshape(-1, 1)\ny_pred_new = pipe.predict(X_new)\n\nplt.plot(X, y, 'b.', label='True values')\nplt.plot(X_new, y_pred_new,'-',color='r', label='Stochastic gradient descent regressor')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nIf we have n features, d degree: PolynomialFeatures transform into (n+d)! / (n!d!) features"
  },
  {
    "objectID": "regression.html#learning-curve",
    "href": "regression.html#learning-curve",
    "title": "Regression",
    "section": "Learning Curve",
    "text": "Learning Curve\n\nHow complex polynomial should be?\n\n\nUnderfitting (1 dgree): too simple model, can’t capture the pattern of data\nOverfitting (300 degrees): too complex model, tend to remember data\n\n\n\n\nFigure 4: Different polynomial degree\n\n\n\nHow can tell overfitting or underfitting? There are 2 strategies\n\nCross-validation\n- Overfitting: model perform well on train set, generate poorly on validation set\n- Underfitting: perform poorly on both train and validation sets\nLearning Curve - Plot training errors and validation errors over training set sizes (using cross-validation)\n- Overfitting: gap between the curves - Underfitting: Plateau (adding more training samples do not help)\n\nSo how do we handle the overfitting/underfitting model?\n\n\nOverfitting: Change too simpler model, feeding more training data, constrain the weights of unimportant features\n\nUnderfitting: Change to more complex algorithm; better features\n\n\n\n\n\n\n\nBias-Variation Trade-Off\n\n\n\n\nBias (underfitting): wrong assumptions (e.g. assump linear while quadratic)\nVariation (overfitting): remember data (sensitive to variations in data)\n=&gt; Trade-Off: Increase model’s complexity will increase variation and decrease bias\n\nIrreducible error: noisiness =&gt; clean up data"
  },
  {
    "objectID": "regression.html#regularized-linear-models",
    "href": "regression.html#regularized-linear-models",
    "title": "Regression",
    "section": "Regularized Linear Models",
    "text": "Regularized Linear Models\nAs mentioned above, to reduce overfitting we constrain the weights of model. These techniques are called regularization including: Ridge regression, Lasso Regression and Elastic net.\n\n\n\n\n\n\n\nRegularized linear models: Sensitive to the scale\n=&gt; StandardScaler before regularize\n\nIn almost cases, we should avoid plain Linear regression\nUse case of Regularized linear models:\n\n\nElastic Net: when there are few useful features, (features &gt; instances, correlated features =&gt; Lasso tends to behave erratically)\nLasso: when there are few useful features\nRidge: good for default (a warmstart)\n\n\nFind out more about RidgeCV, LassoCV and ElasticNetCV\n\n\n\n\n?@fig-l1-l2\n\nRidge Regression\nAdd a regularization term (L2 norm) to the MSE cost function of Linear regression in order to keep the weights as small as possible\nRidge regression cost function \\[\n\\begin{equation}\n\\begin{split}\nJ(θ) & = MSE(θ) + \\frac{α}{2m}\\sum_{i=1}^{m}w_i^2\\\\\n    & = MSE(θ) + \\frac{α}{2m}θ^Τθ\n\\end{split}\n\\end{equation}\n\\]\nClosed-form equation \\[θ = (X^{T}X + αΑ)^{-1}X^{T}Y\\]\n\n\n\n\n\n\nsklearn.linear_model.Ridge(solver=‘cholesky’)\n\n\n\n\nfrom sklearn.linear_model import Ridge\n\nnp.random.seed(29)\nm = 50\nX = 3 * np.random.rand(m, 1)\ny = 1 + 0.5 * X + np.random.randn(m, 1) / 1.5\n\ndef make_plot(alphas):\n    plt.plot(X, y, 'b.')\n    for alpha, style in zip(alphas, ['b:','r--','g-']):\n        pipe = make_pipeline(PolynomialFeatures(degree=5, include_bias=False), Ridge(alpha=alpha, solver='cholesky'))\n        pipe.fit(X, y)\n        X_new = np.linspace(0, 3, 100).reshape(-1, 1)\n        y_pred_new = pipe.predict(X_new)\n\n        plt.plot(X_new, y_pred_new, style, label='alpha = %s' % alpha)\n    plt.axis([0, 3, 0, 3.5])\n    plt.legend()\n    plt.show()\n\nmake_plot([0,0.1,1])\n\n\n\n\nGradient descent \\[\n\\begin{gather}\n∇ = \\frac{1}{m}X^{T}(Xθ - y)+\\frac{α}{m}θ\\\\\n\\\\\nθ = θ - λ∇\\\\\n\\end{gather}\n\\]\nThese 2 models are equally, in which we have to set the lpha in the SGD to be alpha/m\n\nfrom sklearn.linear_model import Ridge\n\nalpha = 0.01\n\nridge = Ridge(alpha=0.1, random_state=29)\nsgd = SGDRegressor(penalty='l2', alpha=0.1/m, random_state=29)\n\n\n\nLasso Regression\nAdd a regularization term (L1 norm) to the MSE cost function of Linear regression, but tend to eliminate weights of least important features\n=&gt; Weights is sparse matrix\nLasso regression cost function \\[\n\\begin{equation}\n\\begin{split}\nJ(θ) & = MSE(θ) + α\\sum_{i=1}^{m}|w|\\\\\n    & = MSE(θ) + αθ\n\\end{split}\n\\end{equation}\n\\]\nGradient descent\nThe L1 regularization is not differentiable at θi = 0, but gradient descent still works if we use a subgradient vector g11 instead when any θi = 0. Learn more about gradient descent for lasso regression\nThese 2 models are equally, and we have to adjust the alpha as same as ridge regression\n\nfrom sklearn.linear_model import Lasso\n\nalpha = 0.01\n\nridge = Lasso(alpha=0.1, random_state=29)\nsgd = SGDRegressor(penalty='l1', alpha=0.1/m, random_state=29)\n\n\n\nElastic Net Regression\nElastic Net is weighted sum of Ridge and Lasso regression, change the weights by r rate: 0 (more Ridge) to 1 (more Lasso)\n\\[\nJ(θ) = MSE(θ) + r*\\frac{α}{2m}\\sum_{i=1}^{m}w_i^2 + (1-r)α\\sum_{i=1}^{m}|w_i|\n\\]\n\nfrom sklearn.linear_model import ElasticNet\n\nelast = ElasticNet(alpha=0.01, l1_ratio=0.5)"
  },
  {
    "objectID": "regression.html#early-stopping",
    "href": "regression.html#early-stopping",
    "title": "Regression",
    "section": "Early Stopping",
    "text": "Early Stopping\nAnother way to regularize iterative learning algorithms (e.g. GD): partial_fit for n epochs and save the model has the lowest validation error\n\nfrom copy import deepcopy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n\n## Create data\n\nnp.random.seed(29)\nm = 100\nX = 6 * np.random.rand(m, 1) - 3\ny = 0.5 * X ** 2 + X + 2 + np.random.randn(m, 1)\nepochs = 500\nbest_rmse = np.inf\n\nx_train, x_test, y_train, y_test = train_test_split(X, y)\n\npreprocessing = make_pipeline(PolynomialFeatures(degree=90, include_bias=False), StandardScaler())\nX_train = preprocessing.fit_transform(x_train)\nX_test = preprocessing.transform(x_test)\n\n\nsgd = SGDRegressor(penalty='elasticnet', alpha=0.01, l1_ratio=0.5, eta0=0.001, random_state=29)\n\nfor epoch in range(epochs):\n    sgd.partial_fit(X_train, y_train.ravel())\n    y_pred = sgd.predict(X_test)\n    rmse = mean_squared_error(y_test, y_pred, squared=False)\n    if rmse &lt; best_rmse:\n        best_rmse = rmse\n        best_model = deepcopy(sgd)\n\ny_pred = best_model.predict(X_test)\n\n\n## Another way to apply early stopping\n\n# sgd = SGDRegressor(penalty='elasticnet', alpha=0.01, l1_ratio=0.5, max_iter = 2000, tol=0.00001, shuffle=True, random_state=29, learning_rate='invscaling', eta0=0.001, early_stopping=True, validation_fraction=0.25, n_iter_no_change=10)\n# sgd.fit(X_train, y_train.ravel())\n# y_pred = sgd.predict(X_test)\nprint('RMSE: %.2f' % mean_squared_error(y_test, y_pred))\n\nRMSE: 0.67\n\n\n\n\n\n\n\n\npartial_fit: max_iter=1 (fit 1 epoch per calling); learn incrementally from a mini-batch of instances =&gt; useful when data is not fit into memory\nfit: train model from scratch (all instances at once)\nfit(warm_start=True) = partial_fit: allow learning from the weights of previous fit\ncopy.deepcopy(): copies both the model’s hyperparameters and the learned parameters\nsklearn.base.clone() only copies the model’s hyperparameters."
  }
]