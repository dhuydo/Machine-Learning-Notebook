[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Notebook",
    "section": "",
    "text": "Welcome\nThis is the website for the 2nd edition of “R for Data Science”. This book will teach you how to do data science with R: You’ll learn how to get your data into R, get it into the most useful structure, transform it and visualize.\nIn this book, you will find a practicum of skills for data science. Just as a chemist learns how to clean test tubes and stock a lab, you’ll learn how to clean data and draw plots—and many other things besides. These are the skills that allow data science to happen, and here you will find the best practices for doing each of these things with R. You’ll learn how to use the grammar of graphics, literate programming, and reproducible research to save time. You’ll also learn how to manage cognitive resources to facilitate discoveries when wrangling, visualizing, and exploring data."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Machine Learning Notebook",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nR4DS is hosted by https://www.netlify.com as part of their support of open source software and communities."
  },
  {
    "objectID": "project-checklist.html#expand-to-view-questions-from-the-lesson",
    "href": "project-checklist.html#expand-to-view-questions-from-the-lesson",
    "title": "2  Structure of Machine Learning Project",
    "section": "2.1 Expand to View Questions from the Lesson",
    "text": "2.1 Expand to View Questions from the Lesson\n\nWhat is Machine Learning (ML)? Comparing ML Approach versus Traditional one?\nApplications of ML? Tasks that ML especially useful for?\nPresent Types of ML System according to:\n\nTraining Supervision: Supervised Learning, Unsupervised Learning, Reinforcement Learning, etc.\nBatch Learning versus Online Learning\nInstance-based versus Model-based\n\nMain Challenges of ML?\nThe ML Project Workflow"
  },
  {
    "objectID": "project-checklist.html",
    "href": "project-checklist.html",
    "title": "2  Structure of Machine Learning Project",
    "section": "",
    "text": "What is Machine Learning (ML)? Comparing ML Approach versus Traditional one?\nApplications of ML? Tasks that ML especially useful for?\nPresent Types of ML System according to:\n\nTraining Supervision: Supervised Learning, Unsupervised Learning, Reinforcement Learning, etc.\nBatch Learning versus Online Learning\nInstance-based versus Model-based\n\nMain Challenges of ML?\nThe ML Project Workflow"
  },
  {
    "objectID": "landscape.html#what-is-machine-learning",
    "href": "landscape.html#what-is-machine-learning",
    "title": "1  Machine Learning Landscape",
    "section": "1.1 What is Machine Learning",
    "text": "1.1 What is Machine Learning\n\n“Machine learning is the field of study that gives computers the ability to learn without being explicitly programmed.” — Arthur Samuel, 1959\n\nTraditional Approach\nIf you have a task using data to make a prediction (e.g. classification), with traditional approach you have to come up with a bunch of if-else clauses to handle it.\n\nFor example, to write a spam email filter program, you have to know some words or phrases that usually appear in a spam email, then your program would flag emails as spam if a number of these patterns were detected. Finally, you would test your program until it was good enough to launch.\n\n\n\nFigure 1.1: Traditional approach\n\n\nBut how is it if the spammers notice the rules that we set?\nThey will adjust these patterns a little bit that our program cannot detect, and if they keep working around our spam filter, we will need to keep writing new rules that gradually become a very long complex list. This is where the ML approach comes into play.\nThe Machine Learning Approach\nAs Arthur Samuel said, the ML system can learn patterns from the data without being explicitly programmed.\nIn this case, a spam filter based on ML techniques automatically notices unusually frequent patterns in spam flagged by users, and it starts flagging them without our intervention.\n\n\n\nFigure 1.2: Machine Learning approach"
  },
  {
    "objectID": "landscape.html#applications-of-machine-learning",
    "href": "landscape.html#applications-of-machine-learning",
    "title": "1  Machine Learning Landscape",
    "section": "1.2 Applications of Machine Learning",
    "text": "1.2 Applications of Machine Learning\nCommon tasks\n- Regression: Forecast revenue, weather, impact of certificate scores on admission, financial and logistical forecasting\n- Classification: Anomaly detection\n- Clustering: Segmenting clients\n\nNatural Language Processing\nText classification (topic modeling, flagging offensive comments), text summarization, questions-answering modules (chatbot), speech recognition\nComputer vision\nImage classification, semantic image segmentation, face recognition, image captioning\nRecommendation system\nWeb search, product recommendations, home page layout\nRobotics\nHandling objects, decision-making\nMedicine\nFinding anomalies in radiology images, measuring features in ultrasounds\nBiology\nClassifying cell/proteins, tumor-normal sequencing and classifying\nPlaying games\nChess, Go, most Atari video games, and many real-time strategy games\n\n\n\n\n\n\nTo summarize, machine learning is especially great for:\n\n\nProblems for which existing solutions are complex and/or require a lot of work\n\nFluctuating environments, hard/retrain to keep up to date\n\nGetting insights about complex problems and large amounts of data"
  },
  {
    "objectID": "landscape.html#types-of-machine-learning-system",
    "href": "landscape.html#types-of-machine-learning-system",
    "title": "1  Machine Learning Landscape",
    "section": "1.3 Types of Machine Learning System",
    "text": "1.3 Types of Machine Learning System\n\n\n\n\n\n\nMachine Learning Systems are divided into 3 common types based on:\n\n\nTraining supervision: supervised learning, unsupervised learning, reinforcement learning\n\nLearn on the fly or not: online learning versus batch learning\n\nLearn by heart or detect patterns: instance-based learning versus model-based learning\n\n\n\n\n\n1.3.1 Training Supervision\nHow they supervised: supervised, unsupervised, semi-supervised, self-supervised, etc.\n\nSupervised: Classification and Regression\nUnsupervised: Clustering, Dimensionality Reduction, Association Rule Learning\nSemi-supervised: Partially labeled\nSelf-supervised: Unlabeled -&gt; Fully labeled. Learn to generate labels. Unlike unsupervised learning, self-supervised focuses on classification and regression, although it learns from fully unlabeled data\nReinforcement Learning: Optimize rewards -&gt; Find best strategy (called policy) itself\n\n\n\n1.3.2 Learn on the fly\nWhether or not they can learn incrementally\nBatch Learning (also called Offline Learning) \n\nLearn offline using all available data, apply what it has learned without learning more.\n\nUseful for areas that do not change too much =&gt; Model’s performance will not decay fast\n\nNot useful for fast-evolving system (ie. financial market) =&gt; Model’s performance will decay rellay fast =&gt; Have to train new model\n\nOnline Learning (also called Incremental Learning)\n\n\nRun and learn simultanously by feeding new data sequentially called mini-batches\n\nSuitable for rapidly changed system, limited computing resources.\nImportant parameter: learning rate\nChallenges: bad data =&gt; decline performance quickly =&gt; Solutions: monitor system (switch learning off, anomaly detection)\n\n\n\n1.3.3 Instance-Based Versus Model-Based Learning\nCompare new data to known data/Detect patterns in data\nInstance-based Learning\n\n\nLearn by heart, then generalize new data using a similarity measure (e.g. k-Nearest Neighbors Regression)\n\nModel-based Learning\n\n\nFeed data as examples to model =&gt; Able to make predictions (e.g. Linear Regression)\nMinimize cost function =&gt; Find best parameters"
  },
  {
    "objectID": "landscape.html#challenges-of-machine-learning",
    "href": "landscape.html#challenges-of-machine-learning",
    "title": "1  Machine Learning Landscape",
    "section": "1.4 Challenges of Machine Learning",
    "text": "1.4 Challenges of Machine Learning\n\n1.4.1 Insufficient Quantity of Data\nConsider the amount of data whether it’s sufficient or not rather than abandon the algorithm\n\n\n\nFigure 1.3: The importance of data versus algorithms\n\n\n\n\n1.4.2 Non-representative Training Data\nThe training data must be representative of the new cases you want to generalize to\n\nIf data is too small: sampling noise\nIf large data but flawed sampling method: sampling bias\n\n\n\n1.4.3 Poor-quality Data\n\nErrors, Outliers, Noises, etc. =&gt; Model perform badly\nSolutions:\n\nRemove outliers/Fix errors\nInstances have missing features =&gt;\n\nIgnore Instances\nIgnore Features\nFill in Missing Values\nTrain model with/without features\n\n\n\n\n\n1.4.4 Irrelevant Features\nFeature Engineering:\n\nFeature Selection: Select most useful features\nFeature Extraction: Combine existing features to produce a more useful one\nCreate new feature: from exist data/gather new data\n\n\n\n1.4.5 Overfitting the Training Data\nModel tends to remember data =&gt; Perform well on Training Data, but generalize badly (e.g. Fit noisy/too small data to complex models (NNs))\nSolutions:\n\nChoose simpler models (less parameters)\nContrain models (regularization)\nGather more data\nReduct noise data\n\n\n\n1.4.6 Underfitting the Training Data\nModel is too simple to learn underlying structure of data\nSolutions:\n\nChoose more powerful model (more parameters)\nReduce the model’s contraints,\nFeature engineering\n\n\n\n1.4.7 Summarise\n\nMachine learning is about making machines get better at some task by learning from data, instead of having to explicitly code rules.\nThere are many different types of ML systems: supervised or not, batch or online, instance-based or model-based.\nIn an ML project you gather data in a training set, and you feed the training set to a learning algorithm. If the algorithm is model-based, it tunes some parameters to fit the model to the training set (i.e., to make good predictions on the training set itself), and then hopefully it will be able to make good predictions on new cases as well. If the algorithm is instance-based, it just learns the examples by heart and generalizes to new instances by using a similarity measure to compare them to the learned instances.\nThe system will not perform well if your training set is too small, or if the data is not representative, is noisy, or is polluted with irrelevant features (garbage in, garbage out). Lastly, your model needs to be neither too simple (in which case it will underfit) nor too complex (in which case it will overfit)."
  },
  {
    "objectID": "landscape.html#testing-and-validating",
    "href": "landscape.html#testing-and-validating",
    "title": "1  Machine Learning Landscape",
    "section": "1.5 Testing and Validating",
    "text": "1.5 Testing and Validating\n\ntrain/val/test split\nError rate on new cases: generalization error (also called out-of-sample error)\ncross-validation:\n\nusing many small validation sets (n).\nTrain models n times with (n-1) validation sets =&gt; Test with the rest set.\nAverage results =&gt; much more accurate measure.\nDrawback: the training time is multiplied by the number of validation sets\n\n\n\n1.5.1 Data Mismatch\n\n\n\nFigure 1.4: Data Mismatch\n\n\n\n\n1.5.2 No Free Lunch Theorem\n\n\n\nFigure 1.5: No Free Lunch Theorem"
  },
  {
    "objectID": "landscape.html#the-machine-learning-workflow",
    "href": "landscape.html#the-machine-learning-workflow",
    "title": "1  Machine Learning Landscape",
    "section": "1.6 The Machine Learning Workflow",
    "text": "1.6 The Machine Learning Workflow\n\nFrame the problem and look at the big picture.\nGet the data.\nExplore the data to gain insights.\nPrepare the data to better expose the underlying data patterns to machine learn‐ ing algorithms.\nExplore many different models and shortlist the best ones.\nFine-tune your models and combine them into a great solution.\nPresent your solution.\nLaunch, monitor, and maintain your system.\n\nGo to Chapter 2 for details of each step"
  },
  {
    "objectID": "project-checklist.html#frame-problem",
    "href": "project-checklist.html#frame-problem",
    "title": "2  Machine Learning Project Workflow",
    "section": "2.1 Frame problem",
    "text": "2.1 Frame problem\n\nObjective and current solutions?\nNew solutions: how to use\nDepend on type of problems: possible models, performance measuring\nMinimum needed performance\nComparable problems -&gt; Can reuse experiment, tools?\nList and verify assumptions (if available)"
  },
  {
    "objectID": "project-checklist.html#get-data",
    "href": "project-checklist.html#get-data",
    "title": "2  Machine Learning Project Workflow",
    "section": "2.2 Get data",
    "text": "2.2 Get data\nAutomate as much as possible\n\n\nList data: where to get, how much (features, instances), storage space\nGet and convert data if necessary\nAnonymize sensitive information\nRecheck data"
  },
  {
    "objectID": "project-checklist.html#gain-insights",
    "href": "project-checklist.html#gain-insights",
    "title": "2  Machine Learning Project Workflow",
    "section": "2.3 Gain Insights",
    "text": "2.3 Gain Insights\n\nCreate a copy of the data for exploration (sampling it down to a manageable size if necessary).\nCreate a notebook to keep a record of your data exploration.\nStudy each attribute and its characteristics:\n\n\nName\nType (categorical, int/float, bounded/unbounded, text, structured, etc.)\n% of missing values\nNoisiness and type of noise (stochastic, outliers, rounding errors, etc.)\nUsefulness for the task\nType of distribution (Gaussian, uniform, logarithmic, etc.)\n\n\nFor supervised learning tasks, identify the target attribute(s).\nVisualize the data.\nStudy the correlations/mututal information\nIdentify the promising transformations/feature engineering\nIdentify extra data that would be useful.\nDocument what you have learned."
  },
  {
    "objectID": "project-checklist.html#prepare-data",
    "href": "project-checklist.html#prepare-data",
    "title": "2  Machine Learning Project Workflow",
    "section": "2.4 Prepare data",
    "text": "2.4 Prepare data\n\n\n\n\n\n\n\nWork on copies of the data (keep the original dataset intact).\nWrite functions for all data transformations you apply, for five reasons:\n— Easily prepare the data the next time you get a fresh dataset\n— Easily to apply transformations for test set/new instances once solution is live\n— Treat preparation choices as hyperparameters\n\n\n\n\n\nClean the data\n\n\nFix or remove outliers (optional).\nFill in missing values (e.g., with zero, mean, median…) or drop their rows (or columns).\n\n\nPerform feature selection (optional)\n\n\nDrop the attributes that provide no useful information for the task.\n\n\nPerform feature engineering, where appropriate: • Discretize continuous features. • Decompose features (e.g., categorical, date/time, etc.). • Add promising transformations of features (e.g., log(x), sqrt(x), x2, etc.). • Aggregate features into promising new features.\nPerform feature scaling: • Standardize or normalize features."
  },
  {
    "objectID": "project-checklist.html#choose-models",
    "href": "project-checklist.html#choose-models",
    "title": "2  Machine Learning Project Workflow",
    "section": "2.5 Choose models",
    "text": "2.5 Choose models\nNotes: • If the data is huge, you may want to sample smaller training sets so you can train many different models in a reasonable time (be aware that this penalizes complex models such as large neural nets or random forests). • Once again, try to automate these steps as much as possible. 1. Train many quick-and-dirty models from different categories (e.g., linear, naive Bayes, SVM, random forest, neural net, etc.) using standard parameters. 2. Measure and compare their performance: • For each model, use N-fold cross-validation and compute the mean and stan‐ dard deviation of the performance measure on the N folds. 3. Analyze the most significant variables for each algorithm. 4. Analyze the types of errors the models make: • What data would a human have used to avoid these errors? 5. Perform a quick round of feature selection and engineering. 6. Perform one or two more quick iterations of the five previous steps. 7. Shortlist the top three to five most promising models, preferring models that make different types of errors."
  },
  {
    "objectID": "project-checklist.html#fine-tune-and-combine-models",
    "href": "project-checklist.html#fine-tune-and-combine-models",
    "title": "2  Machine Learning Project Workflow",
    "section": "2.6 Fine-tune and combine models",
    "text": "2.6 Fine-tune and combine models\nNotes: • You will want to use as much data as possible for this step, especially as you move toward the end of fine-tuning. • As always, automate what you can. 1. Fine-tune the hyperparameters using cross-validation: • Treat your data transformation choices as hyperparameters, especially when you are not sure about them (e.g., if you’re not sure whether to replace missing values with zeros or with the median value, or to just drop the rows). • Unless there are very few hyperparameter values to explore, prefer random search over grid search. If training is very long, you may prefer a Bayesian optimization approach (e.g., using Gaussian process priors, as described by Jasper Snoek et al.1). 2. Try ensemble methods. Combining your best models will often produce better performance than running them individually. 3. Once you are confident about your final model, measure its performance on the test set to estimate the generalization error.\n\n\n\n\n\n\nDon’t tweak your model after measuring the generalization error: you would just start overfitting the test set."
  },
  {
    "objectID": "project-checklist.html#present-solutions",
    "href": "project-checklist.html#present-solutions",
    "title": "2  Machine Learning Project Workflow",
    "section": "2.7 Present solutions",
    "text": "2.7 Present solutions\n\nDocument what you have done.\nCreate a nice presentation: Make sure you highlight the big picture first.\nExplain why your solution achieves the business objective.\nDon’t forget to present interesting points you noticed along the way: • Describe what worked and what did not. • List your assumptions and your system’s limitations.\nEnsure your key findings are communicated through beautiful visualizations or easy-to-remember statements (e.g., “the median income is the number-one predictor of housing prices”)."
  },
  {
    "objectID": "project-checklist.html#launch-monitor-and-maintain-system",
    "href": "project-checklist.html#launch-monitor-and-maintain-system",
    "title": "2  Machine Learning Project Workflow",
    "section": "2.8 Launch, monitor and maintain system",
    "text": "2.8 Launch, monitor and maintain system\n\nGet your solution ready for production (plug into production data inputs, write unit tests, etc.).\nWrite monitoring code to check your system’s live performance at regular inter‐ vals and trigger alerts when it drops: • Beware of slow degradation: models tend to “rot” as data evolves. • Measuring performance may require a human pipeline (e.g., via a crowdsourc‐ ing service). • Also monitor your inputs’ quality (e.g., a malfunctioning sensor sending ran‐ dom values, or another team’s output becoming stale). This is particularly important for online learning systems.\nRetrain your models on a regular basis on fresh data (automate as much as possible)."
  },
  {
    "objectID": "project-checklist.html#section",
    "href": "project-checklist.html#section",
    "title": "2  Machine Learning Project Workflow",
    "section": "2.9 ",
    "text": "2.9"
  },
  {
    "objectID": "project.html#main-steps",
    "href": "project.html#main-steps",
    "title": "3  A Complete Machine Learning Project",
    "section": "3.1 Main Steps",
    "text": "3.1 Main Steps\n\nLook at the big picture.\nGet the data.\nExplore and visualize the data to gain insights.\nPrepare the data for machine learning algorithms.\nSelect a model and train it.\nFine-tune model.\nPresent solution.\nLaunch, monitor, and maintain system."
  },
  {
    "objectID": "project.html#datasets",
    "href": "project.html#datasets",
    "title": "3  A Complete Machine Learning Project",
    "section": "3.1 Datasets",
    "text": "3.1 Datasets\nPopular open data repositories\n\nhttps://openml.org/\n\nhttps://Kaggle.com\nhttps://PapersWithCode.com\n\nUC Irvine Machine Learning Repository — Amazon’s AWS datasets\nTensorFlow datasets\n\nMeta portals (they list open data repositories)\n\nhttps://DataPortals.org\n\nhttps://OpenDataMonitor.eu\n\nOther pages listing many popular open data repositories\n\nWikipedia’s list of machine learning datasets\nhttps://Quora.com\nThe datasets subreddit\n\nIn this chapter we’ll use the California Housing Prices dataset from the StatLib repository. This dataset is based on data from the 1990 California census. It is not exactly recent but it has many qualities for learning."
  },
  {
    "objectID": "project.html#look-at-the-big-picture",
    "href": "project.html#look-at-the-big-picture",
    "title": "3  A Complete Machine Learning Project",
    "section": "3.2 Look at the big picture",
    "text": "3.2 Look at the big picture\nQuestions\n\nBusiness objective? Current solution (if any)?\nHow to use and benefit from the model?\nData Pipelines?\nDetermine kind of model\nSelect preformance measures\nCheck the Assumptions\n\nAnswers\n\nPredict median housing price in any district. The results are used for another ML system for investment analysis. The current solution is to gather up-to-date information about a district, or to estimate manually using complex rules if no information.\nThe current solution is costly and time-consuming, and it was often off by more than 30%. Therefore, a ML model could be more useful\nData pipelines: A sequence of data processing components. Pipelines are very common in machine learning systems, since there is a lot of data to manipulate and many data transformations to apply.\nThis is a regression task (labeled data), batch learning (data is quite small and not change too much) and model-based learning\nMetrics for regression:\n\n\n\n\n\n\n\nExpand to learn more about metrics\n\n\n\n\n\nBoth the RMSE and the MAE are ways to measure the distance between two vectors: the vector of predictions and the vector of target values. Various distance measures, or norms, are possible:\n• Computing the root of a sum of squares (RMSE) corresponds to the Euclidean norm: this is the notion of distance we are all familiar with. It is also called the l2 norm, noted |·|₂ (or just |·|).\n• Computing the sum of absolutes (MAE) corresponds to the l1 norm, noted |·|₁. This is sometimes called the Manhattan norm because it measures the distance between two points in a city if you can only travel along orthogonal city blocks.\n• More generally, the lk norm of a vector v containing n elements is defined as ∥v∥k = (|v₁|ᵏ + |v₂|ᵏ + … + |vₙ|ᵏ)¹/ᵏ. l0 gives the number of nonzero elements in the vector, and l∞ gives the maximum absolute value in the vector.\nThe higher the norm index, the more it focuses on large values and neglects small ones. This is why the RMSE is more sensitive to outliers than the MAE. But when outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs very well and is generally preferred. \n\n\n\nRMSE (root mean squared error): l2 norm\n\\[\nRMSE(X, y)  = \\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}(y_{hat}^{(i)} - y^{(i)})^2}\n\\]\nMAE (mean squared error): l1 norm\n\\[\nMAE(X, y)  = \\frac{1}{m}\\sum_{i=1}^{m}|y_{hat}^{(i)} - y^{(i)}|\n\\]\n\nCheck with the team in charge of the downstream system that use out output whether it is suitable or not (e.g. it is terrible if after several months building model you realize that they need ordinal output not numerical one)"
  },
  {
    "objectID": "project.html#get-the-data",
    "href": "project.html#get-the-data",
    "title": "3  A Complete Machine Learning Project",
    "section": "3.3 Get the Data",
    "text": "3.3 Get the Data\n\n3.3.1 Download Data\n\n## Load data\n\nfrom pathlib import Path\nimport pandas as pd\nimport tarfile\nimport urllib.request\n\ndef load_data(url):\n    path = Path(\"datasets/housing.tgz\")\n    if not path.is_file():\n        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n        urllib.request.urlretrieve(url, path)\n        tarfile.open(path).extractall(path='datasets')\n    return pd.read_csv('datasets/housing/housing.csv')\n\nurl = 'https://github.com/ageron/data/raw/main/housing.tgz'\ndata = load_data(url)\n\n\n\n3.3.2 Quick Look to Data Structure: head(), info(), describe(), value_counts(), histplot()\n\n## Quick look\n\npd.set_option('display.max_columns', None)      # display all columns\ndata.head()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n0\n-122.23\n37.88\n41.0\n880.0\n129.0\n322.0\n126.0\n8.3252\n452600.0\nNEAR BAY\n\n\n1\n-122.22\n37.86\n21.0\n7099.0\n1106.0\n2401.0\n1138.0\n8.3014\n358500.0\nNEAR BAY\n\n\n2\n-122.24\n37.85\n52.0\n1467.0\n190.0\n496.0\n177.0\n7.2574\n352100.0\nNEAR BAY\n\n\n3\n-122.25\n37.85\n52.0\n1274.0\n235.0\n558.0\n219.0\n5.6431\n341300.0\nNEAR BAY\n\n\n4\n-122.25\n37.85\n52.0\n1627.0\n280.0\n565.0\n259.0\n3.8462\n342200.0\nNEAR BAY\n\n\n\n\n\n\n\nThere are total 10 features, each row represents a district observation\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 10 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   longitude           20640 non-null  float64\n 1   latitude            20640 non-null  float64\n 2   housing_median_age  20640 non-null  float64\n 3   total_rooms         20640 non-null  float64\n 4   total_bedrooms      20433 non-null  float64\n 5   population          20640 non-null  float64\n 6   households          20640 non-null  float64\n 7   median_income       20640 non-null  float64\n 8   median_house_value  20640 non-null  float64\n 9   ocean_proximity     20640 non-null  object \ndtypes: float64(9), object(1)\nmemory usage: 1.6+ MB\n\n\nData with 10 columns and 20640 rows 9 numerical features, 1 categorical feature ‘total_bedrooms’ has only 20433 non-null values\n\ndata.describe(include='all')        # describe all type of data\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\ncount\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20433.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640\n\n\nunique\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n5\n\n\ntop\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n&lt;1H OCEAN\n\n\nfreq\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n9136\n\n\nmean\n-119.569704\n35.631861\n28.639486\n2635.763081\n537.870553\n1425.476744\n499.539680\n3.870671\n206855.816909\nNaN\n\n\nstd\n2.003532\n2.135952\n12.585558\n2181.615252\n421.385070\n1132.462122\n382.329753\n1.899822\n115395.615874\nNaN\n\n\nmin\n-124.350000\n32.540000\n1.000000\n2.000000\n1.000000\n3.000000\n1.000000\n0.499900\n14999.000000\nNaN\n\n\n25%\n-121.800000\n33.930000\n18.000000\n1447.750000\n296.000000\n787.000000\n280.000000\n2.563400\n119600.000000\nNaN\n\n\n50%\n-118.490000\n34.260000\n29.000000\n2127.000000\n435.000000\n1166.000000\n409.000000\n3.534800\n179700.000000\nNaN\n\n\n75%\n-118.010000\n37.710000\n37.000000\n3148.000000\n647.000000\n1725.000000\n605.000000\n4.743250\n264725.000000\nNaN\n\n\nmax\n-114.310000\n41.950000\n52.000000\n39320.000000\n6445.000000\n35682.000000\n6082.000000\n15.000100\n500001.000000\nNaN\n\n\n\n\n\n\n\n\nfor ft in data.select_dtypes('object'):     # choose 'object' features only\n    print(data[ft].value_counts())\n    print(f'Number of classes: {data[ft].nunique()}')\n\nocean_proximity\n&lt;1H OCEAN     9136\nINLAND        6551\nNEAR OCEAN    2658\nNEAR BAY      2290\nISLAND           5\nName: count, dtype: int64\nNumber of classes: 5\n\n\nQuite imbalanced classes\n\n## Plot histogram of numerical features\n\nimport matplotlib.pyplot as plt\n\ndata.hist(bins=50, figsize=(8,8))\nplt.show()\n\n\n\n\nhousing_median_age: capped at range (1,52)\nmedian_income: scaled and capped at range ~ (0.5, 15)\nmedian_house_value: capped at top range of 500,000\nThese features are skewed and have very different scales =&gt; Transforming and Feature Scaling\n\n\n3.3.3 Create a Test Set: train_test_split()\nrandom sampling: data must be large enough, otherwise there is a risk of sampling bias stratified sampling: based on some very important features, help avoid bias and ensure train set and test set are representative to full dataset\nHere we assump that median_income is very important feature to predict household value\n\n## Create new feature (income group)\nimport numpy as np\n\ndata['income_grp'] = pd.cut(data['median_income'], bins=[0,1.5,3,4.5,6,np.inf], labels=[1,2,3,4,5])\n\ndata.income_grp.value_counts().sort_index().plot.bar(rot=0, grid=True)\nplt.title('Frequency of income group')\nplt.xlabel('Income group')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n## Split data into train, val, test set\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(data.drop('median_house_value', axis=1), data.median_house_value, stratify=data['income_grp'], test_size=0.2, random_state=24)\n\n\n## Drop the 'income_grp' after using\n\nfor df in [x_train, x_test]:\n    df.drop('income_grp', axis=1, inplace=True)"
  },
  {
    "objectID": "project.html#explore-and-visualize-the-data-to-gain-insights",
    "href": "project.html#explore-and-visualize-the-data-to-gain-insights",
    "title": "3  A Complete Machine Learning Project",
    "section": "3.4 Explore and visualize the data to gain insights",
    "text": "3.4 Explore and visualize the data to gain insights\nIf the data is very large, we will sample an exploration set to manipulate faster and easier. On the other hand, just work directly on full set if the data is quite small\n\ndf_vis = data.copy()\n\n\n3.4.1 Visualize\nPlot\n\n\n\n\n\nFigure 3.1: The geographical plot of data\n\n\n\n\nLarge point size: larger population Blue -&gt; red: higher house price\nCorrelation\nThere are 2 ways to perform: heatmap and pairgrid map\n\n\n\n\n&lt;Axes: &gt;\n(a) Correlation Plot\n\n\n\n\n\n\n(b)\n\n\n\nFigure 3.2: ?(caption)\n\n\n\n\n\n\n\nFigure 3.3: PairGrid Plot of numerical features\n\n\n\n\nTry pd.plotting.scatter_matrix\nLook closely too the relation between ‘median_house_value’ and ‘median_income’, we see there is a strong positive correlation, but there are some clearly horizontal line at 500,000; 450,000; 350,000 and roughly 280,000. We should remove these instances to prevent the algorithms learning these patterns.\n\n\n\n\n\nFigure 3.4: Median income versus median house value\n\n\n\n\n\n\n3.4.2 Attributes combination\nUseful when we want to find better features to predict\n\ndf_vis['room_per_house'] = df_vis['total_rooms']/df_vis['households']\ndf_vis['bedroom_ratio'] = df_vis['total_bedrooms']/df_vis['total_rooms']\ndf_vis['people_per_house'] = df_vis['population']/df_vis['households']\n\ncorr_matrix = df_vis.corr(numeric_only=True)\ncorr_matrix['median_house_value'].sort_values(ascending=False)\n\nmedian_house_value    1.000000\nmedian_income         0.688075\nroom_per_house        0.151948\ntotal_rooms           0.134153\nhousing_median_age    0.105623\nhouseholds            0.065843\ntotal_bedrooms        0.049686\npeople_per_house     -0.023737\npopulation           -0.024650\nlongitude            -0.045967\nlatitude             -0.144160\nbedroom_ratio        -0.255880\nName: median_house_value, dtype: float64"
  },
  {
    "objectID": "project.html#section",
    "href": "project.html#section",
    "title": "3  A Complete Machine Learning Project",
    "section": "3.5 ",
    "text": "3.5"
  },
  {
    "objectID": "hands-on-classification.html#describe-the-used-dataset",
    "href": "hands-on-classification.html#describe-the-used-dataset",
    "title": "7  Hands-on Classification",
    "section": "7.1 Describe the used dataset",
    "text": "7.1 Describe the used dataset\n\nName: MNIST\nAuthor: Yann LeCun, Corinna Cortes, Christopher J.C. Burges\nContent: 70,000 images of digits handwritten\nSource: MNIST Website"
  },
  {
    "objectID": "hands-on-classification.html#get-data",
    "href": "hands-on-classification.html#get-data",
    "title": "7  Hands-on Classification",
    "section": "7.2 Get data",
    "text": "7.2 Get data\n\n7.2.1 Download data\n\nfrom sklearn.datasets import fetch_openml\n\nmnist = fetch_openml('mnist_784', as_frame=False)       # as_frame=False: get data as Numpy Array instead of Pandas DataFrame\nmnist.DESCR\n\n/usr/local/anaconda3/envs/dhuy/lib/python3.11/site-packages/sklearn/datasets/_openml.py:1022: FutureWarning:\n\nThe default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n\n\n\n\"**Author**: Yann LeCun, Corinna Cortes, Christopher J.C. Burges  \\n**Source**: [MNIST Website](http://yann.lecun.com/exdb/mnist/) - Date unknown  \\n**Please cite**:  \\n\\nThe MNIST database of handwritten digits with 784 features, raw data available at: http://yann.lecun.com/exdb/mnist/. It can be split in a training set of the first 60,000 examples, and a test set of 10,000 examples  \\n\\nIt is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image. It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting. The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.  \\n\\nWith some classification methods (particularly template-based methods, such as SVM and K-nearest neighbors), the error rate improves when the digits are centered by bounding box rather than center of mass. If you do this kind of pre-processing, you should report it in your publications. The MNIST database was constructed from NIST's NIST originally designated SD-3 as their training set and SD-1 as their test set. However, SD-3 is much cleaner and easier to recognize than SD-1. The reason for this can be found on the fact that SD-3 was collected among Census Bureau employees, while SD-1 was collected among high-school students. Drawing sensible conclusions from learning experiments requires that the result be independent of the choice of training set and test among the complete set of samples. Therefore it was necessary to build a new database by mixing NIST's datasets.  \\n\\nThe MNIST training set is composed of 30,000 patterns from SD-3 and 30,000 patterns from SD-1. Our test set was composed of 5,000 patterns from SD-3 and 5,000 patterns from SD-1. The 60,000 pattern training set contained examples from approximately 250 writers. We made sure that the sets of writers of the training set and test set were disjoint. SD-1 contains 58,527 digit images written by 500 different writers. In contrast to SD-3, where blocks of data from each writer appeared in sequence, the data in SD-1 is scrambled. Writer identities for SD-1 is available and we used this information to unscramble the writers. We then split SD-1 in two: characters written by the first 250 writers went into our new training set. The remaining 250 writers were placed in our test set. Thus we had two sets with nearly 30,000 examples each. The new training set was completed with enough examples from SD-3, starting at pattern # 0, to make a full set of 60,000 training patterns. Similarly, the new test set was completed with SD-3 examples starting at pattern # 35,000 to make a full set with 60,000 test patterns. Only a subset of 10,000 test images (5,000 from SD-1 and 5,000 from SD-3) is available on this site. The full 60,000 sample training set is available.\\n\\nDownloaded from openml.org.\"\n\n\n\n\n7.2.2 Quick Look\n\n## Size of dataset\n\nX,y = mnist.data, mnist.target\nprint(X.shape, y.shape)\n\n(70000, 784) (70000,)\n\n\n\n## Quick look\n\nimport matplotlib.pyplot as plt\n\ndef plot_digit(data):\n    image = data.reshape(28,28)\n    plt.imshow(image, cmap='binary')   # binary: grayscale color map from 0 (white) to 255 (black)\n    \nsome_digit = X[0]    # Look at first digit\nplot_digit(some_digit)\nplt.show()\n\n\n\n\n\n\n7.2.3 Create train, test set\n\n## Split dataset into train set and test set as its describe (train: first 60000 images, test: last 10000 images)\n\nX_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\nprint(X_train.shape)\n\n(60000, 784)"
  },
  {
    "objectID": "hands-on-classification.html#create-a-binary-classfier5-or-non-5",
    "href": "hands-on-classification.html#create-a-binary-classfier5-or-non-5",
    "title": "7  Hands-on Classification",
    "section": "7.3 Create a Binary Classfier(5 or non-5)",
    "text": "7.3 Create a Binary Classfier(5 or non-5)\n\n## Target labels\n\ny_train_5 = (y_train == '5')\ny_test_5 = (y_test == '5')\n\n\n7.3.1 Stochastic Gradient Descent\n\n7.3.1.1 Train model\n\nfrom sklearn.linear_model import SGDClassifier\n\nsgd_clf = SGDClassifier(random_state=42)\nsgd_clf.fit(X_train, y_train_5)\n\nsgd_clf.predict([some_digit])\n\narray([ True])\n\n\n\n\n7.3.1.2 Evaluate model\n\n\n\n\n\n\nMetrics:\n- Accuracy\n- Confusion matrix: Precision, Recall (TPR), FPR, ROC, ROC AUC\n- Plot: Precision-Recall Curve, ROC Curve\nUse case:\n- Precision-Recall Curve: aim to care more about false positives than the false negatives\n- Otherwise: ROC Curve\n\n\n\nAccuracy\n\nfrom sklearn.model_selection import cross_val_score\n\ncross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring='accuracy')\n\narray([0.95035, 0.96035, 0.9604 ])\n\n\n\n\n\n\n\n\nThe accuracy scores are pretty good, but it may be due to the class imbalance. Let take a look at a Dummy Model which always classify as the most frequent class\n\n\n\n\n## Dummy classifier\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import cross_val_score\n\ndummy_model = DummyClassifier(random_state=248)\ncross_val_score(dummy_model, X_train, y_train_5, cv=3, scoring='accuracy')\n\narray([0.90965, 0.90965, 0.90965])\n\n\n\n\n\n\n\n\nThe accuracy scores are over 90% because there’s only about 10% of training set are 5 digit\n=&gt; With class imbalance, accuracy score is not a useful metric\n=&gt; We will use other metrics such as Precision, Recall, ROC Curve, AUC\n\n\n\nConfusion Matrix\n\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\n\ny_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\nconfusion_matrix(y_train_5, y_train_pred)\n\narray([[53892,   687],\n       [ 1891,  3530]])\n\n\n\n## Precision and Recall\n\nfrom sklearn.metrics import precision_score, recall_score\n\nprint(f'Precision scores: {precision_score(y_train_5, y_train_pred):.4f}')\nprint(f'Recall scores: {recall_score(y_train_5, y_train_pred):.4f}')\n\nPrecision scores: 0.8371\nRecall scores: 0.6512\n\n\n\n## F1-score\n\nfrom sklearn.metrics import f1_score\n\nprint(f'F1-score: {f1_score(y_train_5, y_train_pred):.4f}')\n\nF1-score: 0.7325\n\n\nPrecision-Recall Trade-off\n\nCompute the scores of all instances in the training using decision_function\nChange the threshold to see the difference\n\n\ny_score = sgd_clf.decision_function([some_digit])\n\nthreshold = [0, 1000, 3000]\nfor thr in threshold:\n    print(f'With threshold of {thr:4d}: predicted value is {y_score&gt;thr}')\n\nWith threshold of    0: predicted value is [ True]\nWith threshold of 1000: predicted value is [ True]\nWith threshold of 3000: predicted value is [False]\n\n\n\n\n\n\n\n\nHow to choose the suitable threshold?\n\nUse Precision-Recall Curve\nprecision_recall_curve: require scores computed from decision_function or probabilities from predict_proba\n\n\n\n\n\n## Precision-Recall Curve\n\n\n### Compute scores by decision_function\n\ny_scores = cross_val_predict(sgd_clf, X_train, y_train_5, method='decision_function')\n\n### Plot Precision-Recall Curve vs Threshold\n\nfrom sklearn.metrics import precision_recall_curve\n\nprecisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n\nplt.plot(thresholds, precisions[:-1], label='Precision', color='darkslateblue')\nplt.plot(thresholds, recalls[:-1], label='Recall', color='crimson')\nplt.grid()\nplt.legend(loc='center left')\nplt.xlim([-100000,40000])\nplt.title('Precision and Recall versus Threshold')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nThe higher Precision, the lower Recall and vice versa\n\n\n\n\n## Plot Precision versus Recall\n\nplt.plot(recalls, precisions)\nplt.title('Precision versus Recall')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.grid()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nDepend on your project, you would trade between precision and recall\n\n\n\n\n## Find Threshold of over 0.90 Precision\n\nidx_90_precision = (precisions &gt;= 0.90).argmax()\nthreshold_90_precision = thresholds[idx_90_precision]\nthreshold_90_precision\n\n3045.9258227053647\n\n\n\ny_train_90_precision = (y_scores &gt; threshold_90_precision)\n\nfrom sklearn.metrics import accuracy_score\nprint(f'Accuracy score: {accuracy_score(y_train_5, y_train_90_precision):.4f}')\nprint(f'Precision score: {precision_score(y_train_5, y_train_90_precision):.4f}')\nprint(f'Recall score: {recall_score(y_train_5, y_train_90_precision):.4f}')\nprint(f'F1 score: {f1_score(y_train_5, y_train_90_precision):.4f}')\n\nAccuracy score: 0.9626\nPrecision score: 0.9002\nRecall score: 0.6587\nF1 score: 0.7608\n\n\n\n## ROC AUC\n\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\nprint(f'AUC score: {roc_auc_score(y_train_5, y_scores):.4f}')       \n\nAUC score: 0.9648\n\n\n\n## ROC Curve\n\nfpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\nidx_threshold_90_precision = (thresholds&lt;=threshold_90_precision).argmax()      # thresholds listed decreasing =&gt; use (&lt;=)\nfpr_90, tpr_90 = fpr[idx_threshold_90_precision], tpr[idx_threshold_90_precision]\n\nplt.plot(fpr, tpr, label='ROC Curve', color='darkslateblue')\nplt.plot([fpr_90], [tpr_90], 'o', label='Threshold for 90% precision', color='crimson')\nplt.title('ROC Curve')\nplt.xlabel('False Positive Rate (Fall-out)')\nplt.ylabel('True Positive Rate (Recall)')\nplt.legend(loc='center right')\nplt.grid()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nAnother trade-off: The higher TPR, the lower FPR and vice versa\n\n\n\n\n\n\n7.3.2 Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\n\nlogistic = LogisticRegression(random_state=29)\n\ny_pred_logis = cross_val_predict(logistic, X_train, y_train_5, cv=3, method='predict_proba')[:,1]\n\n/usr/local/anaconda3/envs/dhuy/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning:\n\nlbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n\n\n\n/usr/local/anaconda3/envs/dhuy/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning:\n\nlbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n\n\n\n/usr/local/anaconda3/envs/dhuy/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning:\n\nlbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n\n\n\n\n## Measure performance\n\nthreshold = 0.5\nf1_logis = f1_score(y_train_5, y_pred_logis&gt;=threshold)\nauc_logis = roc_auc_score(y_train_5, y_pred_logis&gt;=threshold)\n\nprint(f'F1 score Random Forest: {f1_logis:.4f}')\nprint(f'AUC Random Forest: {auc_logis:.4f}')\n\nF1 score Random Forest: 0.8487\nAUC Random Forest: 0.9004\n\n\n\n\n7.3.3 Random Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier(random_state=42)\n\ny_train_pred_rf = cross_val_predict(rf_clf, X_train, y_train_5, cv=3, method='predict_proba')[:,1]\n\n\n## Measure performance\n\nthreshold = 0.5\nf1_rf = f1_score(y_train_5, y_train_pred_rf&gt;=threshold)\nauc_rf = roc_auc_score(y_train_5, y_train_pred_rf&gt;=threshold)\n\nprint(f'F1 score Random Forest: {f1_rf:.4f}')\nprint(f'AUC Random Forest: {auc_rf:.4f}')\n\nF1 score Random Forest: 0.9275\nAUC Random Forest: 0.9358\n\n\n\n## PR Curve\n\nprecisions_rf, recalls_rf, thresholds_rf = precision_recall_curve(y_train_5, y_train_pred_rf)\n\nplt.plot(recalls, precisions, \"-\", label='SGD')\nplt.plot(recalls_rf, precisions_rf, label='Random Forest')\nplt.title('Precision versus Recall')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.legend()\nplt.grid()\n\nplt.show()\n\n\n\n\n\n## ROC Curve\n\nfpr_rf, tpr_rf, thresholds = roc_curve(y_train_5, y_train_pred_rf)\n\nplt.plot(fpr, tpr, label='SGD', color='darkslateblue')\nplt.plot(fpr_rf, tpr_rf, label='Random Forest', color='crimson')\nplt.title('ROC Curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.grid()\n\nplt.show()"
  },
  {
    "objectID": "hands-on-classification.html#measure-performance",
    "href": "hands-on-classification.html#measure-performance",
    "title": "5  Hands-on Classification",
    "section": "5.4 Measure Performance",
    "text": "5.4 Measure Performance\n\n\n\n\n\n\nMetrics: - Accuracy - Confusion matrix: Precision, Recall (TPR), FPR, ROC, ROC AUC - Plot: Precision-Recall Curve, ROC Curve Use case: - Precision-Recall Curve: aim to care more about false positives than the false negatives - Otherwise: ROC Curve\n\n\n\n\n## Accuracy\nfrom sklearn.model_selection import cross_val_score\n\ncross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring='accuracy')\n\narray([0.95035, 0.96035, 0.9604 ])\n\n\n\n\n\n\n\n\nThe accuracy scores are pretty good, but it may be due to the class imbalance. Let take a look at a Dummy Model which always classify as the most frequent class\n\n\n\n\n## Dummy classifier\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import cross_val_score\n\ndummy_model = DummyClassifier(random_state=248)\ncross_val_score(dummy_model, X_train, y_train_5, cv=3, scoring='accuracy')\n\narray([0.90965, 0.90965, 0.90965])\n\n\n\n\n\n\n\n\nThe accuracy scores are over 90% because there’s only about 10% of training set are 5 digit\n=&gt; With class imbalance, accuracy score is not a useful metric\n=&gt; We will use other metrics such as Precision, Recall, ROC Curve, AUC\n\n\n\n\n## Confusion Matrix\n\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\n\ny_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\nconfusion_matrix(y_train_5, y_train_pred)\n\narray([[53892,   687],\n       [ 1891,  3530]])\n\n\n\n## Precision and Recall\n\nfrom sklearn.metrics import precision_score, recall_score\n\nprint(f'Precision scores: {precision_score(y_train_5, y_train_pred):.4f}')\nprint(f'Recall scores: {recall_score(y_train_5, y_train_pred):.4f}')\n\nPrecision scores: 0.8371\nRecall scores: 0.6512\n\n\n\n## F1-score\n\nfrom sklearn.metrics import f1_score\n\nprint(f'F1-score: {f1_score(y_train_5, y_train_pred):.4f}')\n\nF1-score: 0.7325\n\n\n\n\n\n\n\n\nPrecision-Recall Trade-off\n\nCompute the scores of all instances in the training using decision_function\nChange the threshold to see the difference\n\n\n\n\n\ny_score = sgd_clf.decision_function([some_digit])\n\nthreshold = [0, 1000, 3000]\nfor thr in threshold:\n    print(f'With threshold of {thr:4d}: predicted value is {y_score&gt;thr}')\n\nWith threshold of    0: predicted value is [ True]\nWith threshold of 1000: predicted value is [ True]\nWith threshold of 3000: predicted value is [False]\n\n\n\n\n\n\n\n\nThe higher Precision, the lower Recall and vice versa\n\n\n\n\n\n\n\n\n\nHow to choose the suitable threshold?\n\nUse Precision-Recall Curve\nprecision_recall_curve: require scores computed from decision_function or probabilities from predict_proba\n\n\n\n\n\n## Precision-Recall Curve\n\n\n### Compute scores by decision_function\n\ny_scores = cross_val_predict(sgd_clf, X_train, y_train_5, method='decision_function')\n\n### Plot Precision-Recall Curve vs Threshold\n\nfrom sklearn.metrics import precision_recall_curve\n\nprecisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n\nplt.plot(thresholds, precisions[:-1], label='Precision', color='darkslateblue')\nplt.plot(thresholds, recalls[:-1], label='Recall', color='crimson')\nplt.grid()\nplt.legend(loc='center left')\nplt.xlim([-100000,40000])\nplt.title('Precision and Recall versus Threshold')\nplt.show()\n\n\n\n\n\n## Plot Precision versus Recall\n\nplt.plot(recalls, precisions)\nplt.title('Precision versus Recall')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.grid()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nDepend on your project, you would trade between precision and recall\n\n\n\n\n## Find Threshold of over 0.90 Precision\n\nidx_90_precision = (precisions &gt;= 0.90).argmax()\nthreshold_90_precision = thresholds[idx_90_precision]\nthreshold_90_precision\n\n3045.9258227053647\n\n\n\ny_train_90_precision = (y_scores &gt; threshold_90_precision)\n\nfrom sklearn.metrics import accuracy_score\nprint(f'Accuracy score: {accuracy_score(y_train_5, y_train_90_precision):.4f}')\nprint(f'Precision score: {precision_score(y_train_5, y_train_90_precision):.4f}')\nprint(f'Recall score: {recall_score(y_train_5, y_train_90_precision):.4f}')\nprint(f'F1 score: {f1_score(y_train_5, y_train_90_precision):.4f}')\n\nAccuracy score: 0.9626\nPrecision score: 0.9002\nRecall score: 0.6587\n\n\nF1 score: 0.7608\n\n\n\n## ROC AUC\n\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\nprint(f'AUC score: {roc_auc_score(y_train_5, y_scores):.4f}')       \n\nAUC score: 0.9648\n\n\n\n## ROC Curve\n\nfpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\nidx_threshold_90_precision = (thresholds&lt;=threshold_90_precision).argmax()      # thresholds listed decreasing =&gt; use (&lt;=)\nfpr_90, tpr_90 = fpr[idx_threshold_90_precision], tpr[idx_threshold_90_precision]\n\nplt.plot(fpr, tpr, label='ROC Curve', color='darkslateblue')\nplt.plot([fpr_90], [tpr_90], 'o', label='Threshold for 90% precision', color='crimson')\nplt.title('ROC Curve')\nplt.xlabel('False Positive Rate (Fall-out)')\nplt.ylabel('True Positive Rate (Recall)')\nplt.legend(loc='center right')\nplt.grid()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nAnother trade-off: The higher TPR, the lower FPR and vice versa\n\n\n\n\n5.4.1 Random Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier(random_state=42)\n\ny_train_pred_rf = cross_val_predict(rf_clf, X_train, y_train_5, cv=3, method='predict_proba')[:,1]\n\n\n## Measure performance\n\nthreshold = 0.5\nf1_rf = f1_score(y_train_5, y_train_pred_rf&gt;=threshold)\nauc_rf = roc_auc_score(y_train_5, y_train_pred_rf&gt;=threshold)\n\nprint(f'F1 score Random Forest: {f1_rf:.4f}')\nprint(f'AUC Random Forest: {auc_rf:.4f}')\n\nF1 score Random Forest: 0.9275\nAUC Random Forest: 0.9358\n\n\n\n## PR Curve\n\nprecisions_rf, recalls_rf, thresholds_rf = precision_recall_curve(y_train_5, y_train_pred_rf)\n\nplt.plot(recalls, precisions, \"-\", label='SGD')\nplt.plot(recalls_rf, precisions_rf, label='Random Forest')\nplt.title('Precision versus Recall')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.legend()\nplt.grid()\n\nplt.show()\n\n\n\n\n\n## ROC Curve\n\nfpr_rf, tpr_rf, thresholds = roc_curve(y_train_5, y_train_pred_rf)\n\nplt.plot(fpr, tpr, label='SGD', color='darkslateblue')\nplt.plot(fpr_rf, tpr_rf, label='Random Forest', color='crimson')\nplt.title('ROC Curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.grid()\n\nplt.show()"
  },
  {
    "objectID": "hands-on-classification.html#multiclass-classification",
    "href": "hands-on-classification.html#multiclass-classification",
    "title": "7  Hands-on Classification",
    "section": "7.4 Multiclass Classification",
    "text": "7.4 Multiclass Classification\n\nLogisticRegression, RandomForestClassifier, GaussianNB: natively handle Multiclass Classification\nSGDClassifier and SVC: strictly binary classifiers\n\novo: one versus one strategy, preferred with scale poorly algorithms (i.e. SVC)\novr: one versus rest strategy, preferred for almost algorithms\n\n\n\n7.4.1 SVC\n\n7.4.1.1 Default: ovo strategy\n\nfrom sklearn.svm import SVC\n\nsvc_clf = SVC(random_state=42)\nsvc_clf.fit(X_train[:1000], y_train[:1000])\nsvc_clf.predict([some_digit])\n\narray(['5'], dtype=object)\n\n\n\n## Scores from decision_function\n\nsome_digit_svc = svc_clf.decision_function([some_digit])\nsome_digit_svc.round(4)\n\narray([[ 1.7583,  2.7496,  6.1381,  8.2854, -0.2873,  9.3012,  0.7423,\n         3.7926,  7.2085,  4.8576]])\n\n\n\n## Class of highest score\n\nidx_svc = some_digit_svc.argmax()\nidx_svc\n\n5\n\n\n\n## Classes of prediction\nsvc_clf.classes_[idx_svc]\n\n'5'\n\n\n\n\n7.4.1.2 Force: ovr strategy\n\n## Train model\n\nfrom sklearn.multiclass import OneVsRestClassifier\n\novr_svc_clf = OneVsRestClassifier(SVC(random_state=42))\novr_svc_clf.fit(X[:1000], y_train[:1000])\novr_svc_clf.predict([some_digit])\n\narray(['5'], dtype='&lt;U1')\n\n\n\n## Compute scores\n\nsome_digit_ovr_svc = ovr_svc_clf.decision_function([some_digit])\nsome_digit_ovr_svc.round(4)\n\narray([[-1.3439, -1.5195, -1.221 , -0.9294, -2.0057,  0.6077, -1.6226,\n        -0.9998, -1.2764, -1.7031]])\n\n\n\n## Class of hishest score\n\nsome_digit_ovr_svc.argmax()\n\n5\n\n\n\n## Extract classes\n\novr_svc_clf.classes_\n\narray(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype='&lt;U1')\n\n\n\n\n\n7.4.2 SGD\n\n## Train model\n\nfrom sklearn.linear_model import SGDClassifier\n\nsgd_clf = SGDClassifier(random_state=42)\nsgd_clf.fit(X_train, y_train)\nsgd_clf.predict([some_digit])\n\narray(['3'], dtype='&lt;U1')\n\n\nThat’s incorrect. As we can see,The Classifier is not very confident about its prediction.\n\n## Compute scores\n\nsgd_clf.decision_function([some_digit])\n\narray([[-31893.03095419, -34419.69069632,  -9530.63950739,\n          1823.73154031, -22320.14822878,  -1385.80478895,\n        -26188.91070951, -16147.51323997,  -4604.35491274,\n        -12050.767298  ]])\n\n\nWe will use cross validation to evaluate our model\n\ncross_val_score(sgd_clf, X_train, y_train, cv=3, scoring='accuracy')\n\narray([0.87365, 0.85835, 0.8689 ])\n\n\nWe can scale the data to get better result\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train.astype('float64'))\ncross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring='accuracy')\n\narray([0.8983, 0.891 , 0.9018])\n\n\nLet’s look at the confusion matrix of our prediction\n\n## Predict using cross_val_predict\n\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\ny_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\n\nConfusion matrix with (right) and without (left) normalization.\n\nfig,ax = plt.subplots(1,2,figsize=(9, 4))\n\nConfusionMatrixDisplay.from_predictions(y_train, y_train_pred, ax=ax[0])\nax[0].set_title(\"Confusion matrix\")\nConfusionMatrixDisplay.from_predictions(y_train, y_train_pred, ax=ax[1], normalize='true', values_format='.0%')\nax[1].set_title(\"CM normalized by row\")\n\nplt.show()\n\n\n\n\nIn row #5 and column #8 on the left plot, it’s means 10% of true 5s is misclassified as 8s. Kinda hard to see the errors made by model. Therefore, we will put 0 weight on correct prediction (error plot).\nConfustion matrix with error normalized by row (left) and by column (right) (normalize=[‘true’,‘pred’])\n\nfig,ax = plt.subplots(1,2,figsize=(9, 4))\n\nsample_weight = (y_train != y_train_pred)\n\nConfusionMatrixDisplay.from_predictions(y_train, y_train_pred, ax=ax[0],sample_weight=sample_weight, normalize='true', values_format='.0%')\nax[0].set_title(\"Confusion matrix\")\nConfusionMatrixDisplay.from_predictions(y_train, y_train_pred, ax=ax[1],sample_weight=sample_weight, normalize='pred', values_format='.0%')\nax[1].set_title(\"CM normalized by row\")\n\nplt.show()\n\n\n\n\nIn row #5 and column #8 on the left plot, it’s means 55% of errors made on true 5s is misclassified as 8s.\nIn row #5 and column #8 on the right plot, it’s means 19% of misclassified 8s are actually 5s.\nAnalyzing the made errors can help us gain insights and why the classifier failing"
  },
  {
    "objectID": "regression.html#datasets",
    "href": "regression.html#datasets",
    "title": "3  Regression",
    "section": "3.1 Datasets",
    "text": "3.1 Datasets\nPopular open data repositories\n\nhttps://openml.org/\n\nhttps://Kaggle.com\nhttps://PapersWithCode.com\n\nUC Irvine Machine Learning Repository — Amazon’s AWS datasets\nTensorFlow datasets\n\nMeta portals (they list open data repositories)\n\nhttps://DataPortals.org\n\nhttps://OpenDataMonitor.eu\n\nOther pages listing many popular open data repositories\n\nWikipedia’s list of machine learning datasets\nhttps://Quora.com\nThe datasets subreddit\n\nIn this chapter we’ll use the California Housing Prices dataset from the StatLib repository. This dataset is based on data from the 1990 California census. It is not exactly recent but it has many qualities for learning."
  },
  {
    "objectID": "regression.html#look-at-the-big-picture",
    "href": "regression.html#look-at-the-big-picture",
    "title": "3  Regression",
    "section": "3.2 Look at the big picture",
    "text": "3.2 Look at the big picture\nQuestions\n\nBusiness objective? Current solution (if any)?\nHow to use and benefit from the model?\nData Pipelines?\nDetermine kind of model\nSelect preformance measures\nCheck the Assumptions\n\nAnswers\n\nPredict median housing price in any district. The results are used for another ML system for investment analysis. The current solution is to gather up-to-date information about a district, or to estimate manually using complex rules if no information.\nThe current solution is costly and time-consuming, and it was often off by more than 30%. Therefore, a ML model could be more useful\nData pipelines: A sequence of data processing components. Pipelines are very common in machine learning systems, since there is a lot of data to manipulate and many data transformations to apply.\nThis is a regression task (labeled data), batch learning (data is quite small and not change too much) and model-based learning\nMetrics for regression:\n\n\n\n\n\n\n\nExpand to learn more about metrics\n\n\n\n\n\nBoth the RMSE and the MAE are ways to measure the distance between two vectors: the vector of predictions and the vector of target values. Various distance measures, or norms, are possible:\n• Computing the root of a sum of squares (RMSE) corresponds to the Euclidean norm: this is the notion of distance we are all familiar with. It is also called the l2 norm, noted |·|₂ (or just |·|).\n• Computing the sum of absolutes (MAE) corresponds to the l1 norm, noted |·|₁. This is sometimes called the Manhattan norm because it measures the distance between two points in a city if you can only travel along orthogonal city blocks.\n• More generally, the lk norm of a vector v containing n elements is defined as ∥v∥k = (|v₁|ᵏ + |v₂|ᵏ + … + |vₙ|ᵏ)¹/ᵏ. l0 gives the number of nonzero elements in the vector, and l∞ gives the maximum absolute value in the vector.\nThe higher the norm index, the more it focuses on large values and neglects small ones. This is why the RMSE is more sensitive to outliers than the MAE. But when outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs very well and is generally preferred. \n\n\n\nRMSE (root mean squared error): l2 norm\n\\[\nRMSE(X, y)  = \\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}(y_{hat}^{(i)} - y^{(i)})^2}\n\\]\nMAE (mean squared error): l1 norm\n\\[\nMAE(X, y)  = \\frac{1}{m}\\sum_{i=1}^{m}|y_{hat}^{(i)} - y^{(i)}|\n\\]\n\nCheck with the team in charge of the downstream system that use out output whether it is suitable or not (e.g. it is terrible if after several months building model you realize that they need ordinal output not numerical one)"
  },
  {
    "objectID": "regression.html#get-the-data",
    "href": "regression.html#get-the-data",
    "title": "3  Regression",
    "section": "3.3 Get the Data",
    "text": "3.3 Get the Data\n\n3.3.1 Download Data\n\n## Load data\n\nfrom pathlib import Path\nimport pandas as pd\nimport tarfile\nimport urllib.request\n\ndef load_data(url):\n    path = Path(\"datasets/housing.tgz\")\n    if not path.is_file():\n        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n        urllib.request.urlretrieve(url, path)\n        tarfile.open(path).extractall(path='datasets')\n    return pd.read_csv('datasets/housing/housing.csv')\n\nurl = 'https://github.com/ageron/data/raw/main/housing.tgz'\ndata = load_data(url)\n\n\n\n3.3.2 Quick Look to Data Structure: head(), info(), describe(), value_counts(), histplot()\n\n## Quick look\n\npd.set_option('display.max_columns', None)      # display all columns\ndata.head()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n0\n-122.23\n37.88\n41.0\n880.0\n129.0\n322.0\n126.0\n8.3252\n452600.0\nNEAR BAY\n\n\n1\n-122.22\n37.86\n21.0\n7099.0\n1106.0\n2401.0\n1138.0\n8.3014\n358500.0\nNEAR BAY\n\n\n2\n-122.24\n37.85\n52.0\n1467.0\n190.0\n496.0\n177.0\n7.2574\n352100.0\nNEAR BAY\n\n\n3\n-122.25\n37.85\n52.0\n1274.0\n235.0\n558.0\n219.0\n5.6431\n341300.0\nNEAR BAY\n\n\n4\n-122.25\n37.85\n52.0\n1627.0\n280.0\n565.0\n259.0\n3.8462\n342200.0\nNEAR BAY\n\n\n\n\n\n\n\nThere are total 10 features, each row represents a district observation\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 10 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   longitude           20640 non-null  float64\n 1   latitude            20640 non-null  float64\n 2   housing_median_age  20640 non-null  float64\n 3   total_rooms         20640 non-null  float64\n 4   total_bedrooms      20433 non-null  float64\n 5   population          20640 non-null  float64\n 6   households          20640 non-null  float64\n 7   median_income       20640 non-null  float64\n 8   median_house_value  20640 non-null  float64\n 9   ocean_proximity     20640 non-null  object \ndtypes: float64(9), object(1)\nmemory usage: 1.6+ MB\n\n\nData with 10 columns and 20640 rows 9 numerical features, 1 categorical feature ‘total_bedrooms’ has only 20433 non-null values\n\ndata.describe(include='all')        # describe all type of data\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\ncount\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20433.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640\n\n\nunique\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n5\n\n\ntop\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n&lt;1H OCEAN\n\n\nfreq\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n9136\n\n\nmean\n-119.569704\n35.631861\n28.639486\n2635.763081\n537.870553\n1425.476744\n499.539680\n3.870671\n206855.816909\nNaN\n\n\nstd\n2.003532\n2.135952\n12.585558\n2181.615252\n421.385070\n1132.462122\n382.329753\n1.899822\n115395.615874\nNaN\n\n\nmin\n-124.350000\n32.540000\n1.000000\n2.000000\n1.000000\n3.000000\n1.000000\n0.499900\n14999.000000\nNaN\n\n\n25%\n-121.800000\n33.930000\n18.000000\n1447.750000\n296.000000\n787.000000\n280.000000\n2.563400\n119600.000000\nNaN\n\n\n50%\n-118.490000\n34.260000\n29.000000\n2127.000000\n435.000000\n1166.000000\n409.000000\n3.534800\n179700.000000\nNaN\n\n\n75%\n-118.010000\n37.710000\n37.000000\n3148.000000\n647.000000\n1725.000000\n605.000000\n4.743250\n264725.000000\nNaN\n\n\nmax\n-114.310000\n41.950000\n52.000000\n39320.000000\n6445.000000\n35682.000000\n6082.000000\n15.000100\n500001.000000\nNaN\n\n\n\n\n\n\n\n\nfor ft in data.select_dtypes('object'):     # choose 'object' features only\n    print(data[ft].value_counts())\n    print(f'Number of classes: {data[ft].nunique()}')\n\nocean_proximity\n&lt;1H OCEAN     9136\nINLAND        6551\nNEAR OCEAN    2658\nNEAR BAY      2290\nISLAND           5\nName: count, dtype: int64\nNumber of classes: 5\n\n\nQuite imbalanced classes\n\n## Plot histogram of numerical features\n\nimport matplotlib.pyplot as plt\n\ndata.hist(bins=50, figsize=(8,8))\nplt.show()\n\n\n\n\nhousing_median_age: capped at range (1,52)\nmedian_income: scaled and capped at range ~ (0.5, 15)\nmedian_house_value: capped at top range of 500,000\nThese features are skewed and have very different scales =&gt; Transforming and Feature Scaling\n\n\n3.3.3 Create train, val and test set: train_test_split()\nrandom sampling: data must be large enough, otherwise there is a risk of sampling bias stratified sampling: based on some very important features, help avoid bias and ensure train set and test set are representative to full dataset\nHere we assump that median_income is very important feature to predict household value\n\n## Create new feature (income group)\nimport numpy as np\n\ndata['income_grp'] = pd.cut(data['median_income'], bins=[0,1.5,3,4.5,6,np.inf], labels=[1,2,3,4,5])\n\ndata.income_grp.value_counts().sort_index().plot.bar(rot=0, grid=True)\nplt.title('Frequency of income group')\nplt.xlabel('Income group')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n## Split data into train, val, test set\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(data.drop('median_house_value', axis=1), data.median_house_value, stratify=data['income_grp'], test_size=0.2, random_state=24)\n\n\n## Drop the 'income_grp' after using\n\nfor df in [x_train, x_test]:\n    df.drop('income_grp', axis=1, inplace=True)"
  },
  {
    "objectID": "regression.html#explore-and-visualize-the-data-to-gain-insights",
    "href": "regression.html#explore-and-visualize-the-data-to-gain-insights",
    "title": "3  Regression",
    "section": "3.4 Explore and visualize the data to gain insights",
    "text": "3.4 Explore and visualize the data to gain insights\nIf the data is very large, we will sample an exploration set to manipulate faster and easier. On the other hand, just work directly on full set if the data is quite small\n\ndf_vis = data.copy()\n\n\n3.4.1 Visualize\nPlot\n\n\n\n\n\nFigure 3.1: The geographical plot of data\n\n\n\n\nLarge point size: larger population Blue -&gt; red: higher house price\nCorrelation\nThere are 2 ways to perform: heatmap and pairgrid map\n\n\n\n\n&lt;Axes: &gt;\n(a) Correlation Plot\n\n\n\n\n\n\n(b)\n\n\n\nFigure 3.2: ?(caption)\n\n\n\n\n\n\n\nFigure 3.3: PairGrid Plot of numerical features\n\n\n\n\nTry pd.plotting.scatter_matrix\nLook closely too the relation between ‘median_house_value’ and ‘median_income’, we see there is a strong positive correlation, but there are some clearly horizontal line at 500,000; 450,000; 350,000 and roughly 280,000. We should remove these instances to prevent the algorithms learning these patterns.\n\n\n\n\n\nFigure 3.4: Median income versus median house value\n\n\n\n\n\n\n3.4.2 Attributes combination\nUseful when we want to find better features to predict\n\ndf_vis['room_per_house'] = df_vis['total_rooms']/df_vis['households']\ndf_vis['bedroom_ratio'] = df_vis['total_bedrooms']/df_vis['total_rooms']\ndf_vis['people_per_house'] = df_vis['population']/df_vis['households']\n\ncorr_matrix = df_vis.corr(numeric_only=True)\ncorr_matrix['median_house_value'].sort_values(ascending=False)\n\nmedian_house_value    1.000000\nmedian_income         0.688075\nroom_per_house        0.151948\ntotal_rooms           0.134153\nhousing_median_age    0.105623\nhouseholds            0.065843\ntotal_bedrooms        0.049686\npeople_per_house     -0.023737\npopulation           -0.024650\nlongitude            -0.045967\nlatitude             -0.144160\nbedroom_ratio        -0.255880\nName: median_house_value, dtype: float64"
  },
  {
    "objectID": "classification.html#binary-classification",
    "href": "classification.html#binary-classification",
    "title": "6  Classification",
    "section": "6.1 Binary Classification",
    "text": "6.1 Binary Classification\n\n6.1.1 Logistic Regression\nPretty much same as method using in Linear regression, Logistic regression use sigmoid function to the same equation using in Linear regression to turn the output into probabilities (range (0,1)).\n\\[\n\\begin{gather}\nLinear Regression: y = θ^{T}X\\\\\nLogistic Regression: p = sigmoid(θ^{T}X)\\\\\nsigmoid(t) = \\frac{1}{1-e^{-t}}\\\\\nlogit(p) = log\\left(\\frac{p}{1-p}\\right)= t\n\\end{gather}\n\\]\nCost function for 1 instance \\[\\begin{equation}\n\\begin{split}\nJ(θ) & = -log(p)\\quad \\quad \\quad if\\;\\;\\; y=1\\\\\n& = -log(1-p) \\quad \\; if\\;\\;\\;  y=0\n\\end{split}\n\\end{equation}\n\\]\n\n\n\n\n\n\nCost function penalizes the model when it estimates the loew probability for the real target class\n\n-log(p) -&gt; inf when p -&gt; 0 for y = 1 instance\n\n-log(1-p) -&gt; inf when p -&gt; 1 for y = 0 instance\n\n\n\n\nThere is no closed-form equation to compute θ. We will use gradient descent to find the best weights.\nCost function for whole training set (log loss): convex function \\[J(θ) = \\frac{−1}{m} \\sum [y_ilog(p_i) + (1−y_i)log(1−p_i)]\\]\nGradient \\[∇ = \\frac{1}{m}X^{T}[sigmoid(Xθ) - y]\\]\n\n\n\n\n\n\n\nLog loss assumption: the instances follow a Gaussian distribution around the mean of their class\n\nMSE assumption: data is purely linear\n\nThe more wrong assumption, the more biased the model\n\n\n\n\nDecision boudaries:\n\n\n\n\n\n\nRegularization in Logistic Regression: l1, l2 using C parameter (inverse of alpha)\n\n\n\nImplement Linear regression using sklearn: Logistic regression\n\n\n6.1.2 Softmax Regression (Multinomial Logistic Regression)\nThe Logistic regression can be generalized to support multipleclass classification directly. It is called softmax regression.\nThe strategy when given an instance x is described like this:\n\nCompute score for each class using softmax score function\nCompute probability for each class using softmax function to each score\nChoose the class with the highest probability. The instance x is belong to this class\n\nSoftmax score for class k \\[s_k(x) = (θ^{(k)})^{T}X\\]\n\n\n\n\n\n\nEach class has own parameter vecto θ(k). Parameter matrix Θ contains all parameter vectors of all classes\n\n\n\nSoftmax function for class k: \\[p_k = σ(s(x))_k = \\frac{exp(s_k(x))}{\\sum\\limits exp(s_j(x))}\\]\n\nK is the number of classes\n\ns(x) is a vector containing the scores of each class for the instance x\n\nσ(s(x))k is the estimated probability that the instance x belongs to class k, given the scores of each class for that instance\n\n\nChoose the class with the highest probability \\[y= argmax\\; σ(s(x))_k= argmax\\;s_k(x) = argmax\\; (θ^{k})^{T}X\\]\nJust like Logistic regression, softmax regression has the cost function called Cross entropy\nCross entropy cost function\n\\[\nJ(Θ) = −\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}y_{k}^{(i)}log(p_{k}^{(i)})\n\\]\n\n\n\n\n\n\n\nyk(i): the label of the target class\n\nWhen k=2, softmax regression is equivalent to logistic regression\n\n\n\n\nCross entropy gradient vector for class k\n\\[\n∇_{θ}k = \\frac{1}{m}\\sum(p_{k}^{(i)} − y_{k}^{i})x^{(i)}\n\\]\nImplement Linear regression using sklearn\n\nfrom sklearn.datasets import load_iris\n\niris = load_iris(as_frame=True)\niris.target_names\n\narray(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10')\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nX = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\ny = iris[\"target\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nsoftmax = LogisticRegression(max_iter=1000, C=30)\nsoftmax.fit(X_train, y_train)\nprint(softmax.predict([X_test[0]]))\nprint(softmax.predict_proba([X_test[0]]).round(4))\n\n[1]\n[[0.     0.9827 0.0173]]"
  },
  {
    "objectID": "classification.html#multiclass-classification",
    "href": "classification.html#multiclass-classification",
    "title": "6  Classification",
    "section": "6.2 Multiclass Classification",
    "text": "6.2 Multiclass Classification"
  },
  {
    "objectID": "classification.html#multilabel-classification",
    "href": "classification.html#multilabel-classification",
    "title": "6  Classification",
    "section": "6.3 Multilabel Classification",
    "text": "6.3 Multilabel Classification"
  },
  {
    "objectID": "classification.html#multioutput-classification",
    "href": "classification.html#multioutput-classification",
    "title": "6  Classification",
    "section": "6.4 Multioutput Classification",
    "text": "6.4 Multioutput Classification"
  },
  {
    "objectID": "hands-on-classification.html#multilabel-classification",
    "href": "hands-on-classification.html#multilabel-classification",
    "title": "7  Hands-on Classification",
    "section": "7.5 Multilabel Classification",
    "text": "7.5 Multilabel Classification\nOutput is multilabel for each instances. For example, we will classify whether the digit is large (&gt;7) and is odd\n\n7.5.1 K Nearest Neighbors\n\n## Train model\n\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsClassifier\n\ny_train_large = (y_train &gt;= '7')\ny_train_odd = (y_train.astype('int8') % 2 == 1)\ny_train_multilabel = np.c_[y_train_large, y_train_odd]\n\nknn = KNeighborsClassifier()\nknn.fit(X_train_scaled, y_train_multilabel)\nknn.predict([some_digit])\n\narray([[False,  True]])\n\n\nCompute average F1 score across all labels (equally important)\n\n## Evaluate model\n\ny_train_pred_knn = cross_val_predict(knn, X_train_scaled, y_train, cv=3)\nf1_score(y_train, y_train_pred_knn, average='macro')\n\n0.9396793112547043\n\n\nAnother approach is to give each label a weight equal to its number of instances\n\nf1_score(y_train, y_train_pred_knn, average='weighted')\n\n0.940171964265114\n\n\n\n\n7.5.2 SVC\n\nSVC does not natively support multilabel classification. Therefore, there are 2 strategies:\n\n\nTrain one model per label. It turns out that it’s hard to capture the dependencies between labels\nTrain models sequentially (ChainClassifier): using input features and all predictions of previous models in the chain\n\n\nfrom sklearn.multioutput import ClassifierChain\n\nchain_clf = ClassifierChain(SVC(), cv=3, random_state=42)\nchain_clf.fit(X_train_scaled[:2000], y_train_multilabel[:2000])\nchain_clf.predict([some_digit])\n\narray([[0., 1.]])"
  },
  {
    "objectID": "regression.html#linear-regression",
    "href": "regression.html#linear-regression",
    "title": "3  Regression",
    "section": "3.1 Linear Regression",
    "text": "3.1 Linear Regression\nSuppose that we have a data set of demographic and healthcare cost for each individual in a city, and we want to predict the total healthcare cost based on age.\nIf we use linear regression method for this task, we will assump that the relationship between these features is linear and try to fit a line so that is closest to the data. The plot looks like this.\n\n\nCode\n## Simple linear regression plot\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(29)\nm = 100      # number of instances\nx = np.random.randint(18,80,m)\ny = np.random.randint(-200,200,m) + 20*x\n\nplt.plot(x,y,'b.', label='True values')\nplt.plot(x, 20*x,'-',color='r', label='Linear regression')\nplt.xlabel('Age')\nplt.ylabel('Healthcare cost')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nIf you have another feature using to predict (e.g. weight), the plot will look like this. For ≥3 features, it’s called ‘Multiple linear regression’ and we will fit a hyperplane instead.\n\n\nCode\n## Multiple linear regression plot\n\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\nfrom mpl_toolkits.mplot3d import Axes3D\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nz = np.random.randint(20,30,m)\ny = np.random.randint(-200,200,m) + 20*x +30*z\n\nX_train = np.c_[x,z]\nlm = LinearRegression()\nlm.fit(X_train, y)\n\n\n# Setting up the 3D plot\nfig = plt.figure(figsize=(10, 7))\nax = fig.add_subplot(111, projection='3d')\n\n# Scatter plot of actual data\nax.scatter(x, z, y, color='blue', marker='o', alpha=0.5, label='True values')\n\n# Creating a meshgrid for the plane\nx_surf = np.linspace(x.min(), x.max(), 100)\nz_surf = np.linspace(z.min(), z.max(), 100)\nx_surf, z_surf = np.meshgrid(x_surf, z_surf)\n\n# Predicting the values from the meshed grid\nvals = pd.DataFrame({'Age': x_surf.ravel(), 'Weight': z_surf.ravel()})\ny_pred = lm.predict(vals)\nax.plot_surface(x_surf, z_surf, y_pred.reshape(x_surf.shape), color='r', alpha=0.3, label='Hyperplane')\n\n# Labeling the axes\nax.set_xlabel('Age')\nax.set_ylabel('Weight')\nax.set_zlabel('Healthcare cost')\n#ax.legend()\n\nplt.show()\n\n\n\n\n\nThe formula of the line (n=1)/hyperplane (n&gt;1) is: \\[\n\\hat{y} = θ_o +θ_1x_1 +θ_2x_2+...+θ_nx_n\n\\]\n\nŷ: predicted value\nn: number of features\nx_i: the i_th feature value\nθ_i: the i_th parameter value (θ_0: intercept; θ_1 - θ_n: weight of parameters)\n\nFor linear algebra, this can be written much more concisely using a vectorized form like this: \\[\\hat{y} = θ.X\\]\n\nθ: vecto of weights (of parameters)\nX: matrix of features\n\nSo how can we find the best fitted line, the left or the right one?\n\n\nCode\nnp.random.seed(29)\nm = 100      # number of instances\nx = np.random.randint(18,80,m)\ny = np.random.randint(-200,200,m) + 20*x\n\nplt.subplot(1,2,1)\nplt.plot(x,y,'b.', label='True values')\nplt.plot(x, 20*x,'-',color='r')\nplt.xlabel('Age')\nplt.ylabel('Healthcare cost')\n\nplt.subplot(1,2,2)\nplt.plot(x,y,'b.', label='True values')\nplt.plot(x, 10+18*(x+10),'-',color='r')\nplt.xlabel('Age')\n\nplt.show()\n\n\n\n\n\nIt turns out that we have 2 common strategy:\n- Linear algebra: using normal equation\n- Optimization: using gradient descent\n\n3.1.1 Normal Equation\n\\[θ = (X^{T}X)^{-1}X^{T}y\\]\n\nθ: vecto of weights (of parameters)\nX: matrix of features\ny: vecto of target value\n\nThat’s all we need to compute the best weights (coefficients).\nBut in reality, not all cases matrix is invertible, so LinearRegression in sklearn compute pseudoinverse (X+) instead, using a standard matrix factorization technique called singular value decomposition (SVD) that decompose X into (UΣV^T): \\[\n\\begin{gather}\nθ = X^{+}Y\\\\\n   X = UΣV^{T}\\\\\nX^{+} = VΣ^{+}U^{T}\n\\end{gather}\n\\]\nImplement Linear regression using sklearn\n\nfrom sklearn.linear_model import LinearRegression\n\nnp.random.seed(29)\nx = np.random.randint(18,80,m)\ny = np.random.randint(-200,200,m) + 20*x\nX_train = x.reshape(-1,1)\n\nlinear = LinearRegression()\nlinear.fit(X_train,y)\ny_pred = linear.predict(X_train)\nprint(f'Equation: {linear.intercept_:.2f} + {linear.coef_[0]:.2f}*x')\n\nplt.plot(x, y, 'b.', label='True values')\nplt.plot(x, y_pred,'-',color='r', label='Linear regression')\nplt.xlabel('Age')\nplt.ylabel('Healthcare cost')\nplt.legend()\n\nplt.show()\n\nEquation: 1.41 + 20.21*x\n\n\n\n\n\nBoth the Normal equation and SVD approach scale well with the number of instances, but scale very badly with number of features. Therefore, we will look at another approach which is better suited for cases where there are a large number of features or too many training instances to fit in memory.\n\n\n3.1.2 Gradient Descent\n\n3.1.2.1 How does GD work?\nIn fact, the computer really like the term ‘optimization’, which means we will take the result roughly equal to the correct one with the acceptable error. Gradient descent (GD) is that kind of method.\nGenerally, GD tweaks the weights iteratively in order to minimize a cost function. Steps to do Gradient Descent:\n\nTake Gradient (derivative) of Loss Function\nRandom initialization (take random weights)\nLoop step 3-5 until converge:\n\nCompute gradient\nCompute step size: StepSize = Gradient * Learning_rate\nCompute new weights: New = Old - StepSize\n\n\n[i] Run single epoch:\n\npartial_fit(): ignore (max_iter, tol) do not reset epoch counter\nfit(warm_start = True)\n\n\n\n\n\n\n\n\n\nLoss function: also called cost function, is the amount that we have to pay if we use the specific set of weights. Of course we want to minimize it cause everyone want to pay less but gain more, right 😆\nLearning rate: the pace of changing the weights in respond to the estimated loss\n\n\nToo small: take a long time to converge\nToo high: diverge\n\nNumber of epochs: times that we update our weights\n\nToo low: can’t get optimal solution\nToo high: waste time (parameters do not change much)\nSolution: set large epoch and a tolerance to interrupt when grandient &lt; tolerance\n\n\n\n\n\n\n\n\n\n\n\n\nSuitable learning rate\n\n\n\n\n\n\n\nToo low learning rate\n\n\n\n\n\n\n\nToo high learning rate\n\n\n\n\nFigure 3.1: Learning rate strategy\n\n\n\n\n3.1.2.2 GD pitfalls\n\nLocal minimum: If we initialize weights from the left, we will reach local minimum instead of global minimum\nPlateau: if we initialize weights from the right, the gradient will change slowly and adding new instances to the training set doesn’t make the average error much better or worse. If early stopping, we will never reach the global minimum\n\n\n\n\nFigure 3.2: Gradient descent pitfalls\n\n\nFortunately, the cost function of linear regression is a convex function, which means it has no local minimum/ just one global minimum, and its slope never changes abruptly\n\\[\nMSE = \\frac{1}{2m}\\sum_{i=1}^{m}{(θ^{T}x_{i}-y_{i})}^2\n\\]\n\nAnother pitfall of GD: features have very different scales. Therefore, when using gradient descent, you should ensure that all features have a similar scale (e.g., using Scikit-Learn’s StandardScaler class), or else it will take much longer to converge.\n\n\n\n\nFigure 3.3: Gradient descent with (left) and without (right) feature scaling\n\n\n\n\n3.1.2.3 Implement gradient descent using sklearn\n\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\nsgd = make_pipeline(StandardScaler(),\nSGDRegressor(max_iter=1000, tol=1e-3))\nsgd.fit(X_train, y)\nprint('Equation: %.2f + %.2f*x' % (sgd['sgdregressor'].intercept_[0], sgd['sgdregressor'].coef_[0]))\n\ny_pred = sgd.predict(X_train)\n\nplt.plot(x, y, 'b.', label='True values')\nplt.plot(x, y_pred,'-',color='r', label='Stochastic gradient descent regressor')\nplt.xlabel('Age')\nplt.ylabel('Healthcare cost')\nplt.legend()\n\nplt.show()\n\nEquation: 961.04 + 372.06*x\n\n\n\n\n\n\n\n\n\n\n\nThe intercept and coefficient in this equation are different from Section 3.1.1 because they implement on scaled X_train\n\n\n\nLearn more about Gradient descent."
  },
  {
    "objectID": "regression.html#polynomial-regression",
    "href": "regression.html#polynomial-regression",
    "title": "3  Regression",
    "section": "3.2 Polynomial Regression",
    "text": "3.2 Polynomial Regression\nIf the data is more complex (non linear), what do we do? In that case, we just create new features by adding powers to existed features, and use them to fit to our linear model. This technique is called polynomial regression.\nFor example, we will use sklearn’s PolynomialFeatures to transform our data to higher degree, and then fit it to LinearRegression.\n\n\nCode\nnp.random.seed(29)\nm = 100\nX = 10*np.random.rand(m, 1)-5\ny = 10+ 1.5*X**2 + X + np.random.randn(m,1)\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\npipe = make_pipeline(PolynomialFeatures(degree=2, include_bias=False), LinearRegression())\n\npipe.fit(X, y)\ny_pred = pipe.predict(X)\n\nX_new = np.linspace(-5, 5, 100).reshape(-1, 1)\ny_pred_new = pipe.predict(X_new)\n\nplt.plot(X, y, 'b.', label='True values')\nplt.plot(X_new, y_pred_new,'-',color='r', label='Stochastic gradient descent regressor')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nIf we have n features, d degree: PolynomialFeatures transform into (n+d)! / (n!d!) features"
  },
  {
    "objectID": "regression.html#learning-curve",
    "href": "regression.html#learning-curve",
    "title": "3  Regression",
    "section": "3.3 Learning Curve",
    "text": "3.3 Learning Curve\n\nHow complex polynomial should be?\n\n\nUnderfitting (1 dgree): too simple model, can’t capture the pattern of data\nOverfitting (300 degrees): too complex model, tend to remember data\n\n\n\n\nFigure 3.4: Different polynomial degree\n\n\n\nHow can tell overfitting or underfitting? There are 2 strategies\n\nCross-validation\n- Overfitting: model perform well on train set, generate poorly on validation set\n- Underfitting: perform poorly on both train and validation sets\nLearning Curve - Plot training errors and validation errors over training set sizes (using cross-validation)\n- Overfitting: gap between the curves - Underfitting: Plateau (adding more training samples do not help)\n\nSo how do we handle the overfitting/underfitting model?\n\n\nOverfitting: Change too simpler model, feeding more training data, constrain the weights of unimportant features\n\nUnderfitting: Change to more complex algorithm; better features\n\n\n\n\n\n\n\nBias-Variation Trade-Off\n\n\n\n\nBias (underfitting): wrong assumptions (e.g. assump linear while quadratic)\nVariation (overfitting): remember data (sensitive to variations in data)\n=&gt; Trade-Off: Increase model’s complexity will increase variation and decrease bias\n\nIrreducible error: noisiness =&gt; clean up data"
  },
  {
    "objectID": "regression.html#regularized-linear-models",
    "href": "regression.html#regularized-linear-models",
    "title": "3  Regression",
    "section": "3.4 Regularized Linear Models",
    "text": "3.4 Regularized Linear Models\nAs mentioned above, to reduce overfitting we constrain the weights of model. These techniques are called regularization including: Ridge regression, Lasso Regression and Elastic net.\n\n\n\n\n\n\n\nRegularized linear models: Sensitive to the scale\n=&gt; StandardScaler before regularize\n\nIn almost cases, we should avoid plain Linear regression\nUse case of Regularized linear models:\n\n\nElastic Net: when there are few useful features, (features &gt; instances, correlated features =&gt; Lasso tends to behave erratically)\nLasso: when there are few useful features\nRidge: good for default (a warmstart)\n\n\nFind out more about RidgeCV, LassoCV and ElasticNetCV\n\n\n\n\n?fig-l1-l2\n\n3.4.1 Ridge Regression\nAdd a regularization term (L2 norm) to the MSE cost function of Linear regression in order to keep the weights as small as possible\nRidge regression cost function \\[\n\\begin{equation}\n\\begin{split}\nJ(θ) & = MSE(θ) + \\frac{α}{2m}\\sum_{i=1}^{m}w_i^2\\\\\n    & = MSE(θ) + \\frac{α}{2m}θ^Τθ\n\\end{split}\n\\end{equation}\n\\]\nClosed-form equation \\[θ = (X^{T}X + αΑ)^{-1}X^{T}Y\\]\n\n\n\n\n\n\nsklearn.linear_model.Ridge(solver=‘cholesky’)\n\n\n\n\nfrom sklearn.linear_model import Ridge\n\nnp.random.seed(29)\nm = 50\nX = 3 * np.random.rand(m, 1)\ny = 1 + 0.5 * X + np.random.randn(m, 1) / 1.5\n\ndef make_plot(alphas):\n    plt.plot(X, y, 'b.')\n    for alpha, style in zip(alphas, ['b:','r--','g-']):\n        pipe = make_pipeline(PolynomialFeatures(degree=5, include_bias=False), Ridge(alpha=alpha, solver='cholesky'))\n        pipe.fit(X, y)\n        X_new = np.linspace(0, 3, 100).reshape(-1, 1)\n        y_pred_new = pipe.predict(X_new)\n\n        plt.plot(X_new, y_pred_new, style, label='alpha = %s' % alpha)\n    plt.axis([0, 3, 0, 3.5])\n    plt.legend()\n    plt.show()\n\nmake_plot([0,0.1,1])\n\n\n\n\nGradient descent \\[\n\\begin{gather}\n∇ = \\frac{1}{m}X^{T}(Xθ - y)+\\frac{α}{m}θ\\\\\n\\\\\nθ = θ - λ∇\\\\\n\\end{gather}\n\\]\nThese 2 models are equally, in which we have to set the lpha in the SGD to be alpha/m\n\nfrom sklearn.linear_model import Ridge\n\nalpha = 0.01\n\nridge = Ridge(alpha=0.1, random_state=29)\nsgd = SGDRegressor(penalty='l2', alpha=0.1/m, random_state=29)\n\n\n\n3.4.2 Lasso Regression\nAdd a regularization term (L1 norm) to the MSE cost function of Linear regression, but tend to eliminate weights of least important features\n=&gt; Weights is sparse matrix\nLasso regression cost function \\[\n\\begin{equation}\n\\begin{split}\nJ(θ) & = MSE(θ) + α\\sum_{i=1}^{m}|w|\\\\\n    & = MSE(θ) + αθ\n\\end{split}\n\\end{equation}\n\\]\nGradient descent\nThe L1 regularization is not differentiable at θi = 0, but gradient descent still works if we use a subgradient vector g11 instead when any θi = 0. Learn more about gradient descent for lasso regression\nThese 2 models are equally, and we have to adjust the alpha as same as ridge regression\n\nfrom sklearn.linear_model import Lasso\n\nalpha = 0.01\n\nridge = Lasso(alpha=0.1, random_state=29)\nsgd = SGDRegressor(penalty='l1', alpha=0.1/m, random_state=29)\n\n\n\n3.4.3 Elastic Net Regression\nElastic Net is weighted sum of Ridge and Lasso regression, change the weights by r rate: 0 (more Ridge) to 1 (more Lasso)\n\\[\nJ(θ) = MSE(θ) + r*\\frac{α}{2m}\\sum_{i=1}^{m}w_i^2 + (1-r)α\\sum_{i=1}^{m}|w_i|\n\\]\n\nfrom sklearn.linear_model import ElasticNet\n\nelast = ElasticNet(alpha=0.01, l1_ratio=0.5)"
  },
  {
    "objectID": "regression.html#early-stopping",
    "href": "regression.html#early-stopping",
    "title": "3  Regression",
    "section": "3.5 Early Stopping",
    "text": "3.5 Early Stopping\nAnother way to regularize iterative learning algorithms (e.g. GD): partial_fit for n epochs and save the model has the lowest validation error\n\nfrom copy import deepcopy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n\n## Create data\n\nnp.random.seed(29)\nm = 100\nX = 6 * np.random.rand(m, 1) - 3\ny = 0.5 * X ** 2 + X + 2 + np.random.randn(m, 1)\nepochs = 500\nbest_rmse = np.inf\n\nx_train, x_test, y_train, y_test = train_test_split(X, y)\n\npreprocessing = make_pipeline(PolynomialFeatures(degree=90, include_bias=False), StandardScaler())\nX_train = preprocessing.fit_transform(x_train)\nX_test = preprocessing.transform(x_test)\n\n\nsgd = SGDRegressor(penalty='elasticnet', alpha=0.01, l1_ratio=0.5, eta0=0.001, random_state=29)\n\nfor epoch in range(epochs):\n    sgd.partial_fit(X_train, y_train.ravel())\n    y_pred = sgd.predict(X_test)\n    rmse = mean_squared_error(y_test, y_pred, squared=False)\n    if rmse &lt; best_rmse:\n        best_rmse = rmse\n        best_model = deepcopy(sgd)\n\ny_pred = best_model.predict(X_test)\n\n\n## Another way to apply early stopping\n\n# sgd = SGDRegressor(penalty='elasticnet', alpha=0.01, l1_ratio=0.5, max_iter = 2000, tol=0.00001, shuffle=True, random_state=29, learning_rate='invscaling', eta0=0.001, early_stopping=True, validation_fraction=0.25, n_iter_no_change=10)\n# sgd.fit(X_train, y_train.ravel())\n# y_pred = sgd.predict(X_test)\nprint('RMSE: %.2f' % mean_squared_error(y_test, y_pred))\n\nRMSE: 0.67\n\n\n\n\n\n\n\n\npartial_fit: max_iter=1 (fit 1 epoch per calling); learn incrementally from a mini-batch of instances =&gt; useful when data is not fit into memory\nfit: train model from scratch (all instances at once)\nfit(warm_start=True) = partial_fit: allow learning from the weights of previous fit\ncopy.deepcopy(): copies both the model’s hyperparameters and the learned parameters\nsklearn.base.clone() only copies the model’s hyperparameters."
  },
  {
    "objectID": "regression.html#logistic-regression",
    "href": "regression.html#logistic-regression",
    "title": "3  Regression",
    "section": "3.6 Logistic Regression",
    "text": "3.6 Logistic Regression\n\n[f] Train function \\[\n\\begin{gather}\nLinear Regression: y = θ^{T}X\\\\\nLogistic Regression: p = sigmoid(θ^{T}X)\\\\\nsigmoid(t) = \\frac{1}{1-e^{-t}}\\\\\nlogit(p) = log\\left(\\frac{p}{1-p}\\right)= t\n\\end{gather}\n\\]\n[f] Cost function for 1 instance \\[\\begin{equation}\n\\begin{split}\nJ(θ) & = -log(p)\\quad \\quad \\quad if\\;\\;\\; y=1\\\\\n& = -log(1-p) \\quad \\; if\\;\\;\\;  y=0\n\\end{split}\n\\end{equation}\\]\n[i] - log(p) -&gt; inf when p -&gt; 0 for y = 1 instance\n[i] -log(1-p) -&gt; inf when p -&gt; 1 for y = 0 instance\n[f] Cost function for whole training set (log loss): convex function \\[J(θ) = \\frac{−1}{m} \\sum [y_ilog(p_i) + (1−y_i)log(1−p_i)]\\]\n[!] Log loss assumption: the instances follow a Gaussian distribution around the mean of their class\n[!] MSE assumption: data is purely linear\n[!] The more wrong assumption, the more biased the model\n[!] No closed-form equation to compute θ \\[∇ = \\frac{1}{m}X^{T}[sigmoid(Xθ) - y]\\]\n[i] Regularization in Logistic Regression: C (inverse of alpha), l1, l2"
  },
  {
    "objectID": "regression.html#softmax-regression-multinomial-logistic-regression",
    "href": "regression.html#softmax-regression-multinomial-logistic-regression",
    "title": "3  Regression",
    "section": "3.7 Softmax Regression (Multinomial Logistic Regression)",
    "text": "3.7 Softmax Regression (Multinomial Logistic Regression)\n\n[f] Multipleclass Classification\n[i] Softmax score for class k \\[s_k(x) = (θ^{(k)})^{T}X\\]\n[i] Softmax function \\[p_k = σ(s(x))_k = \\frac{exp(s_k(x))}{\\sum\\limits exp(s_j(x))}\\]\nK is the number of classes.\ns(x) is a vector containing the scores of each class for the instance x.\nσ(s(x))k is the estimated probability that the instance x belongs to class k, given the scores of each class for that instance.\n[i] Softmax regression classifier prediction \\[y= argmax\\; σ(s(x))_k= argmax\\;s_k(x) = argmax\\; (θ^{k})^{T}X\\]\n[i] Cross entropy cost function\n[i] Cross entropy gradient vector for class k"
  },
  {
    "objectID": "gradient-descent.html#batch-gradient-descent",
    "href": "gradient-descent.html#batch-gradient-descent",
    "title": "4  Gradient Descent",
    "section": "4.1 Batch Gradient Descent",
    "text": "4.1 Batch Gradient Descent\n\n[i] Full Gradient Descent =&gt; terribly slow\n[i] Gradient: \\[∇ = \\frac{1}{m} X^{T}(Χθ-y)\\]\n[i] Scale well with number of features"
  },
  {
    "objectID": "gradient-descent.html#stochastic-gradient-descent",
    "href": "gradient-descent.html#stochastic-gradient-descent",
    "title": "4  Gradient Descent",
    "section": "4.2 Stochastic Gradient Descent",
    "text": "4.2 Stochastic Gradient Descent\n\n[i] Pick a random instance at every step (not epoch) to compute gradient\n[i] Out-of-core algorithm\n[!] Cost function: cost function is erratic, continue bounch around when get to the global minimum\n\nCan jump out local minimum\nWeights are good, not optimal =&gt; Improve by set gradually reduce learning_rate (called learning schedule)\nRandomness =&gt; Improve by shuffling to ensure pick every instance"
  },
  {
    "objectID": "gradient-descent.html#mini-batch-gradient-descent",
    "href": "gradient-descent.html#mini-batch-gradient-descent",
    "title": "4  Gradient Descent",
    "section": "4.3 Mini-Batch Gradient Descent",
    "text": "4.3 Mini-Batch Gradient Descent\n\n[i] Compute gradient on small random sets called mini-batches (boost by GPUs)\n[i] Less erratic\n\n[[Pasted image 20240115083333.png]]"
  },
  {
    "objectID": "hands-on-classification.html#multioutput-classification",
    "href": "hands-on-classification.html#multioutput-classification",
    "title": "7  Hands-on Classification",
    "section": "7.6 Multioutput Classification",
    "text": "7.6 Multioutput Classification\n\nMulticlass-multilabel classification\nFor example, we will build a model that removes noise from an digit image\nOutput is a clean image 28x28: multilabel (one label per pixel) and multiclass (pixel intensity range from 0-255 per label)\n\n\n## Create a noisy train set\n\nnp.random.seed(42)\n\nnoise = np.random.randint(0,100,(len(X_train), 28*28))\nX_train_noise = X_train + noise\ny_train_noise = X_train\n\nnoise = np.random.randint(0,100,(len(X_test), 28*28))\nX_test_noise = X_test + noise\ny_test_noise = X_test\n\nLet’s look at sample images\n\nplt.subplot(1,2,1)\nplot_digit(X_train_noise[0])\nplt.subplot(1,2,2)\nplot_digit(y_train_noise[0])\n\nplt.show()\n\n\n\n\n\nknn.fit(X_train_noise, y_train_noise)\ny_pred_noise = knn.predict([X_train_noise[0]])\nplot_digit(y_pred_noise)"
  },
  {
    "objectID": "metrics.html#sklearn-documents",
    "href": "metrics.html#sklearn-documents",
    "title": "10  Metrics and scoring",
    "section": "10.1 Sklearn documents",
    "text": "10.1 Sklearn documents\nsklearn.metrics"
  },
  {
    "objectID": "metrics.html#classification",
    "href": "metrics.html#classification",
    "title": "10  Metrics and scoring",
    "section": "10.2 Classification",
    "text": "10.2 Classification\n\n10.2.1 Accuracy\n\naccuracy(y_true, y_pred)\nProportion of exact match prediction of the model\nAs this case, the result showed that with the presence of class immbalance, accuracy score is not useful metric\n\n\n\n10.2.2 Confusion matrix\n\nnsdf nnsf"
  },
  {
    "objectID": "metrics.html#confusion-matrix",
    "href": "metrics.html#confusion-matrix",
    "title": "9  Metrics and scoring (#sec-metrics)",
    "section": "9.3 ### Confusion matrix",
    "text": "9.3 ### Confusion matrix"
  },
  {
    "objectID": "hands-on-regression.html#datasets",
    "href": "hands-on-regression.html#datasets",
    "title": "5  Hands-on regression",
    "section": "5.1 Datasets",
    "text": "5.1 Datasets\nPopular open data repositories\n\nhttps://openml.org/\n\nhttps://Kaggle.com\nhttps://PapersWithCode.com\n\nUC Irvine Machine Learning Repository — Amazon’s AWS datasets\nTensorFlow datasets\n\nMeta portals (they list open data repositories)\n\nhttps://DataPortals.org\n\nhttps://OpenDataMonitor.eu\n\nOther pages listing many popular open data repositories\n\nWikipedia’s list of machine learning datasets\nhttps://Quora.com\nThe datasets subreddit\n\nIn this chapter we’ll use the California Housing Prices dataset from the StatLib repository. This dataset is based on data from the 1990 California census. It is not exactly recent but it has many qualities for learning."
  },
  {
    "objectID": "hands-on-regression.html#look-at-the-big-picture",
    "href": "hands-on-regression.html#look-at-the-big-picture",
    "title": "5  Hands-on regression",
    "section": "5.2 Look at the big picture",
    "text": "5.2 Look at the big picture\nQuestions\n\nBusiness objective? Current solution (if any)?\nHow to use and benefit from the model?\nData Pipelines?\nDetermine kind of model\nSelect preformance measures\nCheck the Assumptions\n\nAnswers\n\nPredict median housing price in any district. The results are used for another ML system for investment analysis. The current solution is to gather up-to-date information about a district, or to estimate manually using complex rules if no information.\nThe current solution is costly and time-consuming, and it was often off by more than 30%. Therefore, a ML model could be more useful\nData pipelines: A sequence of data processing components. Pipelines are very common in machine learning systems, since there is a lot of data to manipulate and many data transformations to apply.\nThis is a regression task (labeled data), batch learning (data is quite small and not change too much) and model-based learning\nMetrics for regression:\n\n\n\n\n\n\n\nExpand to learn more about metrics\n\n\n\n\n\nBoth the RMSE and the MAE are ways to measure the distance between two vectors: the vector of predictions and the vector of target values. Various distance measures, or norms, are possible:\n• Computing the root of a sum of squares (RMSE) corresponds to the Euclidean norm: this is the notion of distance we are all familiar with. It is also called the l2 norm, noted |·|₂ (or just |·|).\n• Computing the sum of absolutes (MAE) corresponds to the l1 norm, noted |·|₁. This is sometimes called the Manhattan norm because it measures the distance between two points in a city if you can only travel along orthogonal city blocks.\n• More generally, the lk norm of a vector v containing n elements is defined as ∥v∥k = (|v₁|ᵏ + |v₂|ᵏ + … + |vₙ|ᵏ)¹/ᵏ. l0 gives the number of nonzero elements in the vector, and l∞ gives the maximum absolute value in the vector.\nThe higher the norm index, the more it focuses on large values and neglects small ones. This is why the RMSE is more sensitive to outliers than the MAE. But when outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs very well and is generally preferred. \n\n\n\nRMSE (root mean squared error): l2 norm\n\\[\nRMSE(X, y)  = \\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}(y_{hat}^{(i)} - y^{(i)})^2}\n\\]\nMAE (mean squared error): l1 norm\n\\[\nMAE(X, y)  = \\frac{1}{m}\\sum_{i=1}^{m}|y_{hat}^{(i)} - y^{(i)}|\n\\]\n\nCheck with the team in charge of the downstream system that use out output whether it is suitable or not (e.g. it is terrible if after several months building model you realize that they need ordinal output not numerical one)"
  },
  {
    "objectID": "hands-on-regression.html#get-the-data",
    "href": "hands-on-regression.html#get-the-data",
    "title": "5  Hands-on regression",
    "section": "5.3 Get the Data",
    "text": "5.3 Get the Data\n\n5.3.1 Download Data\n\n## Load data\n\nfrom pathlib import Path\nimport pandas as pd\nimport tarfile\nimport urllib.request\n\ndef load_data(url):\n    path = Path(\"datasets/housing.tgz\")\n    if not path.is_file():\n        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n        urllib.request.urlretrieve(url, path)\n        tarfile.open(path).extractall(path='datasets')\n    return pd.read_csv('datasets/housing/housing.csv')\n\nurl = 'https://github.com/ageron/data/raw/main/housing.tgz'\ndata = load_data(url)\n\n\n\n5.3.2 Quick Look to Data Structure: head(), info(), describe(), value_counts(), histplot()\n\n## Quick look\n\npd.set_option('display.max_columns', None)      # display all columns\ndata.head()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n0\n-122.23\n37.88\n41.0\n880.0\n129.0\n322.0\n126.0\n8.3252\n452600.0\nNEAR BAY\n\n\n1\n-122.22\n37.86\n21.0\n7099.0\n1106.0\n2401.0\n1138.0\n8.3014\n358500.0\nNEAR BAY\n\n\n2\n-122.24\n37.85\n52.0\n1467.0\n190.0\n496.0\n177.0\n7.2574\n352100.0\nNEAR BAY\n\n\n3\n-122.25\n37.85\n52.0\n1274.0\n235.0\n558.0\n219.0\n5.6431\n341300.0\nNEAR BAY\n\n\n4\n-122.25\n37.85\n52.0\n1627.0\n280.0\n565.0\n259.0\n3.8462\n342200.0\nNEAR BAY\n\n\n\n\n\n\n\nThere are total 10 features, each row represents a district observation\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 10 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   longitude           20640 non-null  float64\n 1   latitude            20640 non-null  float64\n 2   housing_median_age  20640 non-null  float64\n 3   total_rooms         20640 non-null  float64\n 4   total_bedrooms      20433 non-null  float64\n 5   population          20640 non-null  float64\n 6   households          20640 non-null  float64\n 7   median_income       20640 non-null  float64\n 8   median_house_value  20640 non-null  float64\n 9   ocean_proximity     20640 non-null  object \ndtypes: float64(9), object(1)\nmemory usage: 1.6+ MB\n\n\nData with 10 columns and 20640 rows 9 numerical features, 1 categorical feature ‘total_bedrooms’ has only 20433 non-null values\n\ndata.describe(include='all')        # describe all type of data\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\ncount\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20433.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640\n\n\nunique\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n5\n\n\ntop\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n&lt;1H OCEAN\n\n\nfreq\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n9136\n\n\nmean\n-119.569704\n35.631861\n28.639486\n2635.763081\n537.870553\n1425.476744\n499.539680\n3.870671\n206855.816909\nNaN\n\n\nstd\n2.003532\n2.135952\n12.585558\n2181.615252\n421.385070\n1132.462122\n382.329753\n1.899822\n115395.615874\nNaN\n\n\nmin\n-124.350000\n32.540000\n1.000000\n2.000000\n1.000000\n3.000000\n1.000000\n0.499900\n14999.000000\nNaN\n\n\n25%\n-121.800000\n33.930000\n18.000000\n1447.750000\n296.000000\n787.000000\n280.000000\n2.563400\n119600.000000\nNaN\n\n\n50%\n-118.490000\n34.260000\n29.000000\n2127.000000\n435.000000\n1166.000000\n409.000000\n3.534800\n179700.000000\nNaN\n\n\n75%\n-118.010000\n37.710000\n37.000000\n3148.000000\n647.000000\n1725.000000\n605.000000\n4.743250\n264725.000000\nNaN\n\n\nmax\n-114.310000\n41.950000\n52.000000\n39320.000000\n6445.000000\n35682.000000\n6082.000000\n15.000100\n500001.000000\nNaN\n\n\n\n\n\n\n\n\nfor ft in data.select_dtypes('object'):     # choose 'object' features only\n    print(data[ft].value_counts())\n    print(f'Number of classes: {data[ft].nunique()}')\n\nocean_proximity\n&lt;1H OCEAN     9136\nINLAND        6551\nNEAR OCEAN    2658\nNEAR BAY      2290\nISLAND           5\nName: count, dtype: int64\nNumber of classes: 5\n\n\nQuite imbalanced classes\n\n## Plot histogram of numerical features\n\nimport matplotlib.pyplot as plt\n\ndata.hist(bins=50, figsize=(8,8))\nplt.show()\n\n\n\n\nhousing_median_age: capped at range (1,52)\nmedian_income: scaled and capped at range ~ (0.5, 15)\nmedian_house_value: capped at top range of 500,000\nThese features are skewed and have very different scales =&gt; Transforming and Feature Scaling\n\n\n5.3.3 Create train, val and test set: train_test_split()\nrandom sampling: data must be large enough, otherwise there is a risk of sampling bias stratified sampling: based on some very important features, help avoid bias and ensure train set and test set are representative to full dataset\nHere we assump that median_income is very important feature to predict household value\n\n## Create new feature (income group)\nimport numpy as np\n\ndata['income_grp'] = pd.cut(data['median_income'], bins=[0,1.5,3,4.5,6,np.inf], labels=[1,2,3,4,5])\n\ndata.income_grp.value_counts().sort_index().plot.bar(rot=0, grid=True)\nplt.title('Frequency of income group')\nplt.xlabel('Income group')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n## Split data into train, val, test set\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(data.drop('median_house_value', axis=1), data.median_house_value, stratify=data['income_grp'], test_size=0.2, random_state=24)\n\n\n## Drop the 'income_grp' after using\n\nfor df in [x_train, x_test]:\n    df.drop('income_grp', axis=1, inplace=True)"
  },
  {
    "objectID": "hands-on-regression.html#explore-and-visualize-the-data-to-gain-insights",
    "href": "hands-on-regression.html#explore-and-visualize-the-data-to-gain-insights",
    "title": "5  Hands-on regression",
    "section": "5.4 Explore and visualize the data to gain insights",
    "text": "5.4 Explore and visualize the data to gain insights\nIf the data is very large, we will sample an exploration set to manipulate faster and easier. On the other hand, just work directly on full set if the data is quite small\n\ndf_vis = data.copy()\n\n\n5.4.1 Visualize\nPlot\n\n\n\n\n\nFigure 5.1: The geographical plot of data\n\n\n\n\nLarge point size: larger population Blue -&gt; red: higher house price\nCorrelation\nThere are 2 ways to perform: heatmap and pairgrid map\n\n\n\n\n&lt;Axes: &gt;\n(a) Correlation Plot\n\n\n\n\n\n\n(b)\n\n\n\nFigure 5.2: ?(caption)\n\n\n\n\n\n\n\nFigure 5.3: PairGrid Plot of numerical features\n\n\n\n\nTry pd.plotting.scatter_matrix\nLook closely too the relation between ‘median_house_value’ and ‘median_income’, we see there is a strong positive correlation, but there are some clearly horizontal line at 500,000; 450,000; 350,000 and roughly 280,000. We should remove these instances to prevent the algorithms learning these patterns.\n\n\n\n\n\nFigure 5.4: Median income versus median house value\n\n\n\n\n\n\n5.4.2 Attributes combination\nUseful when we want to find better features to predict\n\ndf_vis['room_per_house'] = df_vis['total_rooms']/df_vis['households']\ndf_vis['bedroom_ratio'] = df_vis['total_bedrooms']/df_vis['total_rooms']\ndf_vis['people_per_house'] = df_vis['population']/df_vis['households']\n\ncorr_matrix = df_vis.corr(numeric_only=True)\ncorr_matrix['median_house_value'].sort_values(ascending=False)\n\nmedian_house_value    1.000000\nmedian_income         0.688075\nroom_per_house        0.151948\ntotal_rooms           0.134153\nhousing_median_age    0.105623\nhouseholds            0.065843\ntotal_bedrooms        0.049686\npeople_per_house     -0.023737\npopulation           -0.024650\nlongitude            -0.045967\nlatitude             -0.144160\nbedroom_ratio        -0.255880\nName: median_house_value, dtype: float64"
  },
  {
    "objectID": "svm.html#non-linear-svm-classification",
    "href": "svm.html#non-linear-svm-classification",
    "title": "8  Support Vector Machine",
    "section": "8.2 Non-linear SVM Classification",
    "text": "8.2 Non-linear SVM Classification\nWith non-linearly seperable datasets, our model will be underfitting if we use linear algorithms. We can instead use more complex models (Random forest, etc.) or add more features (Polynomial features, similarity features using Gaussian RBF, etc.), but this will lead to a huge bunch of new features and computationally expensive.\nTherefore, SVM supply a powerful technique called kernel trick, allow us to get the same result as if add many polynomial/similarity features, without actually having to add them.\nPolynomial kernel\n\nfrom sklearn.svm import SVC\n\npoly_svc = make_pipeline(StandardScaler(),\nSVC(kernel='poly', degree=3, C=10, coef0=1))\npoly_svc.fit(X, y)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('svc', SVC(C=10, coef0=1, kernel='poly'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('svc', SVC(C=10, coef0=1, kernel='poly'))])StandardScalerStandardScaler()SVCSVC(C=10, coef0=1, kernel='poly')\n\n\nGaussian RBF kernel\n\nrbf_svc = make_pipeline(StandardScaler(),\nSVC(kernel='rbf', C=10, gamma=5))\n\n\n\n\n\n\n\n\ncoef0: controls how much the model is influenced by high-degree terms versus low-degree terms.\n\nC, gamma: high =&gt; overfitting, low =&gt; underfitting\n\ngamma:\n\nHigh: narrow bell-shaped curve, each instance’s range of influence is smaller, tend to wiggling around individual instances\nLow: wide bell-shaped curve, (vice versa)"
  },
  {
    "objectID": "svm.html#linear-svm-classification",
    "href": "svm.html#linear-svm-classification",
    "title": "8  Support Vector Machine",
    "section": "8.1 Linear SVM Classification",
    "text": "8.1 Linear SVM Classification\n\n\nTable 8.1: Hard margin and soft margin classification\n\n\n\n\n\n\nHard margin\nSoft margin\n\n\n\n\n- All instances must be off the street\n- Limiting margin violations\n\n\n- Only work with linearly seperable data\n- Hyperparameter C: control the width of the street\n\n\n- Sensitive to outliers\n- Higher C: narrower street =&gt; more overfit ; vice versa"
  },
  {
    "objectID": "svm.html#linear-svm-classification-support-vector-classification---svc",
    "href": "svm.html#linear-svm-classification-support-vector-classification---svc",
    "title": "8  Support Vector Machine",
    "section": "8.1 Linear SVM Classification (Support Vector Classification - SVC)",
    "text": "8.1 Linear SVM Classification (Support Vector Classification - SVC)\nSupport Vector Machine (Large Margin Classification): Fitting the widest street between classes, supported by support vector instances on the street.\n\n\n\nFigure 8.1: Large margin classification\n\n\nSVMs are sensitive to feature scaling. As we can see, SVM seperate the data better with scaled data.\n\n\n\nFigure 8.2: SVMs are sensitive to feature scaling\n\n\nHard margin/Soft margin classification\n- Hard margin: All instances must be off the street, only work with linearly seperable data and sensitive to outliers\n- Soft margin: improve weakness of hard margin by allow limiting margin violations\n\n\n\n\n\n\nHyperparameter C: the penalty on any misclassified data point.\n- High: high penalty, stricter classification, narrower street and tends to overfit\n- Low: low penalty, allow larger number of misclassifications, wider street and tends to underfit\n\n\n\n\n\n\nFigure 8.3: Different C parameters\n\n\nImplement SVC\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\n\ndata = load_iris(as_frame=True)\nX = data.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\ny = (data.target == 2) # Iris virginica\nsome_flower = X[2,:]\nsvc = make_pipeline(StandardScaler(),\nLinearSVC(random_state=29))\nsvc.fit(X, y)\nprint(svc.predict([some_flower]))\nprint(svc.decision_function([some_flower]))\n\n[False]\n[-6.34263777]\n\n\n/usr/local/anaconda3/envs/dhuy/lib/python3.11/site-packages/sklearn/svm/_classes.py:32: FutureWarning:\n\nThe default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning."
  },
  {
    "objectID": "svm.html#svms-classes-computational-complexity",
    "href": "svm.html#svms-classes-computational-complexity",
    "title": "8  Support Vector Machine",
    "section": "8.3 SVMs Classes Computational Complexity",
    "text": "8.3 SVMs Classes Computational Complexity\n\n\n\nFigure 8.6: BigO of SVM classification"
  },
  {
    "objectID": "svm.html#svm-regression-support-vector-regression---svr",
    "href": "svm.html#svm-regression-support-vector-regression---svr",
    "title": "8  Support Vector Machine",
    "section": "8.4 SVM Regression (Support Vector Regression - SVR)",
    "text": "8.4 SVM Regression (Support Vector Regression - SVR)\nOpposed to SVC, SVR tries to fit as many instances as possible on the street while limiting margin violations (instances off the street)\nHyperparameter epsilon: control the width of the street - Low: narrow street, more support vector, tend to too complex - High: wide street, less support vector, tend to too simple\n\nimport numpy as np\nfrom sklearn.svm import SVR\n\nnp.random.seed(29)\nm = 100\nX = 6 * np.random.rand(m, 1) - 3\ny = 0.5 * X ** 2 + X + 2 + np.random.randn(m, 1)\n\nsvr = make_pipeline(StandardScaler(),\nSVR(kernel='poly', degree=5, C=0.001, epsilon=0.1))\nsvr.fit(X,y.ravel())\nsvr.predict([[3]])\n\narray([3.31601563])\n\n\n\n\n\nFigure 8.7: Different epsilons"
  },
  {
    "objectID": "svm.html#understand-the-fundamentals-of-svm",
    "href": "svm.html#understand-the-fundamentals-of-svm",
    "title": "8  Support Vector Machine",
    "section": "8.5 Understand the Fundamentals of SVM",
    "text": "8.5 Understand the Fundamentals of SVM\nTo predict the class of an instance, SVM compute decision function, then compare to the margin of the street to predict.\n\\[y = θ_{0} + θ^{Τ}X\\]\nSuppose that the margin is (-1,1). With the same margin, to make the wider street, we have to make the θ smaller.\n\n\n\nFigure 8.8: A smaller weights results in a larger margin\n\n\n\n8.5.1 Quadratic Programming Problem (QP solver)\nHard margin classification\nTo avoid the margin violations, we have to minimize the θ while making the decision function ≥1 for positive instances and ≤-1 for negative instances. This constraint can be written using t = 1 or t = -1 repectively:\n\\[\n\\begin{gather}\nminimize(θ,θ_{0})\\;\\;\\frac{1}{2}θ^{Τ}θ\\\\\nsubject\\;to\\;\\;t(θ_{0} + θ^{Τ}X) ≥ 1;\\;\\;t_{i} = [-1;1]\n\\end{gather}\n\\]\nSoft margin classification\nTo perform soft margin classification, we add a slack variable ζ(i) ≥ 0 for each instance: ζ measure how much the instance is allowed to violate the margin.\nExpectedly, we want to keep ζ as small as possible to reduce margin violations, but we also want the margin as wide as possible (too greedy 😆). Don’t worry, this is where the C parameter comes into play.\n\\[\n\\begin{gather}\nminimize(θ,θ_{0})\\;\\;\\frac{1}{2}θ^{Τ}θ + Cζ\\\\\nsubject\\;to\\;\\;t(θ_{0} + θ^{Τ}X) ≥ 1 - ζ;\\;\\;t_{i} = [-1;1];\\;ζ_{i}≥0\n\\end{gather}\n\\]\n\n\n8.5.2 Gradient Descent\nCost function: hinge loss or the squared hinge loss (loss hyperparameter)\nDecision function:\n- ≥ 1: true label is positive =&gt; loss = 0\n- ≤-1: true label is negative =&gt; loss = 0\nBy default: LinearSVC use squared hinge loss, while SGDClassifier use hinge loss\n\n\n\nFigure 8.9: The hinge loss and squared hinge loss\n\n\n\n\n8.5.3 Kernelized SVMs\nAs mentioned in Section 8.2, when we want to perform on more complex model like polynomial or RBF, kernel trick can compute the dot product in the minimization work directly on the original vectors a and b, without even know about the transformation. The Figure 8.10 illustrate the kernel trick for a second-degree polynomial\n\n\n\nFigure 8.10: Kernel trick for a second-degree polynomial\n\n\nThese are the common kernels, in which K is the kernel function:\n\n\n\nFigure 8.11: Common kernels\n\n\n\nd: degree\nr: coef0\nγ: gamma, ≥ 0\n\n\n\n\n\n\n\nLearn more about Dual problem, equation to make predictions with kernel trick"
  },
  {
    "objectID": "svm.html#sec-nonlinear-svm",
    "href": "svm.html#sec-nonlinear-svm",
    "title": "8  Support Vector Machine",
    "section": "8.2 Non-linear SVM Classification",
    "text": "8.2 Non-linear SVM Classification\nWith non-linearly seperable datasets in low dimensions, we want to transform them to a higher dimension where they will be linearly sepparable. Imagine “raising” the green points, then you can sepparate them from the red points with a plane (hyperplane).\n\n\n\nFigure 8.4: Non-linearly seperable\n\n\nTo do that, we can use more complex models (Random forest, etc.) or add more features (Polynomial features, similarity features using Gaussian RBF, etc.), but this will lead to a huge bunch of new features and computationally expensive.\nTherefore, SVM supply a powerful technique called kernel trick, allow us to get the same result as if add many polynomial/similarity features, without actually having to add them.\nPolynomial kernel\n\nfrom sklearn.svm import SVC\n\npoly_svc = make_pipeline(StandardScaler(),\nSVC(kernel='poly', degree=3, C=10, coef0=1))\n\nGaussian RBF kernel\n\nrbf_svc = make_pipeline(StandardScaler(),\nSVC(kernel='rbf', C=10, gamma=5))\n\n\n\n\n\n\n\n\ncoef0 (poly kernel): controls how much the model is influenced by high-degree terms versus low-degree terms.\n\ngamma (RBF kernel): high =&gt; overfitting, low =&gt; underfitting\n\ngamma: controls the shape of the “peaks” where you raise the points\n\nHigh: pointed bump (narrow bell-shaped curve), each instance’s range of influence is smaller, tend to wiggling around individual instances\nLow: softer, broader bump (wide bell-shaped curve), vice versa.\n\n\n\n\n\n\n\n\nFigure 8.5: Different C and gamma parameters"
  },
  {
    "objectID": "decision-tree.html#regularization",
    "href": "decision-tree.html#regularization",
    "title": "9  Decision Tree",
    "section": "9.1 Regularization",
    "text": "9.1 Regularization\n\nParametric model (Linear Regression): predetermined parameters =&gt; degree of freedom is limited =&gt; reducing the risk of overfitting\nNon-parametric model: parameters not determined prior to training =&gt; go freely =&gt; overfitting =&gt; regularization\nIncreasing min_* hyperparameters or reducing max_* hyperparameters will regularize the model:\n\nmax_depth, max_features, max_leaf_nodes\nmin_samples_split, min_samples_leaf, min_weight_fraction_leaf\n\n\n\nfrom sklearn.datasets import make_moons\nX_moons, y_moons = make_moons(n_samples=150, noise=0.2, random_state=42)\n\ndt_clf1 = DecisionTreeClassifier(random_state=29)\ndt_clf2 = DecisionTreeClassifier(max_depth=5, min_samples_leaf=5, random_state=29)\ndt_clf1.fit(X_moons, y_moons)\ndt_clf2.fit(X_moons, y_moons)\n\nX_moons_test, y_moons_test = make_moons(n_samples=1000, noise=0.2, random_state=43)\n\nprint(f'Non-regularized decision tree: {dt_clf1.score(X_moons_test, y_moons_test):.4f}')\nprint(f'Regularized decision tree: {dt_clf2.score(X_moons_test, y_moons_test):.4f}')\n\nNon-regularized decision tree: 0.8940\nRegularized decision tree: 0.9200"
  },
  {
    "objectID": "decision-tree.html#pruning-trees",
    "href": "decision-tree.html#pruning-trees",
    "title": "9  Decision Tree",
    "section": "9.2 Pruning Trees",
    "text": "9.2 Pruning Trees\nPruning (deleting) unnecessary nodes Other algorithms work by first training the decision tree without restrictions, then pruning (deleting) unnecessary nodes. A node whose children are all leaf nodes is considered unnecessary if the purity improvement it provides is not statistically significant. Standard statistical tests, such as the χ2 test (chi-squared test), are used to estimate the probability that the improvement is purely the result of chance (which is called the null hypothesis). If this probability, called the p-value, is higher than a given threshold (typically 5%, controlled by a hyperparameter), then the node is considered unnecessary and its children are deleted. The pruning continues until all unnecessary nodes have been pruned.\nThere are 3 types of Pruning Trees\n- Pre-Tuning - Post-Tuning - Combines"
  },
  {
    "objectID": "decision-tree.html#regression",
    "href": "decision-tree.html#regression",
    "title": "9  Decision Tree",
    "section": "9.3 Regression",
    "text": "9.3 Regression\n\nDecisionTreeRegressor splits each region in a way that makes most training instances as close as possible to that predicted value.\nCART cost function for regression: \\[\nJ_{k, t_k} = \\frac{m_{left}}{m} MSE_{left} + \\frac{m_{right}}{m} MSE_{right}\n\\]\n\nMSE: mean squared error m: number of instances\n[[Pasted image 20240116111255.png]] [[Pasted image 20240116111728.png]]"
  },
  {
    "objectID": "decision-tree.html#limitations-of-decision-tree",
    "href": "decision-tree.html#limitations-of-decision-tree",
    "title": "9  Decision Tree",
    "section": "9.4 Limitations of Decision Tree",
    "text": "9.4 Limitations of Decision Tree\n\nDecision tree tends to make orthogonal decision boudaries =&gt; Sensitive to the data’s orientation\n\nSolution: Scale data -&gt; PCA: reduce dimensions but do not loss too much information, rotate data to reduce correlation between features\n\nHigh variance: randomly select set of features to evaluate at each node =&gt; high variance, unstable\n\nSolution: Ensemble methods (Random Forest, Boosting methods) averaging predictions over many trees"
  }
]