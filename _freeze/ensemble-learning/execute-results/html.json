{
  "hash": "d00af95a38f0c0134043e8f4eedc43f2",
  "result": {
    "markdown": "---\nexecute:\n    freeze: auto\n---\n\n# Ensemble Learning {#sec-ensemble-learning}\n\n\nWhat would we do if we tried training multiple models but the results were not good enough? Let think about ensemble learning. Ensemble learning is a machine learning technique that combines the predictions of multiple weak learners to create a strong learner.\n\nEnsemble learning offers several advantages:\n\n- **Improved Accuracy**: Combining predictions from diverse models often leads to better overall performance compared to individual models.\n\n- **Reduced Overfitting**: Ensemble methods trade higher bias for lower variance, in order to mitigate overfitting, especially when using techniques like bagging and proper hyperparameter tuning.\n\n- **Increased Robustness**: Ensemble models are more resilient to noisy data and outliers.\n\n- **Enhanced Generalization**: The diversity of models in an ensemble allows for better generalization to unseen data.\n\nHowever, ensemble learning also comes with increased computational complexity and may require more careful tuning of hyperparameters. The choice of ensemble method depends on the characteristics of the data and the specific problem at hand.\n\n\n::: {.callout-note collapse=\"true\"}\n## Key ensemble learning techniques\n\n**Bagging (Bootstrap Aggregating)**: Training multiple learner `parallely` on `subset` of dataset sampled with/without `replacement` (`equally` variables) using `random subset` of variables at each step of spliting internal node. Measure performance by '`Out-Of-Bag Error`'. Random Forests (ensemble of decision trees) are a notable example.\n\n**Boosting**: Training multiple learner `sequentially` on `full dataset`, each subsequent model adjust the weights by correcting the errors of the previous ones (`weighted` variables). Popular algorithms using boosting include AdaBoost, Gradient Boosting (e.g., XGBoost, LightGBM), and CatBoost.\n\n**Stacking**: Stacking combines predictions from multiple models by training a meta-model on their outputs. Base models act as input features for the meta-model, which learns to make a final prediction.\n\nRandom forests, AdaBoost, and GBRT are among the first models you should test for most machine learning tasks, and they particularly shine with heterogeneous tabular data. Moreover, as they require very little preprocessing, they’re great for getting a prototype up and running quickly. Lastly, ensemble methods like voting classifiers and stacking classifiers can help push your system’s performance to its limits.\n\n:::\n\n::: {.callout-tip}\nHyperparameters:\n\n- Bagging methods: n_estimators, bootstrap, max_samples, bootstrap_features, max_features, oob_score.\n- Decision tree: [hyperparameters](decision-tree.qmd#regularization).\n- Gradient Boosting: learning_rate, early_stopping, n_iter_no_change, tol, validation_fraction, subsample (like max_samples).\n:::\n\n## Bagging\n\n### Voting Classifier\n\nThe simpliest bagging algorithm with no boostrapping and only aggregating. It aggregates predictions by choosing most voted class (hard voting) or class with highest probability (soft voting).\n\nIn sklearn.ensemble.VotingClassifier, it performs hard voting by default.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier, RandomForestClassifier\nfrom sklearn.svm import SVC\n\nX, y = make_moons(n_samples=500, noise=0.30, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\nvoting_clf = VotingClassifier(\n        estimators=[\n            ('lr', LogisticRegression(random_state=42)),\n            ('rf', RandomForestClassifier(random_state=42)),\n            ('svc', SVC(random_state=42))\n] )\nvoting_clf.fit(X_train, y_train)\n\nprint('Accuracy of individual predictor:')\nfor name,clf in voting_clf.named_estimators_.items():\n    print(f'{name} = {clf.score(X_test,y_test)}')\n\nprint(f'Accuracy of ensemble: {voting_clf.score(X_test, y_test)}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy of individual predictor:\nlr = 0.864\nrf = 0.896\nsvc = 0.896\nAccuracy of ensemble: 0.912\n```\n:::\n:::\n\n\nChange to soft voting\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nvoting_clf.voting = 'soft'\nvoting_clf.named_estimators['svc'].probability = True\nvoting_clf.fit(X_train, y_train)\nvoting_clf.score(X_test, y_test)\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\n0.92\n```\n:::\n:::\n\n\n### Bagging and Pasting\n\n- Training multiple learner parallely => Scale very well.\n- Sampling with replacement (Bagging); sampling without replacement (pasting).\n\n![Bagging and Pasting](images/bagging-pasting.png){#fig-bagging-pasting}\n\n- Sampling instances : **max_samples** indicates number/proportion of instances used to train model; **bootstrap=True/False**.\n- Sampling features: **max_features** and **bootstrap_features** work the same.\\\n    - Random patches: sampling instances and features\\\n    - Random subspaces: sampling only features.\\\n- The randomness trades a higher bias for lower variance.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nbagging_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, max_samples=100, random_state=29)\n\nbagging_clf.fit(X_train, y_train)\nbagging_clf.score(X_train, y_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n0.9413333333333334\n```\n:::\n:::\n\n\n![Bagging decision trees](images/bagging-decision-trees.png){#fig-bagging-decision-trees}\n\n- Measure performance by `Out-Of-Bag Error`: set **oob_score=True**; the score made on the remaining not sampled instances.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nbagging_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, oob_score=True, max_samples=100, random_state=29)\n\nbagging_clf.fit(X_train, y_train)\nbagging_clf.oob_score_\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n0.9226666666666666\n```\n:::\n:::\n\n\n### Random Forest\n\nRandom forest is ensemble of decision trees using bagging on full dataset. RandomForestClassifier has all the hyperparameters of DecisionTreeClassifier and  BaggingClassifier to control ensemble. \n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, min_samples_leaf=10, random_state=29)\nrf_clf.fit(X_train, y_train)\nrf_clf.score(X_test, y_test)\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n0.92\n```\n:::\n:::\n\n\nAdd more randomness:\n\n- Decision tree: searching for the best possible thresholds of each feature.\n- Extra-trees: search for best feature using random thresholds for each feature. Use RandomForestClassifier(splitter=\"random\") or ExtraTreesClassifier.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nbagging_clf = BaggingClassifier(DecisionTreeClassifier(splitter='random'), n_estimators=500, random_state=29)\nbagging_clf.fit(X_train, y_train)\nprint(f'Random forest: {bagging_clf.score(X_test, y_test)}')\n\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nextra_tree = ExtraTreesClassifier(n_estimators=500, random_state=29)\nextra_tree.fit(X_train, y_train)\nprint(f'Extra trees: {extra_tree.score(X_test, y_test)}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRandom forest: 0.888\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nExtra trees: 0.88\n```\n:::\n:::\n\n\n### Feature importance\n\nAlthough we have state that bagging method treat all features equally, there is still a method to get the `weighted average` after training by look at the proportion of training samples used to reduce impurity. This can be used to perform features selection.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.datasets import load_iris\niris = load_iris(as_frame=True)\nrf_clf = RandomForestClassifier(n_estimators=500, random_state=42)\nrf_clf.fit(iris.data, iris.target)\n\nfor score,feature in zip(rf_clf.feature_importances_, iris.data.columns):\n    print(f'{feature}: {score:.3f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nsepal length (cm): 0.112\nsepal width (cm): 0.023\npetal length (cm): 0.441\npetal width (cm): 0.423\n```\n:::\n:::\n\n\n## Boosting\n\nWe will talk about AdaBoost (adaptive boosting) and Gradient boosting.\n\n\n### AdaBoost\n\nThe algorithm work by training predictors sequentially. First it trains the base model and make predictions, then train the new pridictor weighting more on misclassified instances of the previous one, and so on. This is one of the most powerful model, and its main drawback is just do not scale really well.\n\n![AdaBoost](images/adaboost.png){#fig-adaboost}\n\nFundamentals:\n\n1. Initialize weights w(i) of each instance equal to 1/m\n2. Train base predictor\n3. Predict and get weighted error rate r(j)\n Weighted error rate of the j(th) predictor\n$$r_j = \\sum_{i=1}^{m}w^{(i)}\\;\\;\\;with\\;yhat_{j}^{(i)}\\;≠\\;y^{(i)}$$\n\n4. Compute predictor’s weight α(j). The more accurate the predictor, the higher its α(j)\n\n$$α_j = ηlog(\\frac{1-r_j}{r_j})$$\n\n5. Update instances's weights. This give more weights on misclassified instances.\n\n$$\n\\begin{equation}\n\\begin{split}\nw^{(i)} &= w^{(i)}\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;if\\;yhat_{j}^{(i)} = y^{(i)}\\\\\n&=w^{(i)}exp(α_j)\\;\\;if\\;yhat_{j}^{(i)} ≠ y^{(i)}\n\\end{split}\n\\end{equation}\n$$\n\n6. Normalize all instances's weights.\n\n$$w^{(i)} = \\frac{w^{(i)}}{\\sum_{i=1}^{m}w^{(i)}}$$\n\n7. Train new predictor using these weights\n8. Stop training when number of predictors is reached, or when a perfect predictor is found.\n9. Predict by majority vote.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfrom sklearn.ensemble import AdaBoostClassifier\n\nada_clf = AdaBoostClassifier(estimator=DecisionTreeClassifier(), learning_rate=0.001, n_estimators=100, random_state=29)\n\nada_clf.fit(X_train, y_train)\nada_clf.score(X_test, y_test)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n0.824\n```\n:::\n:::\n\n\n### Gradient Boosting\n\nAs same as AdaBoost, but instead of tweaking the instance weights, this method tries to fit the new predictor to the log loss (classification)/residual errors (regression) made by the previous predictor.\n\nRegularization technique: *shrinkage*, adjust the learning rate.\n\nSampling instances: Stochastic Gradient Boosting; set *subsample* hyperparameter. \n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngb_clf = GradientBoostingClassifier(n_estimators=50, max_depth=2, learning_rate=0.05, n_iter_no_change=10)\ngb_clf.fit(X_train, y_train)\ngb_clf.score(X_test, y_test)\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n0.912\n```\n:::\n:::\n\n\n### Histogram-Based Gradient Boosting\n\nOptimize for large dataset. It works by binning the input features, replacing them with integers, in which max_bins ≤ 255. It's more faster but causes a precision loss => Risk of underfitting.\n\n::: {.callout-important}\nLearn more about XGBoost, CatBoost and LightGBM\n:::\n\n\n## Stacking (Stacked Generalization)\n\nStacking method works by training a model to aggregate the predictions instead of majority voting like bagging method. This model, also called *blender* or *meta learner*, uses the prediction of weak learners (out-of-sample) as input and makes final prediction.\n\n![Stacking method](images/stacking.png){#fig-stacking}\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.svm import LinearSVC\n\nstacking_clf = StackingClassifier(\n    estimators=[\n        ('logis', LogisticRegression(random_state=29)),\n        ('rf', RandomForestClassifier(random_state=29)),\n        ('svc', LinearSVC(random_state=29))\n    ],\n    final_estimator=RandomForestClassifier(random_state=29),\n    cv=5\n)\nstacking_clf.fit(X_train, y_train)\nstacking_clf.score(X_test, y_test)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/usr/local/anaconda3/envs/dhuy/lib/python3.11/site-packages/sklearn/svm/_classes.py:32: FutureWarning:\n\nThe default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/usr/local/anaconda3/envs/dhuy/lib/python3.11/site-packages/sklearn/svm/_classes.py:32: FutureWarning:\n\nThe default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n\n/usr/local/anaconda3/envs/dhuy/lib/python3.11/site-packages/sklearn/svm/_classes.py:32: FutureWarning:\n\nThe default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n\n/usr/local/anaconda3/envs/dhuy/lib/python3.11/site-packages/sklearn/svm/_classes.py:32: FutureWarning:\n\nThe default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n\n/usr/local/anaconda3/envs/dhuy/lib/python3.11/site-packages/sklearn/svm/_classes.py:32: FutureWarning:\n\nThe default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n\n/usr/local/anaconda3/envs/dhuy/lib/python3.11/site-packages/sklearn/svm/_classes.py:32: FutureWarning:\n\nThe default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n0.896\n```\n:::\n:::\n\n\nIt can be created with layer of blenders such like this\n\n![Multilayer stacking](images/multilayers-stacking.png){#fig-multilayers-stacking}\n\nIf you don't provide a final estimator, StackingClassifier will use LogisticRegression and StackingRegressor will use RidgeCV.\n\n",
    "supporting": [
      "ensemble-learning_files"
    ],
    "filters": [],
    "includes": {}
  }
}