{
  "hash": "29d9238ff9a36141af271560000f17eb",
  "result": {
    "markdown": "---\nexecute:\n    freeze: auto\n---\n\n# Classification {#sec-classification}\n\nClassification is a *supervised learning* problem whose output are categorical, different from [Linear regression](regression.qmd#linear-regression) of which are numerical. \n\nThe algorithms used to handle classification problems are divided into the following groups: Binary classification, multiclass classification, multilabel classification, multioutput classification, etc.\n\n\n## Binary Classification\n\n### Logistic Regression \n\nPretty much same as method using in [Linear regression](regression.qmd#linear-regression), Logistic regression use *sigmoid function* to the same equation using in Linear regression to turn the output into probabilities (range (0,1)).\n\n$$\n\\begin{gather}\nLinear Regression: y = θ^{T}X\\\\\nLogistic Regression: p = sigmoid(θ^{T}X)\\\\\nsigmoid(t) = \\frac{1}{1-e^{-t}}\\\\\nlogit(p) = log\\left(\\frac{p}{1-p}\\right)= t\n\\end{gather}\n$$\n\nCost function for 1 instance\n$$\\begin{equation}\n\\begin{split}\nJ(θ) & = -log(p)\\quad \\quad \\quad if\\;\\;\\; y=1\\\\\n & = -log(1-p) \\quad \\; if\\;\\;\\;  y=0\n\\end{split}\n\\end{equation}\n$$\n\n::: {.callout-note}\nCost function penalizes the model when it estimates the loew probability for the real target class\n\n- -log(p) -> inf when p -> 0 for y = 1 instance\\\n- -log(1-p) -> inf when p -> 1 for y = 0 instance\n:::\n\nThere is no closed-form equation to compute θ. We will use gradient descent to find the best weights.\n\nCost function for whole training set (log loss): *convex function*\n$$J(θ) = \\frac{−1}{m} \\sum [y_ilog(p_i) + (1−y_i)log(1−p_i)]$$\n\nGradient\n$$∇ = \\frac{1}{m}X^{T}[sigmoid(Xθ) - y]$$\n\n::: {.callout-important}\n- Log loss `assumption`: the instances follow a Gaussian distribution around the mean of their class\\\n- MSE `assumption`: data is purely linear\\\n- The more wrong `assumption`, the more `biased` the model\n:::\n\nDecision boudaries: \n\n\n::: {.callout-tip}\nRegularization in Logistic Regression: l1, l2 using C parameter (inverse of alpha)\n:::\n\nImplement Linear regression using sklearn: [Logistic regression](hands-on-classification.qmd#logistic-regression)\n\n\n### Softmax Regression (Multinomial Logistic Regression)\n\nThe Logistic regression can be generalized to support multipleclass classification directly. It is called *softmax regression*. \n\nThe strategy when given an instance x is described like this:\n\n1. Compute score for each class using *softmax score function*\n2. Compute probability for each class using *softmax function* to each score\n3. Choose the class with the highest probability. The instance x is belong to this class\n\nSoftmax score for class k\n$$s_k(x) = (θ^{(k)})^{T}X$$\n\n::: {.callout-note}\nEach class has own parameter vecto θ(k). *Parameter matrix Θ* contains all parameter vectors of all classes\n:::\n\nSoftmax function for class k: \n$$p_k = σ(s(x))_k = \\frac{exp(s_k(x))}{\\sum\\limits exp(s_j(x))}$$\n\n- K is the number of classes\\\n- s(x) is a vector containing the scores of each class for the instance x\\\n- σ(s(x))k is the estimated probability that the instance x belongs to class k, given the scores of each class for that instance\\\n\nChoose the class with the highest probability\n$$y= argmax\\; σ(s(x))_k= argmax\\;s_k(x) = argmax\\; (θ^{k})^{T}X$$\n\nJust like Logistic regression, softmax regression has the cost function called `Cross entropy`\n\nCross entropy cost function\n\n$$\nJ(Θ) = −\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}y_{k}^{(i)}log(p_{k}^{(i)})\n$$\n\n::: {.callout-note}\n- yk(i): the label of the target class\\\n- When k=2, softmax regression is equivalent to logistic regression\n:::\n\nCross entropy gradient vector for class k\n\n$$\n∇_{θ}k = \\frac{1}{m}\\sum(p_{k}^{(i)} − y_{k}^{i})x^{(i)}\n$$\n\nImplement Linear regression using sklearn\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom sklearn.datasets import load_iris\n\niris = load_iris(as_frame=True)\niris.target_names\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\narray(['setosa', 'versicolor', 'virginica'], dtype='<U10')\n```\n:::\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nX = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\ny = iris[\"target\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nsoftmax = LogisticRegression(max_iter=1000, C=30)\nsoftmax.fit(X_train, y_train)\nprint(softmax.predict([X_test[0]]))\nprint(softmax.predict_proba([X_test[0]]).round(4))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]\n[[0.     0.9827 0.0173]]\n```\n:::\n:::\n\n\n## Multiclass Classification\n\n\n\n## Multilabel Classification\n\n\n\n## Multioutput Classification\n\n",
    "supporting": [
      "classification_files"
    ],
    "filters": [],
    "includes": {}
  }
}