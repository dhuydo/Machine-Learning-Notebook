{
  "hash": "3be5be5174d91c554e19d0d1af8cab86",
  "result": {
    "markdown": "---\nexecute:\n    freeze: auto\n---\n\n# Dimensionality Reduction {#sec-dimensionality-reduction}\n\n\nDimensionality reduction tends to trade lower performance to get higher training speed and easier visualization. However, it makes the pipeline more complex and may not effective. Therefore, we first try to train our system with the original data before considering using dimensionality reduction.\n\nThere are 2 main approaches: projection (PCA, random projection) and manifold learning (locally linear embedding).\n\n\n## The Curse of Dimensionality\n\nBecause of the huge number of spaces in high dimensions, high-dimensional datasets are at risk of being very sparse: most training instances are likely to be far away from each other. Therefore, a new instance will likely be far away from any training instance, making predictions much less reliable than in lower dimensions, since they will be based on much larger extrapolations. In short, the more dimensions the training set has, the greater the risk of overfitting it.\n\n\n## Projection\n\nSuppose that we want to turn a 3D dataset into 2D (called *subspace*), we will project all instances perpendicularly onto this subspace. \n\n![Turn a 3D dataset into 2D](images/lower-dimension.png){#fig-lower-dimension}\n\n### PCA\n\nPCA first identifies the righ hyperplane that lies closest to the data, and then it projects the data onto it. This hyperplane preserves maximum variance and minimizes the mean squared distance between the original dataset and its projection.\n\n![Select the right hyperplane (c1)](images/select-hyperplane.png){#fig-select-hyperplane}\n\nHow does PCA do that? \n\n1. Center the data (minus mean).\n2. Find an axis (principle component) accounts for the largest amount of variance. \n3. Each next priciple components orthogonal to the previous one accounting for the largest amount of the remaining variance.\n\nTo find the principle components, PCA use SVD technique (*singular value decomposition*) that decompose data X into UΣV⊺ where V⊺ contains the unit vectors that define all the principal components.\n\n$$X = UΣV^T$$\n\nThen, we multiply matrix X with V⊺ to get the lower-dimension data.\n\n$$X_d = XV^T$$\n\n::: {.callout-tip}\nHyperparameter: n_components, svd_solver\n\nAttribute of PCA: components_, explained_variance_ratio_, n_components_\n:::\n\nChoose the right number of dimensions by setting the n_components hyperparameter.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.decomposition import PCA\n\n# mnist = fetch_openml('mnist_784', as_frame=False)\n# X_train, y_train = mnist.data[:600], mnist.target[:600] \n# X_test, y_test = mnist.data[600:1200], mnist.target[600:1200]\n\n# pca = PCA(n_components=0.95)    # choose n_components that preserve ~ 95% variance of data\n# X_reduced = pca.fit_transform(X_train)\n# print(f'Number of components: {pca.n_components_}')\n```\n:::\n\n\nTuning the n_components hyperparameter to compress the data.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\npipe = Pipeline([\n    ('pca', PCA(random_state=29)),\n    ('classifier', RandomForestClassifier(random_state=29))\n])\nparams = {\n    'pca__n_components': np.arange(10,15),\n    'classifier__n_estimators': [50,60,70]\n}\ngrid = RandomizedSearchCV(estimator=pipe, param_distributions=params, cv=5, scoring='accuracy', random_state=29)\n# grid.fit(X_train, y_train)\n# print(f'Best params: {grid.best_params_}')\n```\n:::\n\n\nDecompress the transformed data.\n\n$$X_{recovered} = X_dV^T$$\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# X_recovered = pca.inverse_transform(X_reduced)\n```\n:::\n\n\n**Randomized PCA**: svd_solver='random'; quickly find an approximation of d principle components, auto if max(m,n)>500 and n_components < 80% of min(m,n).\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# rand_pca = PCA(n_components=154, svd_solver='randomized', random_state=29)\n# X_reduced = rand_pca.fit_transform(X_train)\n```\n:::\n\n\n**Incremental PCA**: use np.array_split, partial_fit; fit mini-batch of data\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.decomposition import IncrementalPCA\n\nn_batches = 100\ninc_pca = IncrementalPCA(n_components=154)\n# for batch in np.array_split(X_train, n_batches):\n#     inc_pca.fit(batch)\n# X_reduced = inc_pca.transform(X_train)\n```\n:::\n\n\n### Random Projection\n\nFor very high-dimension dataset, PCA can be too slow. Therefore, random projection is solution. It works by implementing random linear projection.\n\n1. Choose the optimal number of dimensions by sklearn.random_projection.johnson_lindenstrauss_min_dim: compute minimum number of dimensions to ensure the squared distance between any two instances to change by more than a tolerance.\n\n$$d ≥ 4log(m)\\frac{1}{(\\frac{1}{2}ε2 - \\frac{1}{3}ε3)}$$\n\nd: target dimension\\\nm: number of instances\\\nε: tolerance\\\n2. Generate a random matrix P of shape [d,n] (n: number of features), from a Gaussian distribution with mean 0 and variance 1/d.\\\n3. Reduced matrix = X @ P.T\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.random_projection import GaussianRandomProjection\n\ngauss_rand_prj = GaussianRandomProjection(eps=0.1, random_state=29)\n# X_reduced = gauss_rand_prj.fit_transform(X_train)\n```\n:::\n\n\n### Sparse Random Projection\n\nWork as same as random projection, except the random matrix is sparse, so that it use much less memory and train much faster and is preferred.\n\n\n\n## Manifold Learning\n\nIn many cases the subspace may twist and turn, projection is not useful @fig-roll-dataset. Simply projecting onto a plane (e.g., by dropping x3) would squash different layers together (@fig-project-unroll, left). Instead, we will unroll to obtain the 2D dataset (@fig-project-unroll, right).\n\n![Roll dataset](images/roll-dataset.png){#fig-roll-dataset}\n\n![Projecting and unrolling the roll](images/project-unroll.png){#fig-project-unroll}\n\nThe roll is an example of 2D manifold. Generally, a d-dimensional manifold is a part of an n-dimensional space (where d < n) that locally resembles a d-dimensional hyperplane. \n\nHowever, manifold learning may not always lead to a better or simpler solution, it all depends on dataset.\n\n![Good use case (upper) and not good use case (lower) of manifold learning](images/manifold-downside.png){#fig-manifold-downside}\n\n\n### LLE (Locally Linear Embedding)\n\nLLE is used for nonlinear task, do not scale well.\n\n1. Measuring how each training instance linearly relates to its nearest neighbors (k-nearest neighbors).\n2. Find low-dimensional data where these local relationships are best preserved.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.datasets import make_swiss_roll\nfrom sklearn.manifold import LocallyLinearEmbedding\n\nX_swiss, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\nlle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)\n# X_unrolled = lle.fit_transform(X_swiss)\n```\n:::\n\n\n",
    "supporting": [
      "dimensionality-reduction_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}